{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc procedures during development and results collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TSA conll data to DatasetDict\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "root_folder = \"data\"\n",
    "tsa_folder = os.path.join(root_folder,\"tsa_conll\")\n",
    "arrow_folder = os.path.join(root_folder,\"tsa_arrow_2\")\n",
    "\n",
    "def parse_conll(raw:str, sep=\"\\t\"):\n",
    "    \"\"\"Parses the norec-fine conll files with tab separator and sentence id\"\"\"\n",
    "    doc_parsed = [] # One dict per sentence. meta, tokens and tags\n",
    "    for sent in raw.strip().split(\"\\n\\n\"):\n",
    "        meta = \"\"\n",
    "        tokens, tags = [], []\n",
    "        for line in sent.split(\"\\n\"):\n",
    "            if line.startswith(\"#\") and \"=\" in line:\n",
    "                meta = line.split(\"=\")[-1]\n",
    "            else:\n",
    "                elems = line.strip().split(sep)\n",
    "                assert len(elems) == 2\n",
    "                tokens.append(elems[0])\n",
    "                tags.append(elems[1])\n",
    "        assert len(meta) > 0\n",
    "        doc_parsed.append({\"idx\": meta, \"tokens\":tokens, \"tsa_tags\":tags})\n",
    "\n",
    "    return doc_parsed\n",
    "\n",
    "\n",
    "splits = {\"train\": \"train\", \"dev\": \"validation\", \"test\": \"test\"} # \"validation\" for HF naming convention\n",
    "d_sets = {}\n",
    "for split in splits:\n",
    "    path = os.path.join(tsa_folder, split+\".conll\")\n",
    "    with open(path) as rf:\n",
    "        conll_txt = rf.read()\n",
    "    print(len(conll_txt.split(\"\\n\\n\")))\n",
    "    sents = parse_conll(conll_txt)\n",
    "    # for sent in sents:\n",
    "        # sent[\"labels\"] = [label_mapping[tag] for tag in sent[\"tsa_tags\"]]\n",
    "    d_sets[splits[split]] = Dataset.from_pandas(pd.DataFrame(sents))\n",
    "\n",
    "DatasetDict(d_sets).save_to_disk(arrow_folder)\n",
    "    # sentences = parse(conll_txt)\n",
    "    # sentences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc08b14a8be94b44a795ac1d6e183843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9065de2ebb84991b88b9869840d3edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31127bb7a505442b859519da8860a235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cdaa272938478c8d1d4bbb0ce12773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54705c1314144d07b6acd5b2906cbefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f228510e4a446ae8f1cf527870c91fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download datasets from HF with configuration\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from pathlib import Path\n",
    "root_folder = \"data\"\n",
    "addr = \"ltg/norec_tsa\"\n",
    "configs = [\"binary\", \"intensity\"]\n",
    "for config in configs:\n",
    "    ds = load_dataset(addr, config)\n",
    "    ds.save_to_disk(Path(root_folder, \"tsa_\"+config))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# If you want a list of models with where to get them from\n",
    "models = {\n",
    "\"NorBERT_3_x-small\": \"ltg/norbert3-xs\", \n",
    "\"NorBERT_3_small\": \"ltg/norbert3-small\", \n",
    "\"NorBERT_1\": \"ltg/norbert\" , \n",
    "\"NorBERT_2\":\"ltg/norbert2\",\n",
    "\"NB-BERT_base\": \"NbAiLab/nb-bert-base\",\n",
    "\"ScandiBERT\": \"vesteinn/ScandiBERT\", \n",
    "\"mBERT\": \"bert-base-multilingual-cased\", \n",
    "\"XLM-R_base\": \"xlm-roberta-base\",  \n",
    "\"NorBERT_3_base\":\"ltg/norbert3-base\",\n",
    "\"XLM-R_large\": \"xlm-roberta-large\",\n",
    "\"NB-BERT_large\": \"NbAiLab/nb-bert-large\", \n",
    "\"NorBERT_3_large\": \"ltg/norbert3-large\",\n",
    "}\n",
    "\n",
    "with open(\"configs/models_name_addr1.json\", \"w\", encoding=\"utf8\") as wf:\n",
    "    json.dump(models, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under here, analysis of the json log files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': '04201157_tsa_NorBERT_3_base', 'epoch': 2, 'eval_f1': 0.5235955056179775}\n",
      "{'idx': '04201157_tsa_NB-BERT_base', 'epoch': 6, 'eval_f1': 0.5252873563218391}\n",
      "{'idx': '04201157_tsa_XLM-R_base', 'epoch': 8, 'eval_f1': 0.4937192790824686}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "# Load json files with details on each epoch for each experiment. Get the best epoch\n",
    "jsons = [(p.stem[:-4] ,json.loads(p.read_text()) )for p in Path(\"logs/jsons\").iterdir()]\n",
    "best_epochs = []\n",
    "for r in jsons:\n",
    "    idx = r[0]\n",
    "    epoch_eval = [ee for ee in r[1] if \"eval_f1\" in ee]\n",
    "    epoch_eval = sorted(epoch_eval, key = lambda l: l[\"eval_f1\"], reverse=True)\n",
    "    best_epochs.append({\"idx\": idx, \n",
    "                    \"epoch\": int(epoch_eval[0][\"epoch\"]),\n",
    "                    \"eval_f1\": epoch_eval[0][\"eval_f1\"] })\n",
    "    print(best_epochs[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>second_best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>XLM-R_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.493719</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XLM-R_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.491877</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NorBERT_3_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.523596</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NorBERT_3_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.523167</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NB-BERT_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.525287</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NB-BERT_base</td>\n",
       "      <td>tsa</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.519016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model task  epoch   eval_f1  best_epoch  second_best\n",
       "20      XLM-R_base  tsa    8.0  0.493719        True        False\n",
       "21      XLM-R_base  tsa    7.0  0.491877       False         True\n",
       "0   NorBERT_3_base  tsa    2.0  0.523596        True        False\n",
       "1   NorBERT_3_base  tsa    3.0  0.523167       False         True\n",
       "10    NB-BERT_base  tsa    6.0  0.525287        True        False\n",
       "11    NB-BERT_base  tsa    7.0  0.519016       False         True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "records = [] # List of dicts that have the model name injected  \n",
    "for log_path in Path(\"logs/jsons\").iterdir():\n",
    "    p_stem = log_path.stem\n",
    "    stem_segments = p_stem.split(\"_\")\n",
    "    task = stem_segments[1]\n",
    "    ts = stem_segments[0]\n",
    "    m_name = p_stem[13:-4]\n",
    "    log = json.loads(log_path.read_text())\n",
    "\n",
    "    epoch_eval = [ee for ee in log if \"eval_f1\" in ee]\n",
    "    epoch_eval = sorted(epoch_eval, key = lambda l: l[\"eval_f1\"], reverse=True)\n",
    "    for i, epoch_log in enumerate(epoch_eval):\n",
    "        epoch_log.update({\"timestamp\":ts,\"model\":m_name, \"task\":task,\"best_epoch\": i==0, \"second_best\": i==1})\n",
    "        records.append(epoch_log)\n",
    "\n",
    "df_all = pd.DataFrame.from_records(records)\n",
    "# Function to filter the df_all according to True in \"best_epoch\" or \"second_best\"\n",
    "\n",
    "\n",
    "df = df_all[(df_all[\"best_epoch\"]== True) | (df_all[\"second_best\"]==True) ].sort_values([\"task\", \"eval_f1\"], ascending=False)\n",
    "df[[\"model\", \"task\", \"epoch\", \"eval_f1\", \"best_epoch\", \"second_best\"]].to_clipboard()\n",
    "# df.to_csv(\"output/dev_evals.csv\", index=False) # Write this for reporting and analysis\n",
    "\n",
    "\n",
    "df[[\"model\", \"task\", \"epoch\", \"eval_f1\", \"best_epoch\", \"second_best\"]].sort_values([\"model\", \"eval_f1\"], ascending=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = [1,2]\n",
    "a,b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f766e18cf02043d406c2f113693a415f1494f09983f05ef5cfd3ee3ed0acbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
