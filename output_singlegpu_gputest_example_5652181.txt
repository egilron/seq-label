Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.25.2
PyTorch: 2.0.1+rocm5.4.2
Transformers: 4.36.2



***Loading config file: configs/lumi/01121628_tsa-bin_XLM-R_base_02.json
config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]config.json: 100%|██████████| 615/615 [00:00<00:00, 5.05MB/s]
sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]sentencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 14.6MB/s]sentencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 14.4MB/s]
tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 11.9MB/s]tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 11.7MB/s]
model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]model.safetensors:   1%|          | 10.5M/1.12G [00:00<00:34, 32.5MB/s]model.safetensors:   2%|▏         | 21.0M/1.12G [00:00<00:25, 42.3MB/s]model.safetensors:   3%|▎         | 31.5M/1.12G [00:00<00:20, 53.0MB/s]model.safetensors:   4%|▍         | 41.9M/1.12G [00:00<00:17, 61.9MB/s]model.safetensors:   6%|▌         | 62.9M/1.12G [00:00<00:11, 88.2MB/s]model.safetensors:   8%|▊         | 83.9M/1.12G [00:01<00:09, 106MB/s] model.safetensors:   9%|▉         | 105M/1.12G [00:01<00:07, 127MB/s] model.safetensors:  11%|█▏        | 126M/1.12G [00:01<00:07, 129MB/s]model.safetensors:  13%|█▎        | 147M/1.12G [00:01<00:06, 141MB/s]model.safetensors:  15%|█▌        | 168M/1.12G [00:01<00:07, 134MB/s]model.safetensors:  17%|█▋        | 189M/1.12G [00:01<00:06, 139MB/s]model.safetensors:  19%|█▉        | 210M/1.12G [00:01<00:06, 149MB/s]model.safetensors:  21%|██        | 231M/1.12G [00:02<00:05, 148MB/s]model.safetensors:  23%|██▎       | 252M/1.12G [00:02<00:06, 141MB/s]model.safetensors:  24%|██▍       | 273M/1.12G [00:02<00:05, 144MB/s]model.safetensors:  26%|██▋       | 294M/1.12G [00:02<00:05, 154MB/s]model.safetensors:  29%|██▉       | 325M/1.12G [00:02<00:04, 161MB/s]model.safetensors:  31%|███       | 346M/1.12G [00:02<00:05, 152MB/s]model.safetensors:  33%|███▎      | 367M/1.12G [00:02<00:05, 148MB/s]model.safetensors:  35%|███▍      | 388M/1.12G [00:03<00:04, 159MB/s]model.safetensors:  37%|███▋      | 409M/1.12G [00:03<00:04, 151MB/s]model.safetensors:  39%|███▉      | 440M/1.12G [00:03<00:03, 170MB/s]model.safetensors:  41%|████▏     | 461M/1.12G [00:03<00:03, 175MB/s]model.safetensors:  43%|████▎     | 482M/1.12G [00:03<00:03, 173MB/s]model.safetensors:  45%|████▌     | 503M/1.12G [00:03<00:03, 174MB/s]model.safetensors:  47%|████▋     | 524M/1.12G [00:03<00:03, 182MB/s]model.safetensors:  49%|████▉     | 545M/1.12G [00:03<00:03, 188MB/s]model.safetensors:  51%|█████     | 566M/1.12G [00:04<00:03, 183MB/s]model.safetensors:  53%|█████▎    | 587M/1.12G [00:04<00:02, 189MB/s]model.safetensors:  55%|█████▌    | 619M/1.12G [00:04<00:02, 185MB/s]model.safetensors:  57%|█████▋    | 640M/1.12G [00:04<00:02, 182MB/s]model.safetensors:  59%|█████▉    | 661M/1.12G [00:04<00:02, 186MB/s]model.safetensors:  61%|██████    | 682M/1.12G [00:04<00:02, 186MB/s]model.safetensors:  63%|██████▎   | 703M/1.12G [00:04<00:02, 176MB/s]model.safetensors:  65%|██████▍   | 724M/1.12G [00:04<00:02, 183MB/s]model.safetensors:  67%|██████▋   | 744M/1.12G [00:05<00:02, 171MB/s]model.safetensors:  70%|██████▉   | 776M/1.12G [00:05<00:01, 183MB/s]model.safetensors:  71%|███████▏  | 797M/1.12G [00:05<00:01, 184MB/s]model.safetensors:  73%|███████▎  | 818M/1.12G [00:05<00:01, 186MB/s]model.safetensors:  75%|███████▌  | 839M/1.12G [00:05<00:01, 190MB/s]model.safetensors:  78%|███████▊  | 870M/1.12G [00:05<00:01, 198MB/s]model.safetensors:  80%|███████▉  | 891M/1.12G [00:05<00:01, 198MB/s]model.safetensors:  82%|████████▏ | 912M/1.12G [00:05<00:01, 200MB/s]model.safetensors:  84%|████████▎ | 933M/1.12G [00:06<00:01, 176MB/s]model.safetensors:  86%|████████▌ | 954M/1.12G [00:06<00:00, 183MB/s]model.safetensors:  87%|████████▋ | 975M/1.12G [00:06<00:00, 181MB/s]model.safetensors:  90%|█████████ | 1.01G/1.12G [00:06<00:00, 185MB/s]model.safetensors:  92%|█████████▏| 1.03G/1.12G [00:06<00:00, 184MB/s]model.safetensors:  94%|█████████▍| 1.05G/1.12G [00:06<00:00, 171MB/s]model.safetensors:  96%|█████████▌| 1.07G/1.12G [00:06<00:00, 175MB/s]model.safetensors:  98%|█████████▊| 1.09G/1.12G [00:06<00:00, 180MB/s]model.safetensors: 100%|█████████▉| 1.11G/1.12G [00:07<00:00, 180MB/s]model.safetensors: 100%|██████████| 1.12G/1.12G [00:07<00:00, 159MB/s]
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01121628_tsa-bin_XLM-R_base Our label2id: {'O': 0, 'B-targ-Negative': 1, 'I-targ-Negative': 2, 'B-targ-Positive': 3, 'I-targ-Positive': 4}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 7200.50 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 9002.80 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 9278.34 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 9482.03 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:00<00:00, 9853.61 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:00<00:00, 10120.71 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:00<00:00, 9609.65 examples/s] 
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 9487.31 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 9062.18 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 9082.10 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 8857.35 examples/s]
Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 6.34k/6.34k [00:00<00:00, 48.5MB/s]
01121628_tsa-bin_XLM-R_base Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'attention_mask', 'labels']
Traceback (most recent call last):
  File "/users/rnningst/seq-label_github-com/seq-label/seq_label.py", line 258, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/opt/conda/envs/conda_container_env/lib/python3.11/site-packages/transformers/trainer.py", line 513, in __init__
    os.makedirs(self.args.output_dir, exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 215, in makedirs
  [Previous line repeated 1 more time]
  File "<frozen os>", line 225, in makedirs
OSError: [Errno 30] Read-only file system: '/scratch'
srun: error: nid005046: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=5652181.0
