Starting job 10901743 on c7-5 on saga at Fri Mar 8 11:01:27 CET 2024

/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_01.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_01.json
03081014_elsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2914.86 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2888.23 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:01, 2828.18 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 2881.93 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:01, 2888.32 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 2937.25 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 2904.31 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 2955.41 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 2964.49 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 2910.77 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 2903.72 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2119.46 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2109.28 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 2890.71 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2732.70 examples/s]
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1007, 'grad_norm': 0.3941308557987213, 'learning_rate': 9e-06, 'epoch': 1.0}
{'eval_loss': 0.05227205902338028, 'eval_precision': 0.5565092989985694, 'eval_recall': 0.6264090177133655, 'eval_f1': 0.5893939393939395, 'eval_accuracy': 0.9843670613983664, 'eval_runtime': 5.2068, 'eval_samples_per_second': 290.579, 'eval_steps_per_second': 36.49, 'epoch': 1.0}
{'loss': 0.0421, 'grad_norm': 0.2901754677295685, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'eval_loss': 0.050179384648799896, 'eval_precision': 0.6370839936608558, 'eval_recall': 0.6473429951690821, 'eval_f1': 0.6421725239616614, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 5.1676, 'eval_samples_per_second': 292.785, 'eval_steps_per_second': 36.767, 'epoch': 2.0}
{'loss': 0.0321, 'grad_norm': 0.03367621451616287, 'learning_rate': 7e-06, 'epoch': 3.0}
{'eval_loss': 0.051430005580186844, 'eval_precision': 0.68, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6955153422501967, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 5.1619, 'eval_samples_per_second': 293.112, 'eval_steps_per_second': 36.808, 'epoch': 3.0}
{'loss': 0.0239, 'grad_norm': 2.254473924636841, 'learning_rate': 6e-06, 'epoch': 4.0}
{'eval_loss': 0.054178327322006226, 'eval_precision': 0.6539589442815249, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6845740598618572, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 5.1718, 'eval_samples_per_second': 292.549, 'eval_steps_per_second': 36.738, 'epoch': 4.0}
{'loss': 0.0176, 'grad_norm': 1.0790948867797852, 'learning_rate': 5e-06, 'epoch': 5.0}
{'eval_loss': 0.06276344507932663, 'eval_precision': 0.6499261447562777, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6779661016949152, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 5.1617, 'eval_samples_per_second': 293.123, 'eval_steps_per_second': 36.81, 'epoch': 5.0}
{'loss': 0.0134, 'grad_norm': 1.4230287075042725, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0}
{'eval_loss': 0.06381404399871826, 'eval_precision': 0.6856240126382307, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6921850079744817, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 5.1559, 'eval_samples_per_second': 293.449, 'eval_steps_per_second': 36.851, 'epoch': 6.0}
{'loss': 0.0105, 'grad_norm': 0.45343440771102905, 'learning_rate': 3e-06, 'epoch': 7.0}
{'eval_loss': 0.06545764952898026, 'eval_precision': 0.6697247706422018, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6870588235294117, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 5.169, 'eval_samples_per_second': 292.706, 'eval_steps_per_second': 36.757, 'epoch': 7.0}
{'loss': 0.0085, 'grad_norm': 0.8096232414245605, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 0.06991391628980637, 'eval_precision': 0.6677018633540373, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6798418972332015, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 5.1522, 'eval_samples_per_second': 293.663, 'eval_steps_per_second': 36.878, 'epoch': 8.0}
{'loss': 0.0072, 'grad_norm': 0.38182348012924194, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0}
{'eval_loss': 0.07056081295013428, 'eval_precision': 0.6646433990895296, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.684375, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 5.1506, 'eval_samples_per_second': 293.75, 'eval_steps_per_second': 36.889, 'epoch': 9.0}
{'loss': 0.0062, 'grad_norm': 0.37462055683135986, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.07151833921670914, 'eval_precision': 0.6737804878048781, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6922474549725921, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 5.1558, 'eval_samples_per_second': 293.454, 'eval_steps_per_second': 36.851, 'epoch': 10.0}
{'train_runtime': 836.6091, 'train_samples_per_second': 102.437, 'train_steps_per_second': 6.407, 'train_loss': 0.026217261579499315, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0262
  train_runtime            = 0:13:56.60
  train_samples            =       8570
  train_samples_per_second =    102.437
  train_steps_per_second   =      6.407
[{'loss': 0.1007, 'grad_norm': 0.3941308557987213, 'learning_rate': 9e-06, 'epoch': 1.0, 'step': 536}, {'eval_loss': 0.05227205902338028, 'eval_precision': 0.5565092989985694, 'eval_recall': 0.6264090177133655, 'eval_f1': 0.5893939393939395, 'eval_accuracy': 0.9843670613983664, 'eval_runtime': 5.2068, 'eval_samples_per_second': 290.579, 'eval_steps_per_second': 36.49, 'epoch': 1.0, 'step': 536}, {'loss': 0.0421, 'grad_norm': 0.2901754677295685, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0, 'step': 1072}, {'eval_loss': 0.050179384648799896, 'eval_precision': 0.6370839936608558, 'eval_recall': 0.6473429951690821, 'eval_f1': 0.6421725239616614, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 5.1676, 'eval_samples_per_second': 292.785, 'eval_steps_per_second': 36.767, 'epoch': 2.0, 'step': 1072}, {'loss': 0.0321, 'grad_norm': 0.03367621451616287, 'learning_rate': 7e-06, 'epoch': 3.0, 'step': 1608}, {'eval_loss': 0.051430005580186844, 'eval_precision': 0.68, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6955153422501967, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 5.1619, 'eval_samples_per_second': 293.112, 'eval_steps_per_second': 36.808, 'epoch': 3.0, 'step': 1608}, {'loss': 0.0239, 'grad_norm': 2.254473924636841, 'learning_rate': 6e-06, 'epoch': 4.0, 'step': 2144}, {'eval_loss': 0.054178327322006226, 'eval_precision': 0.6539589442815249, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6845740598618572, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 5.1718, 'eval_samples_per_second': 292.549, 'eval_steps_per_second': 36.738, 'epoch': 4.0, 'step': 2144}, {'loss': 0.0176, 'grad_norm': 1.0790948867797852, 'learning_rate': 5e-06, 'epoch': 5.0, 'step': 2680}, {'eval_loss': 0.06276344507932663, 'eval_precision': 0.6499261447562777, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6779661016949152, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 5.1617, 'eval_samples_per_second': 293.123, 'eval_steps_per_second': 36.81, 'epoch': 5.0, 'step': 2680}, {'loss': 0.0134, 'grad_norm': 1.4230287075042725, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0, 'step': 3216}, {'eval_loss': 0.06381404399871826, 'eval_precision': 0.6856240126382307, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6921850079744817, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 5.1559, 'eval_samples_per_second': 293.449, 'eval_steps_per_second': 36.851, 'epoch': 6.0, 'step': 3216}, {'loss': 0.0105, 'grad_norm': 0.45343440771102905, 'learning_rate': 3e-06, 'epoch': 7.0, 'step': 3752}, {'eval_loss': 0.06545764952898026, 'eval_precision': 0.6697247706422018, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6870588235294117, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 5.169, 'eval_samples_per_second': 292.706, 'eval_steps_per_second': 36.757, 'epoch': 7.0, 'step': 3752}, {'loss': 0.0085, 'grad_norm': 0.8096232414245605, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0, 'step': 4288}, {'eval_loss': 0.06991391628980637, 'eval_precision': 0.6677018633540373, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6798418972332015, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 5.1522, 'eval_samples_per_second': 293.663, 'eval_steps_per_second': 36.878, 'epoch': 8.0, 'step': 4288}, {'loss': 0.0072, 'grad_norm': 0.38182348012924194, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0, 'step': 4824}, {'eval_loss': 0.07056081295013428, 'eval_precision': 0.6646433990895296, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.684375, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 5.1506, 'eval_samples_per_second': 293.75, 'eval_steps_per_second': 36.889, 'epoch': 9.0, 'step': 4824}, {'loss': 0.0062, 'grad_norm': 0.37462055683135986, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 5360}, {'eval_loss': 0.07151833921670914, 'eval_precision': 0.6737804878048781, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6922474549725921, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 5.1558, 'eval_samples_per_second': 293.454, 'eval_steps_per_second': 36.851, 'epoch': 10.0, 'step': 5360}, {'train_runtime': 836.6091, 'train_samples_per_second': 102.437, 'train_steps_per_second': 6.407, 'total_flos': 2851119422874276.0, 'train_loss': 0.026217261579499315, 'epoch': 10.0, 'step': 5360}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9878
  predict_f1                 =     0.6638
  predict_loss               =     0.0444
  predict_precision          =     0.6505
  predict_recall             =     0.6776
  predict_runtime            = 0:00:04.32
  predict_samples_per_second =      289.6
  predict_steps_per_second   =     36.316
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_01.json completed. F1: 0.6638023630504833
/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_11.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_11.json
03081014_elsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2959.82 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2514.91 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:02, 2452.18 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 2635.30 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:01, 2724.46 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 2826.86 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 2832.16 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 2900.29 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2920.17 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2679.61 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 2908.97 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2958.05 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2443.17 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 2897.11 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2344.88 examples/s]
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_base/checkpoint-1072 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1424, 'grad_norm': 0.5165062546730042, 'learning_rate': 9e-06, 'epoch': 1.0}
{'eval_loss': 0.05736888200044632, 'eval_precision': 0.5547226386806596, 'eval_recall': 0.5958132045088567, 'eval_f1': 0.5745341614906833, 'eval_accuracy': 0.9829991792707234, 'eval_runtime': 5.1862, 'eval_samples_per_second': 291.738, 'eval_steps_per_second': 36.636, 'epoch': 1.0}
{'loss': 0.0464, 'grad_norm': 0.35102981328964233, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'eval_loss': 0.05008818581700325, 'eval_precision': 0.6502384737678856, 'eval_recall': 0.6586151368760065, 'eval_f1': 0.6544000000000001, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 5.2201, 'eval_samples_per_second': 289.843, 'eval_steps_per_second': 36.398, 'epoch': 2.0}
{'loss': 0.0364, 'grad_norm': 0.28528907895088196, 'learning_rate': 7e-06, 'epoch': 3.0}
{'eval_loss': 0.04884575307369232, 'eval_precision': 0.6577777777777778, 'eval_recall': 0.714975845410628, 'eval_f1': 0.6851851851851852, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 5.2949, 'eval_samples_per_second': 285.745, 'eval_steps_per_second': 35.883, 'epoch': 3.0}
{'loss': 0.03, 'grad_norm': 0.6752567291259766, 'learning_rate': 6e-06, 'epoch': 4.0}
{'eval_loss': 0.04789537191390991, 'eval_precision': 0.7031746031746032, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7082334132693845, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 5.1673, 'eval_samples_per_second': 292.803, 'eval_steps_per_second': 36.77, 'epoch': 4.0}
{'loss': 0.024, 'grad_norm': 0.4847274720668793, 'learning_rate': 5e-06, 'epoch': 5.0}
{'eval_loss': 0.053630683571100235, 'eval_precision': 0.6419939577039275, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6625097427903353, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 5.191, 'eval_samples_per_second': 291.468, 'eval_steps_per_second': 36.602, 'epoch': 5.0}
{'loss': 0.0196, 'grad_norm': 1.1096200942993164, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0}
{'eval_loss': 0.05453808233141899, 'eval_precision': 0.6621823617339312, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6868217054263566, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 5.1952, 'eval_samples_per_second': 291.228, 'eval_steps_per_second': 36.572, 'epoch': 6.0}
{'loss': 0.0167, 'grad_norm': 0.3038788437843323, 'learning_rate': 3e-06, 'epoch': 7.0}
{'eval_loss': 0.054741423577070236, 'eval_precision': 0.6707132018209409, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6906249999999999, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 6.2948, 'eval_samples_per_second': 240.355, 'eval_steps_per_second': 30.183, 'epoch': 7.0}
{'loss': 0.0142, 'grad_norm': 0.7545785307884216, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 0.05498635396361351, 'eval_precision': 0.6975308641975309, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7123719464144996, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 5.2864, 'eval_samples_per_second': 286.206, 'eval_steps_per_second': 35.941, 'epoch': 8.0}
{'loss': 0.0126, 'grad_norm': 0.8089251518249512, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0}
{'eval_loss': 0.0570860393345356, 'eval_precision': 0.6753445635528331, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6923076923076923, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 5.2379, 'eval_samples_per_second': 288.854, 'eval_steps_per_second': 36.274, 'epoch': 9.0}
{'loss': 0.0119, 'grad_norm': 0.274061381816864, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.05740262567996979, 'eval_precision': 0.6919504643962848, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7056037884767166, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 5.1773, 'eval_samples_per_second': 292.238, 'eval_steps_per_second': 36.699, 'epoch': 10.0}
{'train_runtime': 863.1425, 'train_samples_per_second': 99.288, 'train_steps_per_second': 3.105, 'train_loss': 0.03540833325528387, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0354
  train_runtime            = 0:14:23.14
  train_samples            =       8570
  train_samples_per_second =     99.288
  train_steps_per_second   =      3.105
[{'loss': 0.1424, 'grad_norm': 0.5165062546730042, 'learning_rate': 9e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05736888200044632, 'eval_precision': 0.5547226386806596, 'eval_recall': 0.5958132045088567, 'eval_f1': 0.5745341614906833, 'eval_accuracy': 0.9829991792707234, 'eval_runtime': 5.1862, 'eval_samples_per_second': 291.738, 'eval_steps_per_second': 36.636, 'epoch': 1.0, 'step': 268}, {'loss': 0.0464, 'grad_norm': 0.35102981328964233, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.05008818581700325, 'eval_precision': 0.6502384737678856, 'eval_recall': 0.6586151368760065, 'eval_f1': 0.6544000000000001, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 5.2201, 'eval_samples_per_second': 289.843, 'eval_steps_per_second': 36.398, 'epoch': 2.0, 'step': 536}, {'loss': 0.0364, 'grad_norm': 0.28528907895088196, 'learning_rate': 7e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04884575307369232, 'eval_precision': 0.6577777777777778, 'eval_recall': 0.714975845410628, 'eval_f1': 0.6851851851851852, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 5.2949, 'eval_samples_per_second': 285.745, 'eval_steps_per_second': 35.883, 'epoch': 3.0, 'step': 804}, {'loss': 0.03, 'grad_norm': 0.6752567291259766, 'learning_rate': 6e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04789537191390991, 'eval_precision': 0.7031746031746032, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7082334132693845, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 5.1673, 'eval_samples_per_second': 292.803, 'eval_steps_per_second': 36.77, 'epoch': 4.0, 'step': 1072}, {'loss': 0.024, 'grad_norm': 0.4847274720668793, 'learning_rate': 5e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.053630683571100235, 'eval_precision': 0.6419939577039275, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6625097427903353, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 5.191, 'eval_samples_per_second': 291.468, 'eval_steps_per_second': 36.602, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0196, 'grad_norm': 1.1096200942993164, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05453808233141899, 'eval_precision': 0.6621823617339312, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6868217054263566, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 5.1952, 'eval_samples_per_second': 291.228, 'eval_steps_per_second': 36.572, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0167, 'grad_norm': 0.3038788437843323, 'learning_rate': 3e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.054741423577070236, 'eval_precision': 0.6707132018209409, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6906249999999999, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 6.2948, 'eval_samples_per_second': 240.355, 'eval_steps_per_second': 30.183, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0142, 'grad_norm': 0.7545785307884216, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05498635396361351, 'eval_precision': 0.6975308641975309, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7123719464144996, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 5.2864, 'eval_samples_per_second': 286.206, 'eval_steps_per_second': 35.941, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0126, 'grad_norm': 0.8089251518249512, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.0570860393345356, 'eval_precision': 0.6753445635528331, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6923076923076923, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 5.2379, 'eval_samples_per_second': 288.854, 'eval_steps_per_second': 36.274, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0119, 'grad_norm': 0.274061381816864, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.05740262567996979, 'eval_precision': 0.6919504643962848, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7056037884767166, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 5.1773, 'eval_samples_per_second': 292.238, 'eval_steps_per_second': 36.699, 'epoch': 10.0, 'step': 2680}, {'train_runtime': 863.1425, 'train_samples_per_second': 99.288, 'train_steps_per_second': 3.105, 'total_flos': 3268179917358360.0, 'train_loss': 0.03540833325528387, 'epoch': 10.0, 'step': 2680}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9897
  predict_f1                 =     0.7306
  predict_loss               =     0.0416
  predict_precision          =     0.7182
  predict_recall             =     0.7434
  predict_runtime            = 0:00:04.46
  predict_samples_per_second =     280.33
  predict_steps_per_second   =     35.153
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_11.json completed. F1: 0.7306034482758621
/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_21.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_21.json
03081014_elsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2967.23 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2919.92 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:02, 2540.40 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 2698.71 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:01, 2766.96 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 2857.04 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 2855.35 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 2921.38 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2939.96 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2694.81 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 2839.95 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2529.12 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2067.56 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 2907.86 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 1734.85 examples/s]
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_base/checkpoint-1072 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2217, 'grad_norm': 0.3173534572124481, 'learning_rate': 9e-06, 'epoch': 1.0}
{'eval_loss': 0.06231837719678879, 'eval_precision': 0.5813204508856683, 'eval_recall': 0.5813204508856683, 'eval_f1': 0.5813204508856683, 'eval_accuracy': 0.981904873568609, 'eval_runtime': 5.2438, 'eval_samples_per_second': 288.531, 'eval_steps_per_second': 36.233, 'epoch': 1.0}
{'loss': 0.0529, 'grad_norm': 0.31335750222206116, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0}
{'eval_loss': 0.0541459284722805, 'eval_precision': 0.5563689604685212, 'eval_recall': 0.6119162640901772, 'eval_f1': 0.5828220858895706, 'eval_accuracy': 0.9840153202798296, 'eval_runtime': 5.1658, 'eval_samples_per_second': 292.887, 'eval_steps_per_second': 36.78, 'epoch': 2.0}
{'loss': 0.0423, 'grad_norm': 0.4106011390686035, 'learning_rate': 7e-06, 'epoch': 3.0}
{'eval_loss': 0.05117582529783249, 'eval_precision': 0.6214953271028038, 'eval_recall': 0.642512077294686, 'eval_f1': 0.6318289786223279, 'eval_accuracy': 0.9850705436354399, 'eval_runtime': 5.199, 'eval_samples_per_second': 291.015, 'eval_steps_per_second': 36.545, 'epoch': 3.0}
{'loss': 0.0363, 'grad_norm': 0.44804510474205017, 'learning_rate': 6e-06, 'epoch': 4.0}
{'eval_loss': 0.05217327922582626, 'eval_precision': 0.6286594761171033, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6425196850393702, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 5.1747, 'eval_samples_per_second': 292.385, 'eval_steps_per_second': 36.717, 'epoch': 4.0}
{'loss': 0.032, 'grad_norm': 0.34550660848617554, 'learning_rate': 5e-06, 'epoch': 5.0}
{'eval_loss': 0.054907508194446564, 'eval_precision': 0.6041666666666666, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6279969064191802, 'eval_accuracy': 0.9846015554773908, 'eval_runtime': 5.1581, 'eval_samples_per_second': 293.328, 'eval_steps_per_second': 36.836, 'epoch': 5.0}
{'loss': 0.029, 'grad_norm': 0.443815141916275, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0}
{'eval_loss': 0.05758286267518997, 'eval_precision': 0.6259659969088099, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6388012618296531, 'eval_accuracy': 0.9850705436354399, 'eval_runtime': 5.202, 'eval_samples_per_second': 290.848, 'eval_steps_per_second': 36.524, 'epoch': 6.0}
{'loss': 0.0262, 'grad_norm': 0.3765718936920166, 'learning_rate': 3e-06, 'epoch': 7.0}
{'eval_loss': 0.055933475494384766, 'eval_precision': 0.6693037974683544, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6751795690343176, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 5.1703, 'eval_samples_per_second': 292.635, 'eval_steps_per_second': 36.749, 'epoch': 7.0}
{'loss': 0.0235, 'grad_norm': 0.6843019723892212, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 0.055917058140039444, 'eval_precision': 0.6692546583850931, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6814229249011857, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 5.1862, 'eval_samples_per_second': 291.734, 'eval_steps_per_second': 36.635, 'epoch': 8.0}
{'loss': 0.0223, 'grad_norm': 0.5701226592063904, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0}
{'eval_loss': 0.05698684602975845, 'eval_precision': 0.6548536209553159, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6692913385826772, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 5.1709, 'eval_samples_per_second': 292.599, 'eval_steps_per_second': 36.744, 'epoch': 9.0}
{'loss': 0.0212, 'grad_norm': 0.42017340660095215, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.05723470076918602, 'eval_precision': 0.6651017214397497, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6746031746031748, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 5.163, 'eval_samples_per_second': 293.049, 'eval_steps_per_second': 36.801, 'epoch': 10.0}
{'train_runtime': 776.6427, 'train_samples_per_second': 110.347, 'train_steps_per_second': 1.725, 'train_loss': 0.05072583387147135, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0507
  train_runtime            = 0:12:56.64
  train_samples            =       8570
  train_samples_per_second =    110.347
  train_steps_per_second   =      1.725
[{'loss': 0.2217, 'grad_norm': 0.3173534572124481, 'learning_rate': 9e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.06231837719678879, 'eval_precision': 0.5813204508856683, 'eval_recall': 0.5813204508856683, 'eval_f1': 0.5813204508856683, 'eval_accuracy': 0.981904873568609, 'eval_runtime': 5.2438, 'eval_samples_per_second': 288.531, 'eval_steps_per_second': 36.233, 'epoch': 1.0, 'step': 134}, {'loss': 0.0529, 'grad_norm': 0.31335750222206116, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.0541459284722805, 'eval_precision': 0.5563689604685212, 'eval_recall': 0.6119162640901772, 'eval_f1': 0.5828220858895706, 'eval_accuracy': 0.9840153202798296, 'eval_runtime': 5.1658, 'eval_samples_per_second': 292.887, 'eval_steps_per_second': 36.78, 'epoch': 2.0, 'step': 268}, {'loss': 0.0423, 'grad_norm': 0.4106011390686035, 'learning_rate': 7e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.05117582529783249, 'eval_precision': 0.6214953271028038, 'eval_recall': 0.642512077294686, 'eval_f1': 0.6318289786223279, 'eval_accuracy': 0.9850705436354399, 'eval_runtime': 5.199, 'eval_samples_per_second': 291.015, 'eval_steps_per_second': 36.545, 'epoch': 3.0, 'step': 402}, {'loss': 0.0363, 'grad_norm': 0.44804510474205017, 'learning_rate': 6e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05217327922582626, 'eval_precision': 0.6286594761171033, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6425196850393702, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 5.1747, 'eval_samples_per_second': 292.385, 'eval_steps_per_second': 36.717, 'epoch': 4.0, 'step': 536}, {'loss': 0.032, 'grad_norm': 0.34550660848617554, 'learning_rate': 5e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.054907508194446564, 'eval_precision': 0.6041666666666666, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6279969064191802, 'eval_accuracy': 0.9846015554773908, 'eval_runtime': 5.1581, 'eval_samples_per_second': 293.328, 'eval_steps_per_second': 36.836, 'epoch': 5.0, 'step': 670}, {'loss': 0.029, 'grad_norm': 0.443815141916275, 'learning_rate': 4.000000000000001e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05758286267518997, 'eval_precision': 0.6259659969088099, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6388012618296531, 'eval_accuracy': 0.9850705436354399, 'eval_runtime': 5.202, 'eval_samples_per_second': 290.848, 'eval_steps_per_second': 36.524, 'epoch': 6.0, 'step': 804}, {'loss': 0.0262, 'grad_norm': 0.3765718936920166, 'learning_rate': 3e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.055933475494384766, 'eval_precision': 0.6693037974683544, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6751795690343176, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 5.1703, 'eval_samples_per_second': 292.635, 'eval_steps_per_second': 36.749, 'epoch': 7.0, 'step': 938}, {'loss': 0.0235, 'grad_norm': 0.6843019723892212, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.055917058140039444, 'eval_precision': 0.6692546583850931, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6814229249011857, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 5.1862, 'eval_samples_per_second': 291.734, 'eval_steps_per_second': 36.635, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0223, 'grad_norm': 0.5701226592063904, 'learning_rate': 1.0000000000000002e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05698684602975845, 'eval_precision': 0.6548536209553159, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6692913385826772, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 5.1709, 'eval_samples_per_second': 292.599, 'eval_steps_per_second': 36.744, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0212, 'grad_norm': 0.42017340660095215, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05723470076918602, 'eval_precision': 0.6651017214397497, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6746031746031748, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 5.163, 'eval_samples_per_second': 293.049, 'eval_steps_per_second': 36.801, 'epoch': 10.0, 'step': 1340}, {'train_runtime': 776.6427, 'train_samples_per_second': 110.347, 'train_steps_per_second': 1.725, 'total_flos': 3699062705914272.0, 'train_loss': 0.05072583387147135, 'epoch': 10.0, 'step': 1340}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9877
  predict_f1                 =     0.6582
  predict_loss               =     0.0438
  predict_precision          =     0.6379
  predict_recall             =     0.6798
  predict_runtime            = 0:00:04.33
  predict_samples_per_second =    289.119
  predict_steps_per_second   =     36.255
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_21.json completed. F1: 0.6581740976645435
/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_16.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_16.json
03081014_elsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2774.22 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2910.22 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:01, 2793.14 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 2860.10 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:01, 2870.62 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 2925.90 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 2894.82 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 2945.55 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 2957.23 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2837.81 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 2899.39 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2907.97 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2429.03 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 2617.90 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 1772.87 examples/s]
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0814, 'grad_norm': 0.3965211808681488, 'learning_rate': 4.5e-05, 'epoch': 1.0}
{'eval_loss': 0.04965125396847725, 'eval_precision': 0.6178343949044586, 'eval_recall': 0.6247987117552335, 'eval_f1': 0.621297037630104, 'eval_accuracy': 0.9844452260913745, 'eval_runtime': 5.1917, 'eval_samples_per_second': 291.426, 'eval_steps_per_second': 36.597, 'epoch': 1.0}
{'loss': 0.0375, 'grad_norm': 0.36801955103874207, 'learning_rate': 4e-05, 'epoch': 2.0}
{'eval_loss': 0.04791984707117081, 'eval_precision': 0.6555891238670695, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.676539360872954, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 5.1652, 'eval_samples_per_second': 292.922, 'eval_steps_per_second': 36.785, 'epoch': 2.0}
{'loss': 0.025, 'grad_norm': 0.09704462438821793, 'learning_rate': 3.5e-05, 'epoch': 3.0}
{'eval_loss': 0.05251413211226463, 'eval_precision': 0.671664167916042, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6956521739130436, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 5.17, 'eval_samples_per_second': 292.65, 'eval_steps_per_second': 36.751, 'epoch': 3.0}
{'loss': 0.0147, 'grad_norm': 0.8211461305618286, 'learning_rate': 3e-05, 'epoch': 4.0}
{'eval_loss': 0.06134713068604469, 'eval_precision': 0.586608442503639, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6162079510703364, 'eval_accuracy': 0.9844061437448705, 'eval_runtime': 5.1634, 'eval_samples_per_second': 293.026, 'eval_steps_per_second': 36.798, 'epoch': 4.0}
{'loss': 0.0094, 'grad_norm': 0.23010720312595367, 'learning_rate': 2.5e-05, 'epoch': 5.0}
{'eval_loss': 0.06844030320644379, 'eval_precision': 0.6119402985074627, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.635166537567777, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 5.1995, 'eval_samples_per_second': 290.991, 'eval_steps_per_second': 36.542, 'epoch': 5.0}
{'loss': 0.0056, 'grad_norm': 1.2822352647781372, 'learning_rate': 2e-05, 'epoch': 6.0}
{'eval_loss': 0.06677220016717911, 'eval_precision': 0.6330708661417322, 'eval_recall': 0.6473429951690821, 'eval_f1': 0.6401273885350317, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 5.1667, 'eval_samples_per_second': 292.836, 'eval_steps_per_second': 36.774, 'epoch': 6.0}
{'loss': 0.0031, 'grad_norm': 0.19967223703861237, 'learning_rate': 1.5e-05, 'epoch': 7.0}
{'eval_loss': 0.07329391688108444, 'eval_precision': 0.6394453004622496, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6535433070866142, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 5.6392, 'eval_samples_per_second': 268.299, 'eval_steps_per_second': 33.692, 'epoch': 7.0}
{'loss': 0.0016, 'grad_norm': 0.13529431819915771, 'learning_rate': 1e-05, 'epoch': 8.0}
{'eval_loss': 0.07475227117538452, 'eval_precision': 0.629742033383915, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6484375, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 5.1754, 'eval_samples_per_second': 292.346, 'eval_steps_per_second': 36.712, 'epoch': 8.0}
{'loss': 0.0009, 'grad_norm': 0.03668295964598656, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.07700026780366898, 'eval_precision': 0.6485225505443235, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6598101265822784, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 5.1791, 'eval_samples_per_second': 292.136, 'eval_steps_per_second': 36.686, 'epoch': 9.0}
{'loss': 0.0007, 'grad_norm': 0.11372451484203339, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.07702767848968506, 'eval_precision': 0.6408668730650154, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6535122336227308, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 5.2363, 'eval_samples_per_second': 288.943, 'eval_steps_per_second': 36.285, 'epoch': 10.0}
{'train_runtime': 774.9624, 'train_samples_per_second': 110.586, 'train_steps_per_second': 3.458, 'train_loss': 0.01798732809500018, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =      0.018
  train_runtime            = 0:12:54.96
  train_samples            =       8570
  train_samples_per_second =    110.586
  train_steps_per_second   =      3.458
[{'loss': 0.0814, 'grad_norm': 0.3965211808681488, 'learning_rate': 4.5e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04965125396847725, 'eval_precision': 0.6178343949044586, 'eval_recall': 0.6247987117552335, 'eval_f1': 0.621297037630104, 'eval_accuracy': 0.9844452260913745, 'eval_runtime': 5.1917, 'eval_samples_per_second': 291.426, 'eval_steps_per_second': 36.597, 'epoch': 1.0, 'step': 268}, {'loss': 0.0375, 'grad_norm': 0.36801955103874207, 'learning_rate': 4e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04791984707117081, 'eval_precision': 0.6555891238670695, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.676539360872954, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 5.1652, 'eval_samples_per_second': 292.922, 'eval_steps_per_second': 36.785, 'epoch': 2.0, 'step': 536}, {'loss': 0.025, 'grad_norm': 0.09704462438821793, 'learning_rate': 3.5e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05251413211226463, 'eval_precision': 0.671664167916042, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6956521739130436, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 5.17, 'eval_samples_per_second': 292.65, 'eval_steps_per_second': 36.751, 'epoch': 3.0, 'step': 804}, {'loss': 0.0147, 'grad_norm': 0.8211461305618286, 'learning_rate': 3e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.06134713068604469, 'eval_precision': 0.586608442503639, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6162079510703364, 'eval_accuracy': 0.9844061437448705, 'eval_runtime': 5.1634, 'eval_samples_per_second': 293.026, 'eval_steps_per_second': 36.798, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0094, 'grad_norm': 0.23010720312595367, 'learning_rate': 2.5e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06844030320644379, 'eval_precision': 0.6119402985074627, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.635166537567777, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 5.1995, 'eval_samples_per_second': 290.991, 'eval_steps_per_second': 36.542, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0056, 'grad_norm': 1.2822352647781372, 'learning_rate': 2e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06677220016717911, 'eval_precision': 0.6330708661417322, 'eval_recall': 0.6473429951690821, 'eval_f1': 0.6401273885350317, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 5.1667, 'eval_samples_per_second': 292.836, 'eval_steps_per_second': 36.774, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0031, 'grad_norm': 0.19967223703861237, 'learning_rate': 1.5e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07329391688108444, 'eval_precision': 0.6394453004622496, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6535433070866142, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 5.6392, 'eval_samples_per_second': 268.299, 'eval_steps_per_second': 33.692, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0016, 'grad_norm': 0.13529431819915771, 'learning_rate': 1e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07475227117538452, 'eval_precision': 0.629742033383915, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6484375, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 5.1754, 'eval_samples_per_second': 292.346, 'eval_steps_per_second': 36.712, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0009, 'grad_norm': 0.03668295964598656, 'learning_rate': 5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07700026780366898, 'eval_precision': 0.6485225505443235, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6598101265822784, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 5.1791, 'eval_samples_per_second': 292.136, 'eval_steps_per_second': 36.686, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0007, 'grad_norm': 0.11372451484203339, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07702767848968506, 'eval_precision': 0.6408668730650154, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6535122336227308, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 5.2363, 'eval_samples_per_second': 288.943, 'eval_steps_per_second': 36.285, 'epoch': 10.0, 'step': 2680}, {'train_runtime': 774.9624, 'train_samples_per_second': 110.586, 'train_steps_per_second': 3.458, 'total_flos': 3268179917358360.0, 'train_loss': 0.01798732809500018, 'epoch': 10.0, 'step': 2680}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =      0.988
  predict_f1                 =     0.6863
  predict_loss               =     0.0442
  predict_precision          =     0.6599
  predict_recall             =     0.7149
  predict_runtime            = 0:00:04.41
  predict_samples_per_second =    283.287
  predict_steps_per_second   =     35.524
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_16.json completed. F1: 0.6863157894736842
/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_06.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_06.json
03081014_elsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2569.43 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:03, 2141.20 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:02, 2346.37 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 2561.21 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:01, 2673.68 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 2789.98 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 2810.20 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 2885.81 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2909.43 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2571.49 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 2909.02 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2173.24 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 1577.02 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 2905.79 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2043.73 examples/s]
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0689, 'grad_norm': 0.10662124305963516, 'learning_rate': 4.5e-05, 'epoch': 1.0}
{'eval_loss': 0.05047404766082764, 'eval_precision': 0.6270676691729323, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6485225505443235, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 5.4244, 'eval_samples_per_second': 278.925, 'eval_steps_per_second': 35.027, 'epoch': 1.0}
{'loss': 0.0356, 'grad_norm': 0.01935894973576069, 'learning_rate': 4e-05, 'epoch': 2.0}
{'eval_loss': 0.051697008311748505, 'eval_precision': 0.6893687707641196, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.678659035159444, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 5.1845, 'eval_samples_per_second': 291.831, 'eval_steps_per_second': 36.648, 'epoch': 2.0}
{'loss': 0.021, 'grad_norm': 0.007819460704922676, 'learning_rate': 3.5e-05, 'epoch': 3.0}
{'eval_loss': 0.05862559750676155, 'eval_precision': 0.6661514683153014, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6798107255520505, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 5.7034, 'eval_samples_per_second': 265.282, 'eval_steps_per_second': 33.314, 'epoch': 3.0}
{'loss': 0.0142, 'grad_norm': 2.6148037910461426, 'learning_rate': 3e-05, 'epoch': 4.0}
{'eval_loss': 0.06211003288626671, 'eval_precision': 0.6370370370370371, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6635802469135802, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 5.1875, 'eval_samples_per_second': 291.661, 'eval_steps_per_second': 36.626, 'epoch': 4.0}
{'loss': 0.0087, 'grad_norm': 2.2946746349334717, 'learning_rate': 2.5e-05, 'epoch': 5.0}
{'eval_loss': 0.07129467278718948, 'eval_precision': 0.6130952380952381, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6372776488785771, 'eval_accuracy': 0.9846015554773908, 'eval_runtime': 5.1899, 'eval_samples_per_second': 291.525, 'eval_steps_per_second': 36.609, 'epoch': 5.0}
{'loss': 0.0049, 'grad_norm': 0.12692523002624512, 'learning_rate': 2e-05, 'epoch': 6.0}
{'eval_loss': 0.07871196419000626, 'eval_precision': 0.5981735159817352, 'eval_recall': 0.6328502415458938, 'eval_f1': 0.6150234741784038, 'eval_accuracy': 0.9840544026263337, 'eval_runtime': 5.24, 'eval_samples_per_second': 288.742, 'eval_steps_per_second': 36.26, 'epoch': 6.0}
{'loss': 0.0028, 'grad_norm': 0.011599939316511154, 'learning_rate': 1.5e-05, 'epoch': 7.0}
{'eval_loss': 0.07966086268424988, 'eval_precision': 0.6445880452342488, 'eval_recall': 0.642512077294686, 'eval_f1': 0.6435483870967743, 'eval_accuracy': 0.9854222847539766, 'eval_runtime': 5.185, 'eval_samples_per_second': 291.805, 'eval_steps_per_second': 36.644, 'epoch': 7.0}
{'loss': 0.0011, 'grad_norm': 0.009759311564266682, 'learning_rate': 1e-05, 'epoch': 8.0}
{'eval_loss': 0.08139833062887192, 'eval_precision': 0.6436420722135008, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.6518282988871225, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 5.1842, 'eval_samples_per_second': 291.848, 'eval_steps_per_second': 36.65, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.006369404960423708, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.09051893651485443, 'eval_precision': 0.6123076923076923, 'eval_recall': 0.6409017713365539, 'eval_f1': 0.6262785208497247, 'eval_accuracy': 0.9849142142494235, 'eval_runtime': 5.2138, 'eval_samples_per_second': 290.19, 'eval_steps_per_second': 36.442, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.01890227198600769, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.09005719423294067, 'eval_precision': 0.6355140186915887, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6460807600950119, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 5.1834, 'eval_samples_per_second': 291.891, 'eval_steps_per_second': 36.655, 'epoch': 10.0}
{'train_runtime': 855.6385, 'train_samples_per_second': 100.159, 'train_steps_per_second': 6.264, 'train_loss': 0.015805050335935694, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0158
  train_runtime            = 0:14:15.63
  train_samples            =       8570
  train_samples_per_second =    100.159
  train_steps_per_second   =      6.264
[{'loss': 0.0689, 'grad_norm': 0.10662124305963516, 'learning_rate': 4.5e-05, 'epoch': 1.0, 'step': 536}, {'eval_loss': 0.05047404766082764, 'eval_precision': 0.6270676691729323, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6485225505443235, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 5.4244, 'eval_samples_per_second': 278.925, 'eval_steps_per_second': 35.027, 'epoch': 1.0, 'step': 536}, {'loss': 0.0356, 'grad_norm': 0.01935894973576069, 'learning_rate': 4e-05, 'epoch': 2.0, 'step': 1072}, {'eval_loss': 0.051697008311748505, 'eval_precision': 0.6893687707641196, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.678659035159444, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 5.1845, 'eval_samples_per_second': 291.831, 'eval_steps_per_second': 36.648, 'epoch': 2.0, 'step': 1072}, {'loss': 0.021, 'grad_norm': 0.007819460704922676, 'learning_rate': 3.5e-05, 'epoch': 3.0, 'step': 1608}, {'eval_loss': 0.05862559750676155, 'eval_precision': 0.6661514683153014, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6798107255520505, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 5.7034, 'eval_samples_per_second': 265.282, 'eval_steps_per_second': 33.314, 'epoch': 3.0, 'step': 1608}, {'loss': 0.0142, 'grad_norm': 2.6148037910461426, 'learning_rate': 3e-05, 'epoch': 4.0, 'step': 2144}, {'eval_loss': 0.06211003288626671, 'eval_precision': 0.6370370370370371, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6635802469135802, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 5.1875, 'eval_samples_per_second': 291.661, 'eval_steps_per_second': 36.626, 'epoch': 4.0, 'step': 2144}, {'loss': 0.0087, 'grad_norm': 2.2946746349334717, 'learning_rate': 2.5e-05, 'epoch': 5.0, 'step': 2680}, {'eval_loss': 0.07129467278718948, 'eval_precision': 0.6130952380952381, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6372776488785771, 'eval_accuracy': 0.9846015554773908, 'eval_runtime': 5.1899, 'eval_samples_per_second': 291.525, 'eval_steps_per_second': 36.609, 'epoch': 5.0, 'step': 2680}, {'loss': 0.0049, 'grad_norm': 0.12692523002624512, 'learning_rate': 2e-05, 'epoch': 6.0, 'step': 3216}, {'eval_loss': 0.07871196419000626, 'eval_precision': 0.5981735159817352, 'eval_recall': 0.6328502415458938, 'eval_f1': 0.6150234741784038, 'eval_accuracy': 0.9840544026263337, 'eval_runtime': 5.24, 'eval_samples_per_second': 288.742, 'eval_steps_per_second': 36.26, 'epoch': 6.0, 'step': 3216}, {'loss': 0.0028, 'grad_norm': 0.011599939316511154, 'learning_rate': 1.5e-05, 'epoch': 7.0, 'step': 3752}, {'eval_loss': 0.07966086268424988, 'eval_precision': 0.6445880452342488, 'eval_recall': 0.642512077294686, 'eval_f1': 0.6435483870967743, 'eval_accuracy': 0.9854222847539766, 'eval_runtime': 5.185, 'eval_samples_per_second': 291.805, 'eval_steps_per_second': 36.644, 'epoch': 7.0, 'step': 3752}, {'loss': 0.0011, 'grad_norm': 0.009759311564266682, 'learning_rate': 1e-05, 'epoch': 8.0, 'step': 4288}, {'eval_loss': 0.08139833062887192, 'eval_precision': 0.6436420722135008, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.6518282988871225, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 5.1842, 'eval_samples_per_second': 291.848, 'eval_steps_per_second': 36.65, 'epoch': 8.0, 'step': 4288}, {'loss': 0.0005, 'grad_norm': 0.006369404960423708, 'learning_rate': 5e-06, 'epoch': 9.0, 'step': 4824}, {'eval_loss': 0.09051893651485443, 'eval_precision': 0.6123076923076923, 'eval_recall': 0.6409017713365539, 'eval_f1': 0.6262785208497247, 'eval_accuracy': 0.9849142142494235, 'eval_runtime': 5.2138, 'eval_samples_per_second': 290.19, 'eval_steps_per_second': 36.442, 'epoch': 9.0, 'step': 4824}, {'loss': 0.0003, 'grad_norm': 0.01890227198600769, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 5360}, {'eval_loss': 0.09005719423294067, 'eval_precision': 0.6355140186915887, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6460807600950119, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 5.1834, 'eval_samples_per_second': 291.891, 'eval_steps_per_second': 36.655, 'epoch': 10.0, 'step': 5360}, {'train_runtime': 855.6385, 'train_samples_per_second': 100.159, 'train_steps_per_second': 6.264, 'total_flos': 2851119422874276.0, 'train_loss': 0.015805050335935694, 'epoch': 10.0, 'step': 5360}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9881
  predict_f1                 =      0.706
  predict_loss               =     0.0419
  predict_precision          =     0.6795
  predict_recall             =     0.7346
  predict_runtime            = 0:00:04.46
  predict_samples_per_second =    280.641
  predict_steps_per_second   =     35.192
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_06.json completed. F1: 0.7060063224446786
/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_26.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_26.json
03081014_elsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2880.77 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2825.67 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:02, 2641.70 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 2769.30 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:01, 2819.85 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 2894.04 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 2881.06 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 2941.57 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 2956.18 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2794.37 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 2708.39 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 1691.35 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:01<00:00, 1437.00 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 2808.32 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 1713.28 examples/s]
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0984, 'grad_norm': 0.29207345843315125, 'learning_rate': 4.5e-05, 'epoch': 1.0}
{'eval_loss': 0.04891238734126091, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 5.2714, 'eval_samples_per_second': 287.021, 'eval_steps_per_second': 36.044, 'epoch': 1.0}
{'loss': 0.0384, 'grad_norm': 0.2752843201160431, 'learning_rate': 4e-05, 'epoch': 2.0}
{'eval_loss': 0.04599606618285179, 'eval_precision': 0.6248153618906942, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6517719568567026, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 5.1947, 'eval_samples_per_second': 291.259, 'eval_steps_per_second': 36.576, 'epoch': 2.0}
{'loss': 0.0268, 'grad_norm': 0.3601002097129822, 'learning_rate': 3.5e-05, 'epoch': 3.0}
{'eval_loss': 0.04835498705506325, 'eval_precision': 0.6757575757575758, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6963309914129585, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 5.1936, 'eval_samples_per_second': 291.321, 'eval_steps_per_second': 36.584, 'epoch': 3.0}
{'loss': 0.0172, 'grad_norm': 0.3889005482196808, 'learning_rate': 3e-05, 'epoch': 4.0}
{'eval_loss': 0.05345837026834488, 'eval_precision': 0.6468842729970327, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6733590733590734, 'eval_accuracy': 0.9863993434165788, 'eval_runtime': 5.1896, 'eval_samples_per_second': 291.542, 'eval_steps_per_second': 36.611, 'epoch': 4.0}
{'loss': 0.0117, 'grad_norm': 0.2764173448085785, 'learning_rate': 2.5e-05, 'epoch': 5.0}
{'eval_loss': 0.056767839938402176, 'eval_precision': 0.6870967741935484, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6865431103948428, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 5.1924, 'eval_samples_per_second': 291.389, 'eval_steps_per_second': 36.592, 'epoch': 5.0}
{'loss': 0.0078, 'grad_norm': 0.4712076485157013, 'learning_rate': 2e-05, 'epoch': 6.0}
{'eval_loss': 0.06841747462749481, 'eval_precision': 0.597667638483965, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.6273909716908952, 'eval_accuracy': 0.9845624731308867, 'eval_runtime': 5.6909, 'eval_samples_per_second': 265.865, 'eval_steps_per_second': 33.387, 'epoch': 6.0}
{'loss': 0.0052, 'grad_norm': 0.2090490609407425, 'learning_rate': 1.5e-05, 'epoch': 7.0}
{'eval_loss': 0.07121781259775162, 'eval_precision': 0.6213450292397661, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6513409961685824, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 5.1718, 'eval_samples_per_second': 292.547, 'eval_steps_per_second': 36.738, 'epoch': 7.0}
{'loss': 0.0036, 'grad_norm': 0.28536906838417053, 'learning_rate': 1e-05, 'epoch': 8.0}
{'eval_loss': 0.06914933025836945, 'eval_precision': 0.6489859594383776, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6592709984152139, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 5.1783, 'eval_samples_per_second': 292.182, 'eval_steps_per_second': 36.692, 'epoch': 8.0}
{'loss': 0.0026, 'grad_norm': 0.2779790759086609, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.06847944110631943, 'eval_precision': 0.6609642301710731, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6724683544303798, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 5.1809, 'eval_samples_per_second': 292.033, 'eval_steps_per_second': 36.673, 'epoch': 9.0}
{'loss': 0.0018, 'grad_norm': 0.13153745234012604, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.06984594464302063, 'eval_precision': 0.6605222734254992, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6761006289308176, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 5.1733, 'eval_samples_per_second': 292.462, 'eval_steps_per_second': 36.727, 'epoch': 10.0}
{'train_runtime': 784.6508, 'train_samples_per_second': 109.221, 'train_steps_per_second': 1.708, 'train_loss': 0.021341982795231376, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0213
  train_runtime            = 0:13:04.65
  train_samples            =       8570
  train_samples_per_second =    109.221
  train_steps_per_second   =      1.708
[{'loss': 0.0984, 'grad_norm': 0.29207345843315125, 'learning_rate': 4.5e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04891238734126091, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 5.2714, 'eval_samples_per_second': 287.021, 'eval_steps_per_second': 36.044, 'epoch': 1.0, 'step': 134}, {'loss': 0.0384, 'grad_norm': 0.2752843201160431, 'learning_rate': 4e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04599606618285179, 'eval_precision': 0.6248153618906942, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6517719568567026, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 5.1947, 'eval_samples_per_second': 291.259, 'eval_steps_per_second': 36.576, 'epoch': 2.0, 'step': 268}, {'loss': 0.0268, 'grad_norm': 0.3601002097129822, 'learning_rate': 3.5e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04835498705506325, 'eval_precision': 0.6757575757575758, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6963309914129585, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 5.1936, 'eval_samples_per_second': 291.321, 'eval_steps_per_second': 36.584, 'epoch': 3.0, 'step': 402}, {'loss': 0.0172, 'grad_norm': 0.3889005482196808, 'learning_rate': 3e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05345837026834488, 'eval_precision': 0.6468842729970327, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6733590733590734, 'eval_accuracy': 0.9863993434165788, 'eval_runtime': 5.1896, 'eval_samples_per_second': 291.542, 'eval_steps_per_second': 36.611, 'epoch': 4.0, 'step': 536}, {'loss': 0.0117, 'grad_norm': 0.2764173448085785, 'learning_rate': 2.5e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.056767839938402176, 'eval_precision': 0.6870967741935484, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6865431103948428, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 5.1924, 'eval_samples_per_second': 291.389, 'eval_steps_per_second': 36.592, 'epoch': 5.0, 'step': 670}, {'loss': 0.0078, 'grad_norm': 0.4712076485157013, 'learning_rate': 2e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06841747462749481, 'eval_precision': 0.597667638483965, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.6273909716908952, 'eval_accuracy': 0.9845624731308867, 'eval_runtime': 5.6909, 'eval_samples_per_second': 265.865, 'eval_steps_per_second': 33.387, 'epoch': 6.0, 'step': 804}, {'loss': 0.0052, 'grad_norm': 0.2090490609407425, 'learning_rate': 1.5e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.07121781259775162, 'eval_precision': 0.6213450292397661, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6513409961685824, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 5.1718, 'eval_samples_per_second': 292.547, 'eval_steps_per_second': 36.738, 'epoch': 7.0, 'step': 938}, {'loss': 0.0036, 'grad_norm': 0.28536906838417053, 'learning_rate': 1e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.06914933025836945, 'eval_precision': 0.6489859594383776, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6592709984152139, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 5.1783, 'eval_samples_per_second': 292.182, 'eval_steps_per_second': 36.692, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0026, 'grad_norm': 0.2779790759086609, 'learning_rate': 5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06847944110631943, 'eval_precision': 0.6609642301710731, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6724683544303798, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 5.1809, 'eval_samples_per_second': 292.033, 'eval_steps_per_second': 36.673, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0018, 'grad_norm': 0.13153745234012604, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06984594464302063, 'eval_precision': 0.6605222734254992, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6761006289308176, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 5.1733, 'eval_samples_per_second': 292.462, 'eval_steps_per_second': 36.727, 'epoch': 10.0, 'step': 1340}, {'train_runtime': 784.6508, 'train_samples_per_second': 109.221, 'train_steps_per_second': 1.708, 'total_flos': 3699062705914272.0, 'train_loss': 0.021341982795231376, 'epoch': 10.0, 'step': 1340}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9873
  predict_f1                 =     0.6667
  predict_loss               =     0.0435
  predict_precision          =     0.6367
  predict_recall             =     0.6996
  predict_runtime            = 0:00:04.37
  predict_samples_per_second =    285.852
  predict_steps_per_second   =     35.846
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_base_26.json completed. F1: 0.6666666666666666


GPU usage stats:
Job 10901743 completed at Fri Mar 8 12:28:04 CET 2024
