Starting job 10910776 on gpu-12-8 on saga at Sat Mar 9 10:40:40 CET 2024

/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_404.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2983.07 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 3658.42 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4222.47 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5009.77 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5480.19 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6052.53 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6360.98 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6656.79 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5528.29 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 5894.47 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5188.73 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6462.97 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3498.54 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1394, 'grad_norm': 0.6058071255683899, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.056055501103401184, 'eval_precision': 0.593192868719611, 'eval_recall': 0.5893719806763285, 'eval_f1': 0.591276252019386, 'eval_accuracy': 0.9824911087661703, 'eval_runtime': 2.5512, 'eval_samples_per_second': 593.063, 'eval_steps_per_second': 74.476, 'epoch': 1.0}
{'loss': 0.0463, 'grad_norm': 0.30985555052757263, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04981693625450134, 'eval_precision': 0.6377708978328174, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6503551696921862, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.4211, 'eval_samples_per_second': 624.923, 'eval_steps_per_second': 78.477, 'epoch': 2.0}
{'loss': 0.0365, 'grad_norm': 0.3267260491847992, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.05039091408252716, 'eval_precision': 0.6464174454828661, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6571654790182105, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3012, 'eval_samples_per_second': 657.486, 'eval_steps_per_second': 82.566, 'epoch': 3.0}
{'loss': 0.0305, 'grad_norm': 0.3810541033744812, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.05169256031513214, 'eval_precision': 0.6496913580246914, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6635145784081955, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.2531, 'eval_samples_per_second': 671.514, 'eval_steps_per_second': 84.328, 'epoch': 4.0}
{'loss': 0.0249, 'grad_norm': 1.1135002374649048, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05321557819843292, 'eval_precision': 0.6370597243491577, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6530612244897959, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3047, 'eval_samples_per_second': 656.476, 'eval_steps_per_second': 82.439, 'epoch': 5.0}
{'loss': 0.0205, 'grad_norm': 1.0515949726104736, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05264293774962425, 'eval_precision': 0.6676691729323309, 'eval_recall': 0.714975845410628, 'eval_f1': 0.6905132192846034, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.2794, 'eval_samples_per_second': 663.771, 'eval_steps_per_second': 83.355, 'epoch': 6.0}
{'loss': 0.0172, 'grad_norm': 0.33115747570991516, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.055790964514017105, 'eval_precision': 0.6631419939577039, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6843335931410756, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2565, 'eval_samples_per_second': 670.514, 'eval_steps_per_second': 84.202, 'epoch': 7.0}
{'loss': 0.0143, 'grad_norm': 0.9397203922271729, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.06075630336999893, 'eval_precision': 0.6671779141104295, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6834249803613511, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.2622, 'eval_samples_per_second': 668.819, 'eval_steps_per_second': 83.989, 'epoch': 8.0}
{'loss': 0.0124, 'grad_norm': 0.9190934300422668, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06188284233212471, 'eval_precision': 0.6691729323308271, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6920684292379471, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 2.3703, 'eval_samples_per_second': 638.323, 'eval_steps_per_second': 80.16, 'epoch': 9.0}
{'loss': 0.0107, 'grad_norm': 0.21727602183818817, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06319830566644669, 'eval_precision': 0.661144578313253, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6832684824902724, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.2757, 'eval_samples_per_second': 664.848, 'eval_steps_per_second': 83.491, 'epoch': 10.0}
{'loss': 0.0097, 'grad_norm': 0.7537952065467834, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06426382064819336, 'eval_precision': 0.655223880597015, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6800929512006197, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.2405, 'eval_samples_per_second': 675.304, 'eval_steps_per_second': 84.803, 'epoch': 11.0}
{'loss': 0.0088, 'grad_norm': 0.14823517203330994, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06474818289279938, 'eval_precision': 0.6492537313432836, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6738962044926414, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.3037, 'eval_samples_per_second': 656.766, 'eval_steps_per_second': 82.476, 'epoch': 12.0}
{'train_runtime': 400.4048, 'train_samples_per_second': 256.84, 'train_steps_per_second': 8.032, 'train_loss': 0.03092937379037563, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0309
  train_runtime            = 0:06:40.40
  train_samples            =       8570
  train_samples_per_second =     256.84
  train_steps_per_second   =      8.032
[{'loss': 0.1394, 'grad_norm': 0.6058071255683899, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.056055501103401184, 'eval_precision': 0.593192868719611, 'eval_recall': 0.5893719806763285, 'eval_f1': 0.591276252019386, 'eval_accuracy': 0.9824911087661703, 'eval_runtime': 2.5512, 'eval_samples_per_second': 593.063, 'eval_steps_per_second': 74.476, 'epoch': 1.0, 'step': 268}, {'loss': 0.0463, 'grad_norm': 0.30985555052757263, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04981693625450134, 'eval_precision': 0.6377708978328174, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6503551696921862, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.4211, 'eval_samples_per_second': 624.923, 'eval_steps_per_second': 78.477, 'epoch': 2.0, 'step': 536}, {'loss': 0.0365, 'grad_norm': 0.3267260491847992, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05039091408252716, 'eval_precision': 0.6464174454828661, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6571654790182105, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3012, 'eval_samples_per_second': 657.486, 'eval_steps_per_second': 82.566, 'epoch': 3.0, 'step': 804}, {'loss': 0.0305, 'grad_norm': 0.3810541033744812, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05169256031513214, 'eval_precision': 0.6496913580246914, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6635145784081955, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.2531, 'eval_samples_per_second': 671.514, 'eval_steps_per_second': 84.328, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0249, 'grad_norm': 1.1135002374649048, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05321557819843292, 'eval_precision': 0.6370597243491577, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6530612244897959, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3047, 'eval_samples_per_second': 656.476, 'eval_steps_per_second': 82.439, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0205, 'grad_norm': 1.0515949726104736, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05264293774962425, 'eval_precision': 0.6676691729323309, 'eval_recall': 0.714975845410628, 'eval_f1': 0.6905132192846034, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.2794, 'eval_samples_per_second': 663.771, 'eval_steps_per_second': 83.355, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0172, 'grad_norm': 0.33115747570991516, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.055790964514017105, 'eval_precision': 0.6631419939577039, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6843335931410756, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2565, 'eval_samples_per_second': 670.514, 'eval_steps_per_second': 84.202, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0143, 'grad_norm': 0.9397203922271729, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06075630336999893, 'eval_precision': 0.6671779141104295, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6834249803613511, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.2622, 'eval_samples_per_second': 668.819, 'eval_steps_per_second': 83.989, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0124, 'grad_norm': 0.9190934300422668, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06188284233212471, 'eval_precision': 0.6691729323308271, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6920684292379471, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 2.3703, 'eval_samples_per_second': 638.323, 'eval_steps_per_second': 80.16, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0107, 'grad_norm': 0.21727602183818817, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06319830566644669, 'eval_precision': 0.661144578313253, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6832684824902724, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.2757, 'eval_samples_per_second': 664.848, 'eval_steps_per_second': 83.491, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0097, 'grad_norm': 0.7537952065467834, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06426382064819336, 'eval_precision': 0.655223880597015, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6800929512006197, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.2405, 'eval_samples_per_second': 675.304, 'eval_steps_per_second': 84.803, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0088, 'grad_norm': 0.14823517203330994, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06474818289279938, 'eval_precision': 0.6492537313432836, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6738962044926414, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.3037, 'eval_samples_per_second': 656.766, 'eval_steps_per_second': 82.476, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 400.4048, 'train_samples_per_second': 256.84, 'train_steps_per_second': 8.032, 'total_flos': 3919226198440236.0, 'train_loss': 0.03092937379037563, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =      0.988
  predict_f1                 =     0.6617
  predict_loss               =     0.0435
  predict_precision          =     0.6445
  predict_recall             =     0.6798
  predict_runtime            = 0:00:01.88
  predict_samples_per_second =    664.261
  predict_steps_per_second   =     83.298
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_404.json completed. F1: 0.6616862326574173
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_202.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5116.69 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5193.29 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5544.23 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5346.84 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5953.17 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6480.58 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6717.71 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7044.74 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6309.43 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7375.11 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 6053.12 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 5713.91 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4759.14 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1228, 'grad_norm': 0.3267459571361542, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04759020730853081, 'eval_precision': 0.5690690690690691, 'eval_recall': 0.6103059581320451, 'eval_f1': 0.588966588966589, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 4.539, 'eval_samples_per_second': 333.337, 'eval_steps_per_second': 41.86, 'epoch': 1.0}
{'loss': 0.0415, 'grad_norm': 0.9289942383766174, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04656697064638138, 'eval_precision': 0.6873977086743044, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6818181818181819, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 4.5024, 'eval_samples_per_second': 336.045, 'eval_steps_per_second': 42.2, 'epoch': 2.0}
{'loss': 0.0287, 'grad_norm': 0.33479228615760803, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04345317184925079, 'eval_precision': 0.6894904458598726, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6933546837469975, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.5263, 'eval_samples_per_second': 334.267, 'eval_steps_per_second': 41.977, 'epoch': 3.0}
{'loss': 0.0207, 'grad_norm': 0.28227704763412476, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.046071670949459076, 'eval_precision': 0.7184, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7207062600321028, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.5236, 'eval_samples_per_second': 334.466, 'eval_steps_per_second': 42.002, 'epoch': 4.0}
{'loss': 0.0147, 'grad_norm': 0.5840945243835449, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05029971897602081, 'eval_precision': 0.7140600315955766, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7208931419457736, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.5159, 'eval_samples_per_second': 335.041, 'eval_steps_per_second': 42.074, 'epoch': 5.0}
{'loss': 0.011, 'grad_norm': 0.36926913261413574, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05021226406097412, 'eval_precision': 0.7127329192546584, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7256916996047431, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5237, 'eval_samples_per_second': 334.463, 'eval_steps_per_second': 42.001, 'epoch': 6.0}
{'loss': 0.0081, 'grad_norm': 0.7966845631599426, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05230313539505005, 'eval_precision': 0.6956521739130435, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7083003952569169, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.5094, 'eval_samples_per_second': 335.523, 'eval_steps_per_second': 42.134, 'epoch': 7.0}
{'loss': 0.0061, 'grad_norm': 0.12085631489753723, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.055039580911397934, 'eval_precision': 0.7104851330203443, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7206349206349206, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5123, 'eval_samples_per_second': 335.306, 'eval_steps_per_second': 42.107, 'epoch': 8.0}
{'loss': 0.0046, 'grad_norm': 0.6426228284835815, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05742332711815834, 'eval_precision': 0.6987767584097859, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7168627450980393, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6015, 'eval_samples_per_second': 328.809, 'eval_steps_per_second': 41.291, 'epoch': 9.0}
{'loss': 0.0034, 'grad_norm': 0.3135838508605957, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06004715338349342, 'eval_precision': 0.7169811320754716, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7255369928400954, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6463, 'eval_samples_per_second': 325.632, 'eval_steps_per_second': 40.892, 'epoch': 10.0}
{'loss': 0.0031, 'grad_norm': 0.04495861381292343, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06256973743438721, 'eval_precision': 0.7085987261146497, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7125700560448358, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5544, 'eval_samples_per_second': 332.206, 'eval_steps_per_second': 41.718, 'epoch': 11.0}
{'loss': 0.0026, 'grad_norm': 0.10008943825960159, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.062357284128665924, 'eval_precision': 0.7158730158730159, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7210231814548362, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.5663, 'eval_samples_per_second': 331.343, 'eval_steps_per_second': 41.61, 'epoch': 12.0}
{'train_runtime': 968.0511, 'train_samples_per_second': 106.234, 'train_steps_per_second': 3.322, 'train_loss': 0.022266087358567253, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0223
  train_runtime            = 0:16:08.05
  train_samples            =       8570
  train_samples_per_second =    106.234
  train_steps_per_second   =      3.322
[{'loss': 0.1228, 'grad_norm': 0.3267459571361542, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04759020730853081, 'eval_precision': 0.5690690690690691, 'eval_recall': 0.6103059581320451, 'eval_f1': 0.588966588966589, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 4.539, 'eval_samples_per_second': 333.337, 'eval_steps_per_second': 41.86, 'epoch': 1.0, 'step': 268}, {'loss': 0.0415, 'grad_norm': 0.9289942383766174, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04656697064638138, 'eval_precision': 0.6873977086743044, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6818181818181819, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 4.5024, 'eval_samples_per_second': 336.045, 'eval_steps_per_second': 42.2, 'epoch': 2.0, 'step': 536}, {'loss': 0.0287, 'grad_norm': 0.33479228615760803, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04345317184925079, 'eval_precision': 0.6894904458598726, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6933546837469975, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.5263, 'eval_samples_per_second': 334.267, 'eval_steps_per_second': 41.977, 'epoch': 3.0, 'step': 804}, {'loss': 0.0207, 'grad_norm': 0.28227704763412476, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.046071670949459076, 'eval_precision': 0.7184, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7207062600321028, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.5236, 'eval_samples_per_second': 334.466, 'eval_steps_per_second': 42.002, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0147, 'grad_norm': 0.5840945243835449, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05029971897602081, 'eval_precision': 0.7140600315955766, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7208931419457736, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.5159, 'eval_samples_per_second': 335.041, 'eval_steps_per_second': 42.074, 'epoch': 5.0, 'step': 1340}, {'loss': 0.011, 'grad_norm': 0.36926913261413574, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05021226406097412, 'eval_precision': 0.7127329192546584, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7256916996047431, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5237, 'eval_samples_per_second': 334.463, 'eval_steps_per_second': 42.001, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0081, 'grad_norm': 0.7966845631599426, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05230313539505005, 'eval_precision': 0.6956521739130435, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7083003952569169, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.5094, 'eval_samples_per_second': 335.523, 'eval_steps_per_second': 42.134, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0061, 'grad_norm': 0.12085631489753723, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.055039580911397934, 'eval_precision': 0.7104851330203443, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7206349206349206, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5123, 'eval_samples_per_second': 335.306, 'eval_steps_per_second': 42.107, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0046, 'grad_norm': 0.6426228284835815, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05742332711815834, 'eval_precision': 0.6987767584097859, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7168627450980393, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6015, 'eval_samples_per_second': 328.809, 'eval_steps_per_second': 41.291, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0034, 'grad_norm': 0.3135838508605957, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06004715338349342, 'eval_precision': 0.7169811320754716, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7255369928400954, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6463, 'eval_samples_per_second': 325.632, 'eval_steps_per_second': 40.892, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0031, 'grad_norm': 0.04495861381292343, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06256973743438721, 'eval_precision': 0.7085987261146497, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7125700560448358, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5544, 'eval_samples_per_second': 332.206, 'eval_steps_per_second': 41.718, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0026, 'grad_norm': 0.10008943825960159, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.062357284128665924, 'eval_precision': 0.7158730158730159, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7210231814548362, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.5663, 'eval_samples_per_second': 331.343, 'eval_steps_per_second': 41.61, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 968.0511, 'train_samples_per_second': 106.234, 'train_steps_per_second': 3.322, 'total_flos': 1.1358393195545172e+16, 'train_loss': 0.022266087358567253, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9898
  predict_f1                 =     0.7029
  predict_loss               =     0.0395
  predict_precision          =     0.6832
  predict_recall             =     0.7237
  predict_runtime            = 0:00:04.63
  predict_samples_per_second =    270.319
  predict_steps_per_second   =     33.898
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_202.json completed. F1: 0.7028753993610224
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_101.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:01<00:12, 589.66 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:01<00:05, 1153.04 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:02<00:03, 1779.55 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:02<00:02, 2270.88 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:02<00:01, 3026.30 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:00, 3831.08 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:02<00:00, 4534.04 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:02<00:00, 5243.95 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:03<00:00, 2602.63 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 3770.78 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:01<00:00, 727.63 examples/s] Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:02<00:00, 606.64 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7425.90 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2411.23 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0906, 'grad_norm': 0.30562084913253784, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.045549970120191574, 'eval_precision': 0.6292834890965732, 'eval_recall': 0.6505636070853462, 'eval_f1': 0.6397466349960411, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 31.4062, 'eval_samples_per_second': 48.175, 'eval_steps_per_second': 6.05, 'epoch': 1.0}
{'loss': 0.0349, 'grad_norm': 0.19942183792591095, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.047010160982608795, 'eval_precision': 0.6282051282051282, 'eval_recall': 0.6312399355877617, 'eval_f1': 0.629718875502008, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 4.7004, 'eval_samples_per_second': 321.885, 'eval_steps_per_second': 40.422, 'epoch': 2.0}
{'loss': 0.0226, 'grad_norm': 0.39812806248664856, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05024119094014168, 'eval_precision': 0.7264, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7287319422150884, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.5125, 'eval_samples_per_second': 335.29, 'eval_steps_per_second': 42.105, 'epoch': 3.0}
{'loss': 0.0126, 'grad_norm': 0.23682239651679993, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.0493653230369091, 'eval_precision': 0.6326836581709145, 'eval_recall': 0.679549114331723, 'eval_f1': 0.65527950310559, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 4.5469, 'eval_samples_per_second': 332.758, 'eval_steps_per_second': 41.787, 'epoch': 4.0}
{'loss': 0.0069, 'grad_norm': 0.4617382884025574, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.060030609369277954, 'eval_precision': 0.6763358778625954, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6943573667711598, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.5213, 'eval_samples_per_second': 334.639, 'eval_steps_per_second': 42.023, 'epoch': 5.0}
{'loss': 0.0042, 'grad_norm': 0.360665500164032, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06387045979499817, 'eval_precision': 0.6740506329113924, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6799680766161215, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 16.4664, 'eval_samples_per_second': 91.884, 'eval_steps_per_second': 11.539, 'epoch': 6.0}
{'loss': 0.0022, 'grad_norm': 0.0826411098241806, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06467026472091675, 'eval_precision': 0.6772727272727272, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6978922716627635, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5334, 'eval_samples_per_second': 333.743, 'eval_steps_per_second': 41.911, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.05039992183446884, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.0673806369304657, 'eval_precision': 0.6802507836990596, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6894360603653694, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.5533, 'eval_samples_per_second': 332.287, 'eval_steps_per_second': 41.728, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.021233079954981804, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06845465302467346, 'eval_precision': 0.6744186046511628, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6872037914691943, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 4.5248, 'eval_samples_per_second': 334.379, 'eval_steps_per_second': 41.991, 'epoch': 9.0}
{'loss': 0.0004, 'grad_norm': 0.017103023827075958, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07309500873088837, 'eval_precision': 0.6842923794712286, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6962025316455697, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.4958, 'eval_samples_per_second': 336.534, 'eval_steps_per_second': 42.261, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.004376696422696114, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07497724890708923, 'eval_precision': 0.6932707355242567, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7031746031746031, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.5251, 'eval_samples_per_second': 334.355, 'eval_steps_per_second': 41.988, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.014579365029931068, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07496830821037292, 'eval_precision': 0.6949685534591195, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7032617342879872, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.5828, 'eval_samples_per_second': 330.149, 'eval_steps_per_second': 41.46, 'epoch': 12.0}
{'train_runtime': 1310.2759, 'train_samples_per_second': 78.487, 'train_steps_per_second': 1.227, 'train_loss': 0.014699745875787897, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0147
  train_runtime            = 0:21:50.27
  train_samples            =       8570
  train_samples_per_second =     78.487
  train_steps_per_second   =      1.227
[{'loss': 0.0906, 'grad_norm': 0.30562084913253784, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.045549970120191574, 'eval_precision': 0.6292834890965732, 'eval_recall': 0.6505636070853462, 'eval_f1': 0.6397466349960411, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 31.4062, 'eval_samples_per_second': 48.175, 'eval_steps_per_second': 6.05, 'epoch': 1.0, 'step': 134}, {'loss': 0.0349, 'grad_norm': 0.19942183792591095, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.047010160982608795, 'eval_precision': 0.6282051282051282, 'eval_recall': 0.6312399355877617, 'eval_f1': 0.629718875502008, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 4.7004, 'eval_samples_per_second': 321.885, 'eval_steps_per_second': 40.422, 'epoch': 2.0, 'step': 268}, {'loss': 0.0226, 'grad_norm': 0.39812806248664856, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.05024119094014168, 'eval_precision': 0.7264, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7287319422150884, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.5125, 'eval_samples_per_second': 335.29, 'eval_steps_per_second': 42.105, 'epoch': 3.0, 'step': 402}, {'loss': 0.0126, 'grad_norm': 0.23682239651679993, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.0493653230369091, 'eval_precision': 0.6326836581709145, 'eval_recall': 0.679549114331723, 'eval_f1': 0.65527950310559, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 4.5469, 'eval_samples_per_second': 332.758, 'eval_steps_per_second': 41.787, 'epoch': 4.0, 'step': 536}, {'loss': 0.0069, 'grad_norm': 0.4617382884025574, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.060030609369277954, 'eval_precision': 0.6763358778625954, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6943573667711598, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.5213, 'eval_samples_per_second': 334.639, 'eval_steps_per_second': 42.023, 'epoch': 5.0, 'step': 670}, {'loss': 0.0042, 'grad_norm': 0.360665500164032, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06387045979499817, 'eval_precision': 0.6740506329113924, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6799680766161215, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 16.4664, 'eval_samples_per_second': 91.884, 'eval_steps_per_second': 11.539, 'epoch': 6.0, 'step': 804}, {'loss': 0.0022, 'grad_norm': 0.0826411098241806, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06467026472091675, 'eval_precision': 0.6772727272727272, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6978922716627635, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5334, 'eval_samples_per_second': 333.743, 'eval_steps_per_second': 41.911, 'epoch': 7.0, 'step': 938}, {'loss': 0.001, 'grad_norm': 0.05039992183446884, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.0673806369304657, 'eval_precision': 0.6802507836990596, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6894360603653694, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.5533, 'eval_samples_per_second': 332.287, 'eval_steps_per_second': 41.728, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0005, 'grad_norm': 0.021233079954981804, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06845465302467346, 'eval_precision': 0.6744186046511628, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6872037914691943, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 4.5248, 'eval_samples_per_second': 334.379, 'eval_steps_per_second': 41.991, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0004, 'grad_norm': 0.017103023827075958, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07309500873088837, 'eval_precision': 0.6842923794712286, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6962025316455697, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.4958, 'eval_samples_per_second': 336.534, 'eval_steps_per_second': 42.261, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0002, 'grad_norm': 0.004376696422696114, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07497724890708923, 'eval_precision': 0.6932707355242567, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7031746031746031, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.5251, 'eval_samples_per_second': 334.355, 'eval_steps_per_second': 41.988, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0002, 'grad_norm': 0.014579365029931068, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07496830821037292, 'eval_precision': 0.6949685534591195, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7032617342879872, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.5828, 'eval_samples_per_second': 330.149, 'eval_steps_per_second': 41.46, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1310.2759, 'train_samples_per_second': 78.487, 'train_steps_per_second': 1.227, 'total_flos': 1.2913056233591976e+16, 'train_loss': 0.014699745875787897, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9875
  predict_f1                 =     0.6437
  predict_loss               =     0.0442
  predict_precision          =     0.6321
  predict_recall             =     0.6557
  predict_runtime            = 0:00:03.90
  predict_samples_per_second =    320.423
  predict_steps_per_second   =     40.181
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_101.json completed. F1: 0.643702906350915
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_303.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3506.42 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4957.97 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5887.68 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6422.15 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6733.33 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7040.57 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7096.01 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7280.35 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6302.28 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6389.93 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5571.16 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 5923.97 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4257.70 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1464, 'grad_norm': 0.2588278353214264, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05626082047820091, 'eval_precision': 0.5777777777777777, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.6018518518518519, 'eval_accuracy': 0.9826865204986908, 'eval_runtime': 2.365, 'eval_samples_per_second': 639.759, 'eval_steps_per_second': 80.34, 'epoch': 1.0}
{'loss': 0.0468, 'grad_norm': 1.013866901397705, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04897955432534218, 'eval_precision': 0.6360902255639098, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6578538102643857, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.2925, 'eval_samples_per_second': 659.974, 'eval_steps_per_second': 82.878, 'epoch': 2.0}
{'loss': 0.0363, 'grad_norm': 0.3942541778087616, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.048713568598032, 'eval_precision': 0.6784, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.680577849117175, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.2394, 'eval_samples_per_second': 675.64, 'eval_steps_per_second': 84.846, 'epoch': 3.0}
{'loss': 0.0294, 'grad_norm': 0.41111868619918823, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04734041169285774, 'eval_precision': 0.6904024767801857, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7040252565114443, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 2.228, 'eval_samples_per_second': 679.099, 'eval_steps_per_second': 85.28, 'epoch': 4.0}
{'loss': 0.0232, 'grad_norm': 0.40865206718444824, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05274093896150589, 'eval_precision': 0.6707132018209409, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6906249999999999, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2437, 'eval_samples_per_second': 674.342, 'eval_steps_per_second': 84.683, 'epoch': 5.0}
{'loss': 0.0185, 'grad_norm': 0.5617530941963196, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05502965673804283, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7027027027027027, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.2483, 'eval_samples_per_second': 672.959, 'eval_steps_per_second': 84.509, 'epoch': 6.0}
{'loss': 0.015, 'grad_norm': 0.2902010381221771, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05827932432293892, 'eval_precision': 0.6873065015479877, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7008681925808997, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 2.27, 'eval_samples_per_second': 666.512, 'eval_steps_per_second': 83.699, 'epoch': 7.0}
{'loss': 0.0124, 'grad_norm': 0.382650226354599, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.06172427162528038, 'eval_precision': 0.6615620214395099, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6781789638932496, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 2.4137, 'eval_samples_per_second': 626.83, 'eval_steps_per_second': 78.716, 'epoch': 8.0}
{'loss': 0.0107, 'grad_norm': 0.18641839921474457, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06356014311313629, 'eval_precision': 0.6793313069908815, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6989835809225957, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2252, 'eval_samples_per_second': 679.944, 'eval_steps_per_second': 85.386, 'epoch': 9.0}
{'loss': 0.0093, 'grad_norm': 0.011707774363458157, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06351184844970703, 'eval_precision': 0.6856702619414484, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7007874015748031, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 2.2873, 'eval_samples_per_second': 661.476, 'eval_steps_per_second': 83.067, 'epoch': 10.0}
{'loss': 0.0084, 'grad_norm': 0.9437680244445801, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06431838870048523, 'eval_precision': 0.691358024691358, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.706067769897557, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 2.238, 'eval_samples_per_second': 676.037, 'eval_steps_per_second': 84.896, 'epoch': 11.0}
{'loss': 0.0076, 'grad_norm': 0.28958627581596375, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06423784047365189, 'eval_precision': 0.6965944272445821, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7103393843725335, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.2341, 'eval_samples_per_second': 677.233, 'eval_steps_per_second': 85.046, 'epoch': 12.0}
{'train_runtime': 422.3413, 'train_samples_per_second': 243.5, 'train_steps_per_second': 7.615, 'train_loss': 0.030331134499602057, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0303
  train_runtime            = 0:07:02.34
  train_samples            =       8570
  train_samples_per_second =      243.5
  train_steps_per_second   =      7.615
[{'loss': 0.1464, 'grad_norm': 0.2588278353214264, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05626082047820091, 'eval_precision': 0.5777777777777777, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.6018518518518519, 'eval_accuracy': 0.9826865204986908, 'eval_runtime': 2.365, 'eval_samples_per_second': 639.759, 'eval_steps_per_second': 80.34, 'epoch': 1.0, 'step': 268}, {'loss': 0.0468, 'grad_norm': 1.013866901397705, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04897955432534218, 'eval_precision': 0.6360902255639098, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6578538102643857, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.2925, 'eval_samples_per_second': 659.974, 'eval_steps_per_second': 82.878, 'epoch': 2.0, 'step': 536}, {'loss': 0.0363, 'grad_norm': 0.3942541778087616, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.048713568598032, 'eval_precision': 0.6784, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.680577849117175, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.2394, 'eval_samples_per_second': 675.64, 'eval_steps_per_second': 84.846, 'epoch': 3.0, 'step': 804}, {'loss': 0.0294, 'grad_norm': 0.41111868619918823, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04734041169285774, 'eval_precision': 0.6904024767801857, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7040252565114443, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 2.228, 'eval_samples_per_second': 679.099, 'eval_steps_per_second': 85.28, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0232, 'grad_norm': 0.40865206718444824, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05274093896150589, 'eval_precision': 0.6707132018209409, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6906249999999999, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2437, 'eval_samples_per_second': 674.342, 'eval_steps_per_second': 84.683, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0185, 'grad_norm': 0.5617530941963196, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05502965673804283, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7027027027027027, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.2483, 'eval_samples_per_second': 672.959, 'eval_steps_per_second': 84.509, 'epoch': 6.0, 'step': 1608}, {'loss': 0.015, 'grad_norm': 0.2902010381221771, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05827932432293892, 'eval_precision': 0.6873065015479877, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7008681925808997, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 2.27, 'eval_samples_per_second': 666.512, 'eval_steps_per_second': 83.699, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0124, 'grad_norm': 0.382650226354599, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06172427162528038, 'eval_precision': 0.6615620214395099, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6781789638932496, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 2.4137, 'eval_samples_per_second': 626.83, 'eval_steps_per_second': 78.716, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0107, 'grad_norm': 0.18641839921474457, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06356014311313629, 'eval_precision': 0.6793313069908815, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6989835809225957, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2252, 'eval_samples_per_second': 679.944, 'eval_steps_per_second': 85.386, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0093, 'grad_norm': 0.011707774363458157, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06351184844970703, 'eval_precision': 0.6856702619414484, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7007874015748031, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 2.2873, 'eval_samples_per_second': 661.476, 'eval_steps_per_second': 83.067, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0084, 'grad_norm': 0.9437680244445801, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06431838870048523, 'eval_precision': 0.691358024691358, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.706067769897557, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 2.238, 'eval_samples_per_second': 676.037, 'eval_steps_per_second': 84.896, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0076, 'grad_norm': 0.28958627581596375, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06423784047365189, 'eval_precision': 0.6965944272445821, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7103393843725335, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.2341, 'eval_samples_per_second': 677.233, 'eval_steps_per_second': 85.046, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 422.3413, 'train_samples_per_second': 243.5, 'train_steps_per_second': 7.615, 'total_flos': 3923819679226236.0, 'train_loss': 0.030331134499602057, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =      0.989
  predict_f1                 =     0.7193
  predict_loss               =     0.0432
  predict_precision          =     0.7006
  predict_recall             =      0.739
  predict_runtime            = 0:00:01.86
  predict_samples_per_second =    672.309
  predict_steps_per_second   =     84.307
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_303.json completed. F1: 0.71931696905016
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_505.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5448.83 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5719.01 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6598.59 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6560.37 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6855.69 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7165.99 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7231.74 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7400.01 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6542.24 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6067.74 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3522.94 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7429.82 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3408.69 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1171, 'grad_norm': 1.8498014211654663, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04850706085562706, 'eval_precision': 0.594553706505295, 'eval_recall': 0.6328502415458938, 'eval_f1': 0.6131045241809673, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 4.9561, 'eval_samples_per_second': 305.281, 'eval_steps_per_second': 38.337, 'epoch': 1.0}
{'loss': 0.0417, 'grad_norm': 0.9321057200431824, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04251226782798767, 'eval_precision': 0.7006472491909385, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6989507667473768, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.5351, 'eval_samples_per_second': 333.616, 'eval_steps_per_second': 41.895, 'epoch': 2.0}
{'loss': 0.0295, 'grad_norm': 0.5915159583091736, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.041426561772823334, 'eval_precision': 0.6943573667711599, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7037331215250199, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.5193, 'eval_samples_per_second': 334.787, 'eval_steps_per_second': 42.042, 'epoch': 3.0}
{'loss': 0.0212, 'grad_norm': 0.7535667419433594, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.045748889446258545, 'eval_precision': 0.7135922330097088, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7118644067796611, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.5246, 'eval_samples_per_second': 334.396, 'eval_steps_per_second': 41.993, 'epoch': 4.0}
{'loss': 0.0143, 'grad_norm': 0.08582263439893723, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04914119839668274, 'eval_precision': 0.6909667194928685, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6964856230031949, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.5206, 'eval_samples_per_second': 334.688, 'eval_steps_per_second': 42.03, 'epoch': 5.0}
{'loss': 0.0107, 'grad_norm': 0.49346718192100525, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.0572090744972229, 'eval_precision': 0.7184942716857611, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7126623376623377, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.5686, 'eval_samples_per_second': 331.172, 'eval_steps_per_second': 41.588, 'epoch': 6.0}
{'loss': 0.0075, 'grad_norm': 0.26156195998191833, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05503186210989952, 'eval_precision': 0.6889580093312597, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7009493670886074, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.5462, 'eval_samples_per_second': 332.808, 'eval_steps_per_second': 41.793, 'epoch': 7.0}
{'loss': 0.0058, 'grad_norm': 0.23544646799564362, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.058890581130981445, 'eval_precision': 0.6911076443057722, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7020602218700476, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.5041, 'eval_samples_per_second': 335.919, 'eval_steps_per_second': 42.184, 'epoch': 8.0}
{'loss': 0.0042, 'grad_norm': 0.24834762513637543, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06345043331384659, 'eval_precision': 0.6817472698907956, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6925515055467512, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.5129, 'eval_samples_per_second': 335.264, 'eval_steps_per_second': 42.102, 'epoch': 9.0}
{'loss': 0.0033, 'grad_norm': 0.0421917587518692, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06317049264907837, 'eval_precision': 0.6833073322932918, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.694136291600634, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.9632, 'eval_samples_per_second': 304.843, 'eval_steps_per_second': 38.282, 'epoch': 10.0}
{'loss': 0.0026, 'grad_norm': 0.009755220264196396, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06584686785936356, 'eval_precision': 0.6897637795275591, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.697452229299363, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.522, 'eval_samples_per_second': 334.586, 'eval_steps_per_second': 42.017, 'epoch': 11.0}
{'loss': 0.002, 'grad_norm': 0.3021848797798157, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06672777235507965, 'eval_precision': 0.6907378335949764, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6995230524642289, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.5836, 'eval_samples_per_second': 330.093, 'eval_steps_per_second': 41.453, 'epoch': 12.0}
{'train_runtime': 969.364, 'train_samples_per_second': 106.09, 'train_steps_per_second': 3.318, 'train_loss': 0.02165565306126182, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0217
  train_runtime            = 0:16:09.36
  train_samples            =       8570
  train_samples_per_second =     106.09
  train_steps_per_second   =      3.318
[{'loss': 0.1171, 'grad_norm': 1.8498014211654663, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04850706085562706, 'eval_precision': 0.594553706505295, 'eval_recall': 0.6328502415458938, 'eval_f1': 0.6131045241809673, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 4.9561, 'eval_samples_per_second': 305.281, 'eval_steps_per_second': 38.337, 'epoch': 1.0, 'step': 268}, {'loss': 0.0417, 'grad_norm': 0.9321057200431824, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04251226782798767, 'eval_precision': 0.7006472491909385, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6989507667473768, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.5351, 'eval_samples_per_second': 333.616, 'eval_steps_per_second': 41.895, 'epoch': 2.0, 'step': 536}, {'loss': 0.0295, 'grad_norm': 0.5915159583091736, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.041426561772823334, 'eval_precision': 0.6943573667711599, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7037331215250199, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.5193, 'eval_samples_per_second': 334.787, 'eval_steps_per_second': 42.042, 'epoch': 3.0, 'step': 804}, {'loss': 0.0212, 'grad_norm': 0.7535667419433594, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.045748889446258545, 'eval_precision': 0.7135922330097088, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7118644067796611, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.5246, 'eval_samples_per_second': 334.396, 'eval_steps_per_second': 41.993, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0143, 'grad_norm': 0.08582263439893723, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.04914119839668274, 'eval_precision': 0.6909667194928685, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6964856230031949, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.5206, 'eval_samples_per_second': 334.688, 'eval_steps_per_second': 42.03, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0107, 'grad_norm': 0.49346718192100525, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.0572090744972229, 'eval_precision': 0.7184942716857611, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7126623376623377, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.5686, 'eval_samples_per_second': 331.172, 'eval_steps_per_second': 41.588, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0075, 'grad_norm': 0.26156195998191833, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05503186210989952, 'eval_precision': 0.6889580093312597, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7009493670886074, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.5462, 'eval_samples_per_second': 332.808, 'eval_steps_per_second': 41.793, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0058, 'grad_norm': 0.23544646799564362, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.058890581130981445, 'eval_precision': 0.6911076443057722, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7020602218700476, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.5041, 'eval_samples_per_second': 335.919, 'eval_steps_per_second': 42.184, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0042, 'grad_norm': 0.24834762513637543, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06345043331384659, 'eval_precision': 0.6817472698907956, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6925515055467512, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.5129, 'eval_samples_per_second': 335.264, 'eval_steps_per_second': 42.102, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0033, 'grad_norm': 0.0421917587518692, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06317049264907837, 'eval_precision': 0.6833073322932918, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.694136291600634, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.9632, 'eval_samples_per_second': 304.843, 'eval_steps_per_second': 38.282, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0026, 'grad_norm': 0.009755220264196396, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06584686785936356, 'eval_precision': 0.6897637795275591, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.697452229299363, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.522, 'eval_samples_per_second': 334.586, 'eval_steps_per_second': 42.017, 'epoch': 11.0, 'step': 2948}, {'loss': 0.002, 'grad_norm': 0.3021848797798157, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06672777235507965, 'eval_precision': 0.6907378335949764, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6995230524642289, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.5836, 'eval_samples_per_second': 330.093, 'eval_steps_per_second': 41.453, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 969.364, 'train_samples_per_second': 106.09, 'train_steps_per_second': 3.318, 'total_flos': 1.138674864612978e+16, 'train_loss': 0.02165565306126182, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9899
  predict_f1                 =     0.7099
  predict_loss               =     0.0391
  predict_precision          =     0.6794
  predict_recall             =     0.7434
  predict_runtime            = 0:00:03.83
  predict_samples_per_second =    326.102
  predict_steps_per_second   =     40.893
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_505.json completed. F1: 0.7099476439790576
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_101.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5116.55 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5892.14 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6470.54 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6868.61 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 7023.54 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7149.10 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7023.42 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7116.01 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6475.80 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6510.78 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3896.79 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7225.27 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4748.23 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-1072 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1035, 'grad_norm': 0.24869053065776825, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04937349259853363, 'eval_precision': 0.6603174603174603, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6650679456434853, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 27.6397, 'eval_samples_per_second': 54.74, 'eval_steps_per_second': 6.874, 'epoch': 1.0}
{'loss': 0.039, 'grad_norm': 0.2514505088329315, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04722631350159645, 'eval_precision': 0.6456456456456456, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6682206682206682, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.2497, 'eval_samples_per_second': 672.528, 'eval_steps_per_second': 84.455, 'epoch': 2.0}
{'loss': 0.0267, 'grad_norm': 0.3584924042224884, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05120410397648811, 'eval_precision': 0.6592261904761905, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6852281515854602, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.2367, 'eval_samples_per_second': 676.43, 'eval_steps_per_second': 84.945, 'epoch': 3.0}
{'loss': 0.0175, 'grad_norm': 0.31396061182022095, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.054564643651247025, 'eval_precision': 0.6743119266055045, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.691764705882353, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.236, 'eval_samples_per_second': 676.64, 'eval_steps_per_second': 84.971, 'epoch': 4.0}
{'loss': 0.0117, 'grad_norm': 0.24030901491641998, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.0657915472984314, 'eval_precision': 0.618978102189781, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6493108728943339, 'eval_accuracy': 0.9847188025169031, 'eval_runtime': 2.2323, 'eval_samples_per_second': 677.787, 'eval_steps_per_second': 85.115, 'epoch': 5.0}
{'loss': 0.0084, 'grad_norm': 0.41023269295692444, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.0698263943195343, 'eval_precision': 0.6498422712933754, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6565737051792828, 'eval_accuracy': 0.9856958611795053, 'eval_runtime': 2.2276, 'eval_samples_per_second': 679.208, 'eval_steps_per_second': 85.294, 'epoch': 6.0}
{'loss': 0.0056, 'grad_norm': 0.17511917650699615, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07334565371274948, 'eval_precision': 0.6257668711656442, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.641005498821681, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.2481, 'eval_samples_per_second': 673.018, 'eval_steps_per_second': 84.516, 'epoch': 7.0}
{'loss': 0.0032, 'grad_norm': 0.5092970728874207, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.08019769191741943, 'eval_precision': 0.6197604790419161, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6423584173778122, 'eval_accuracy': 0.9849923789424317, 'eval_runtime': 2.2348, 'eval_samples_per_second': 677.029, 'eval_steps_per_second': 85.02, 'epoch': 8.0}
{'loss': 0.0024, 'grad_norm': 0.2527483105659485, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08093279600143433, 'eval_precision': 0.5990990990990991, 'eval_recall': 0.642512077294686, 'eval_f1': 0.62004662004662, 'eval_accuracy': 0.9847578848634072, 'eval_runtime': 2.2389, 'eval_samples_per_second': 675.788, 'eval_steps_per_second': 84.864, 'epoch': 9.0}
{'loss': 0.0017, 'grad_norm': 0.07792442291975021, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08359292894601822, 'eval_precision': 0.6391437308868502, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6556862745098039, 'eval_accuracy': 0.9854222847539766, 'eval_runtime': 2.2316, 'eval_samples_per_second': 677.974, 'eval_steps_per_second': 85.139, 'epoch': 10.0}
{'loss': 0.001, 'grad_norm': 0.161637544631958, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08509054780006409, 'eval_precision': 0.6417682926829268, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6593578700078307, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 2.2407, 'eval_samples_per_second': 675.22, 'eval_steps_per_second': 84.793, 'epoch': 11.0}
{'loss': 0.0009, 'grad_norm': 0.08663606643676758, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08513770997524261, 'eval_precision': 0.6448170731707317, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6624902114330461, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 2.3399, 'eval_samples_per_second': 646.622, 'eval_steps_per_second': 81.202, 'epoch': 12.0}
{'train_runtime': 420.1655, 'train_samples_per_second': 244.761, 'train_steps_per_second': 3.827, 'train_loss': 0.018482027112029085, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0185
  train_runtime            = 0:07:00.16
  train_samples            =       8570
  train_samples_per_second =    244.761
  train_steps_per_second   =      3.827
[{'loss': 0.1035, 'grad_norm': 0.24869053065776825, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04937349259853363, 'eval_precision': 0.6603174603174603, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6650679456434853, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 27.6397, 'eval_samples_per_second': 54.74, 'eval_steps_per_second': 6.874, 'epoch': 1.0, 'step': 134}, {'loss': 0.039, 'grad_norm': 0.2514505088329315, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04722631350159645, 'eval_precision': 0.6456456456456456, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6682206682206682, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.2497, 'eval_samples_per_second': 672.528, 'eval_steps_per_second': 84.455, 'epoch': 2.0, 'step': 268}, {'loss': 0.0267, 'grad_norm': 0.3584924042224884, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.05120410397648811, 'eval_precision': 0.6592261904761905, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6852281515854602, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.2367, 'eval_samples_per_second': 676.43, 'eval_steps_per_second': 84.945, 'epoch': 3.0, 'step': 402}, {'loss': 0.0175, 'grad_norm': 0.31396061182022095, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.054564643651247025, 'eval_precision': 0.6743119266055045, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.691764705882353, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.236, 'eval_samples_per_second': 676.64, 'eval_steps_per_second': 84.971, 'epoch': 4.0, 'step': 536}, {'loss': 0.0117, 'grad_norm': 0.24030901491641998, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.0657915472984314, 'eval_precision': 0.618978102189781, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6493108728943339, 'eval_accuracy': 0.9847188025169031, 'eval_runtime': 2.2323, 'eval_samples_per_second': 677.787, 'eval_steps_per_second': 85.115, 'epoch': 5.0, 'step': 670}, {'loss': 0.0084, 'grad_norm': 0.41023269295692444, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.0698263943195343, 'eval_precision': 0.6498422712933754, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6565737051792828, 'eval_accuracy': 0.9856958611795053, 'eval_runtime': 2.2276, 'eval_samples_per_second': 679.208, 'eval_steps_per_second': 85.294, 'epoch': 6.0, 'step': 804}, {'loss': 0.0056, 'grad_norm': 0.17511917650699615, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.07334565371274948, 'eval_precision': 0.6257668711656442, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.641005498821681, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.2481, 'eval_samples_per_second': 673.018, 'eval_steps_per_second': 84.516, 'epoch': 7.0, 'step': 938}, {'loss': 0.0032, 'grad_norm': 0.5092970728874207, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.08019769191741943, 'eval_precision': 0.6197604790419161, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6423584173778122, 'eval_accuracy': 0.9849923789424317, 'eval_runtime': 2.2348, 'eval_samples_per_second': 677.029, 'eval_steps_per_second': 85.02, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0024, 'grad_norm': 0.2527483105659485, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.08093279600143433, 'eval_precision': 0.5990990990990991, 'eval_recall': 0.642512077294686, 'eval_f1': 0.62004662004662, 'eval_accuracy': 0.9847578848634072, 'eval_runtime': 2.2389, 'eval_samples_per_second': 675.788, 'eval_steps_per_second': 84.864, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0017, 'grad_norm': 0.07792442291975021, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.08359292894601822, 'eval_precision': 0.6391437308868502, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6556862745098039, 'eval_accuracy': 0.9854222847539766, 'eval_runtime': 2.2316, 'eval_samples_per_second': 677.974, 'eval_steps_per_second': 85.139, 'epoch': 10.0, 'step': 1340}, {'loss': 0.001, 'grad_norm': 0.161637544631958, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.08509054780006409, 'eval_precision': 0.6417682926829268, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6593578700078307, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 2.2407, 'eval_samples_per_second': 675.22, 'eval_steps_per_second': 84.793, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0009, 'grad_norm': 0.08663606643676758, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.08513770997524261, 'eval_precision': 0.6448170731707317, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6624902114330461, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 2.3399, 'eval_samples_per_second': 646.622, 'eval_steps_per_second': 81.202, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 420.1655, 'train_samples_per_second': 244.761, 'train_steps_per_second': 3.827, 'total_flos': 4438763166165948.0, 'train_loss': 0.018482027112029085, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9876
  predict_f1                 =     0.6681
  predict_loss               =     0.0422
  predict_precision          =     0.6356
  predict_recall             =     0.7039
  predict_runtime            = 0:00:01.88
  predict_samples_per_second =    664.187
  predict_steps_per_second   =     83.289
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_101.json completed. F1: 0.6680541103017689
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_404.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5917.39 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5934.81 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6067.02 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6554.36 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6754.35 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7022.66 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7028.96 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7227.73 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6548.24 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7265.32 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4676.46 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 4823.49 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3967.31 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0833, 'grad_norm': 0.45704028010368347, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05043806880712509, 'eval_precision': 0.6458684654300169, 'eval_recall': 0.6167471819645732, 'eval_f1': 0.630971993410214, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 2.3932, 'eval_samples_per_second': 632.203, 'eval_steps_per_second': 79.391, 'epoch': 1.0}
{'loss': 0.038, 'grad_norm': 0.08797314763069153, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.05151369050145149, 'eval_precision': 0.6213292117465224, 'eval_recall': 0.6473429951690821, 'eval_f1': 0.6340694006309148, 'eval_accuracy': 0.985148708328448, 'eval_runtime': 2.2602, 'eval_samples_per_second': 669.4, 'eval_steps_per_second': 84.062, 'epoch': 2.0}
{'loss': 0.0252, 'grad_norm': 0.21446599066257477, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04783843830227852, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6869633099141296, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.2536, 'eval_samples_per_second': 671.372, 'eval_steps_per_second': 84.31, 'epoch': 3.0}
{'loss': 0.0156, 'grad_norm': 0.3114602565765381, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05469321832060814, 'eval_precision': 0.6903633491311216, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.696969696969697, 'eval_accuracy': 0.9872200726931645, 'eval_runtime': 2.2382, 'eval_samples_per_second': 675.989, 'eval_steps_per_second': 84.89, 'epoch': 4.0}
{'loss': 0.0092, 'grad_norm': 0.39610975980758667, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06678277999162674, 'eval_precision': 0.6846275752773375, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6900958466453673, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 2.2639, 'eval_samples_per_second': 668.312, 'eval_steps_per_second': 83.925, 'epoch': 5.0}
{'loss': 0.0064, 'grad_norm': 0.21039225161075592, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06933861970901489, 'eval_precision': 0.6448736998514116, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.670788253477589, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.3806, 'eval_samples_per_second': 635.544, 'eval_steps_per_second': 79.811, 'epoch': 6.0}
{'loss': 0.0035, 'grad_norm': 0.0050359973683953285, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07301386445760727, 'eval_precision': 0.6558641975308642, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.669818754925138, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.2508, 'eval_samples_per_second': 672.203, 'eval_steps_per_second': 84.414, 'epoch': 7.0}
{'loss': 0.0019, 'grad_norm': 0.20968584716320038, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07937073707580566, 'eval_precision': 0.6443057722308893, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6545166402535657, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 2.2431, 'eval_samples_per_second': 674.512, 'eval_steps_per_second': 84.704, 'epoch': 8.0}
{'loss': 0.001, 'grad_norm': 0.4178777039051056, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08066286891698837, 'eval_precision': 0.6229508196721312, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6470588235294117, 'eval_accuracy': 0.9850314612889358, 'eval_runtime': 2.2373, 'eval_samples_per_second': 676.277, 'eval_steps_per_second': 84.926, 'epoch': 9.0}
{'loss': 0.0005, 'grad_norm': 0.01711910590529442, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08191327750682831, 'eval_precision': 0.6550151975683891, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6739640344018765, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2424, 'eval_samples_per_second': 674.711, 'eval_steps_per_second': 84.729, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.0451328381896019, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08453474938869476, 'eval_precision': 0.6609642301710731, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6724683544303798, 'eval_accuracy': 0.9863993434165788, 'eval_runtime': 2.2574, 'eval_samples_per_second': 670.246, 'eval_steps_per_second': 84.168, 'epoch': 11.0}
{'loss': 0.0003, 'grad_norm': 0.0022196380887180567, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08506263792514801, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2512, 'eval_samples_per_second': 672.081, 'eval_steps_per_second': 84.399, 'epoch': 12.0}
{'train_runtime': 392.1776, 'train_samples_per_second': 262.228, 'train_steps_per_second': 8.2, 'train_loss': 0.015443335101935105, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0154
  train_runtime            = 0:06:32.17
  train_samples            =       8570
  train_samples_per_second =    262.228
  train_steps_per_second   =        8.2
[{'loss': 0.0833, 'grad_norm': 0.45704028010368347, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05043806880712509, 'eval_precision': 0.6458684654300169, 'eval_recall': 0.6167471819645732, 'eval_f1': 0.630971993410214, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 2.3932, 'eval_samples_per_second': 632.203, 'eval_steps_per_second': 79.391, 'epoch': 1.0, 'step': 268}, {'loss': 0.038, 'grad_norm': 0.08797314763069153, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.05151369050145149, 'eval_precision': 0.6213292117465224, 'eval_recall': 0.6473429951690821, 'eval_f1': 0.6340694006309148, 'eval_accuracy': 0.985148708328448, 'eval_runtime': 2.2602, 'eval_samples_per_second': 669.4, 'eval_steps_per_second': 84.062, 'epoch': 2.0, 'step': 536}, {'loss': 0.0252, 'grad_norm': 0.21446599066257477, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04783843830227852, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6869633099141296, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.2536, 'eval_samples_per_second': 671.372, 'eval_steps_per_second': 84.31, 'epoch': 3.0, 'step': 804}, {'loss': 0.0156, 'grad_norm': 0.3114602565765381, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05469321832060814, 'eval_precision': 0.6903633491311216, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.696969696969697, 'eval_accuracy': 0.9872200726931645, 'eval_runtime': 2.2382, 'eval_samples_per_second': 675.989, 'eval_steps_per_second': 84.89, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0092, 'grad_norm': 0.39610975980758667, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06678277999162674, 'eval_precision': 0.6846275752773375, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6900958466453673, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 2.2639, 'eval_samples_per_second': 668.312, 'eval_steps_per_second': 83.925, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0064, 'grad_norm': 0.21039225161075592, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06933861970901489, 'eval_precision': 0.6448736998514116, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.670788253477589, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.3806, 'eval_samples_per_second': 635.544, 'eval_steps_per_second': 79.811, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0035, 'grad_norm': 0.0050359973683953285, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07301386445760727, 'eval_precision': 0.6558641975308642, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.669818754925138, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.2508, 'eval_samples_per_second': 672.203, 'eval_steps_per_second': 84.414, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0019, 'grad_norm': 0.20968584716320038, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07937073707580566, 'eval_precision': 0.6443057722308893, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6545166402535657, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 2.2431, 'eval_samples_per_second': 674.512, 'eval_steps_per_second': 84.704, 'epoch': 8.0, 'step': 2144}, {'loss': 0.001, 'grad_norm': 0.4178777039051056, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.08066286891698837, 'eval_precision': 0.6229508196721312, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6470588235294117, 'eval_accuracy': 0.9850314612889358, 'eval_runtime': 2.2373, 'eval_samples_per_second': 676.277, 'eval_steps_per_second': 84.926, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0005, 'grad_norm': 0.01711910590529442, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.08191327750682831, 'eval_precision': 0.6550151975683891, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6739640344018765, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2424, 'eval_samples_per_second': 674.711, 'eval_steps_per_second': 84.729, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0004, 'grad_norm': 0.0451328381896019, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08453474938869476, 'eval_precision': 0.6609642301710731, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6724683544303798, 'eval_accuracy': 0.9863993434165788, 'eval_runtime': 2.2574, 'eval_samples_per_second': 670.246, 'eval_steps_per_second': 84.168, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0003, 'grad_norm': 0.0022196380887180567, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08506263792514801, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2512, 'eval_samples_per_second': 672.081, 'eval_steps_per_second': 84.399, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 392.1776, 'train_samples_per_second': 262.228, 'train_steps_per_second': 8.2, 'total_flos': 3919226198440236.0, 'train_loss': 0.015443335101935105, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9894
  predict_f1                 =     0.7188
  predict_loss               =     0.0444
  predict_precision          =     0.6939
  predict_recall             =     0.7456
  predict_runtime            = 0:00:01.94
  predict_samples_per_second =    642.936
  predict_steps_per_second   =     80.624
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_404.json completed. F1: 0.7188160676532769
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_303.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2871.70 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 3688.52 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4560.87 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5383.92 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5938.28 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6420.21 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6649.13 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6951.27 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5495.88 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6212.22 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 1591.84 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7305.44 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5021.62 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0809, 'grad_norm': 0.29347938299179077, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04946739971637726, 'eval_precision': 0.6233183856502242, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6465116279069767, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3654, 'eval_samples_per_second': 639.647, 'eval_steps_per_second': 80.326, 'epoch': 1.0}
{'loss': 0.0377, 'grad_norm': 0.4918366074562073, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04960697889328003, 'eval_precision': 0.6284074605451937, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6646433990895296, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 2.2448, 'eval_samples_per_second': 674.006, 'eval_steps_per_second': 84.641, 'epoch': 2.0}
{'loss': 0.0237, 'grad_norm': 0.5184271931648254, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04725063592195511, 'eval_precision': 0.6624803767660911, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6709062003179651, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.2847, 'eval_samples_per_second': 662.225, 'eval_steps_per_second': 83.161, 'epoch': 3.0}
{'loss': 0.0151, 'grad_norm': 0.26250430941581726, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.054522447288036346, 'eval_precision': 0.664179104477612, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6893880712625872, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.2272, 'eval_samples_per_second': 679.314, 'eval_steps_per_second': 85.307, 'epoch': 4.0}
{'loss': 0.0095, 'grad_norm': 0.02365884743630886, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06500338762998581, 'eval_precision': 0.6288343558282209, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.6441476826394344, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.2451, 'eval_samples_per_second': 673.902, 'eval_steps_per_second': 84.627, 'epoch': 5.0}
{'loss': 0.0061, 'grad_norm': 0.38302138447761536, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06685250252485275, 'eval_precision': 0.6785714285714286, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6909090909090909, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 2.2398, 'eval_samples_per_second': 675.495, 'eval_steps_per_second': 84.828, 'epoch': 6.0}
{'loss': 0.0043, 'grad_norm': 0.07898149639368057, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07353177666664124, 'eval_precision': 0.6578125, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6677240285487709, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.2323, 'eval_samples_per_second': 677.783, 'eval_steps_per_second': 85.115, 'epoch': 7.0}
{'loss': 0.002, 'grad_norm': 0.31759536266326904, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07619080692529678, 'eval_precision': 0.675, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6851704996034893, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2431, 'eval_samples_per_second': 674.528, 'eval_steps_per_second': 84.706, 'epoch': 8.0}
{'loss': 0.0014, 'grad_norm': 0.07496574521064758, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08127599209547043, 'eval_precision': 0.6826625386996904, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.696132596685083, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2274, 'eval_samples_per_second': 679.273, 'eval_steps_per_second': 85.302, 'epoch': 9.0}
{'loss': 0.0007, 'grad_norm': 0.0009088069782592356, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08429825305938721, 'eval_precision': 0.6630602782071098, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6766561514195584, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.2349, 'eval_samples_per_second': 676.974, 'eval_steps_per_second': 85.013, 'epoch': 10.0}
{'loss': 0.0006, 'grad_norm': 0.14429152011871338, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08433591574430466, 'eval_precision': 0.6574923547400612, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6745098039215686, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.2407, 'eval_samples_per_second': 675.223, 'eval_steps_per_second': 84.793, 'epoch': 11.0}
{'loss': 0.0004, 'grad_norm': 0.03129088506102562, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08484870940446854, 'eval_precision': 0.6539634146341463, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6718872357086924, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.2168, 'eval_samples_per_second': 682.519, 'eval_steps_per_second': 85.71, 'epoch': 12.0}
{'train_runtime': 414.4245, 'train_samples_per_second': 248.151, 'train_steps_per_second': 7.76, 'train_loss': 0.015196314424424623, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0152
  train_runtime            = 0:06:54.42
  train_samples            =       8570
  train_samples_per_second =    248.151
  train_steps_per_second   =       7.76
[{'loss': 0.0809, 'grad_norm': 0.29347938299179077, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04946739971637726, 'eval_precision': 0.6233183856502242, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6465116279069767, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3654, 'eval_samples_per_second': 639.647, 'eval_steps_per_second': 80.326, 'epoch': 1.0, 'step': 268}, {'loss': 0.0377, 'grad_norm': 0.4918366074562073, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04960697889328003, 'eval_precision': 0.6284074605451937, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6646433990895296, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 2.2448, 'eval_samples_per_second': 674.006, 'eval_steps_per_second': 84.641, 'epoch': 2.0, 'step': 536}, {'loss': 0.0237, 'grad_norm': 0.5184271931648254, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04725063592195511, 'eval_precision': 0.6624803767660911, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6709062003179651, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.2847, 'eval_samples_per_second': 662.225, 'eval_steps_per_second': 83.161, 'epoch': 3.0, 'step': 804}, {'loss': 0.0151, 'grad_norm': 0.26250430941581726, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.054522447288036346, 'eval_precision': 0.664179104477612, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6893880712625872, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.2272, 'eval_samples_per_second': 679.314, 'eval_steps_per_second': 85.307, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0095, 'grad_norm': 0.02365884743630886, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06500338762998581, 'eval_precision': 0.6288343558282209, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.6441476826394344, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.2451, 'eval_samples_per_second': 673.902, 'eval_steps_per_second': 84.627, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0061, 'grad_norm': 0.38302138447761536, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06685250252485275, 'eval_precision': 0.6785714285714286, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6909090909090909, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 2.2398, 'eval_samples_per_second': 675.495, 'eval_steps_per_second': 84.828, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0043, 'grad_norm': 0.07898149639368057, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07353177666664124, 'eval_precision': 0.6578125, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6677240285487709, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.2323, 'eval_samples_per_second': 677.783, 'eval_steps_per_second': 85.115, 'epoch': 7.0, 'step': 1876}, {'loss': 0.002, 'grad_norm': 0.31759536266326904, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07619080692529678, 'eval_precision': 0.675, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6851704996034893, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2431, 'eval_samples_per_second': 674.528, 'eval_steps_per_second': 84.706, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0014, 'grad_norm': 0.07496574521064758, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.08127599209547043, 'eval_precision': 0.6826625386996904, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.696132596685083, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2274, 'eval_samples_per_second': 679.273, 'eval_steps_per_second': 85.302, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0007, 'grad_norm': 0.0009088069782592356, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.08429825305938721, 'eval_precision': 0.6630602782071098, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6766561514195584, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.2349, 'eval_samples_per_second': 676.974, 'eval_steps_per_second': 85.013, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0006, 'grad_norm': 0.14429152011871338, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08433591574430466, 'eval_precision': 0.6574923547400612, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6745098039215686, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.2407, 'eval_samples_per_second': 675.223, 'eval_steps_per_second': 84.793, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0004, 'grad_norm': 0.03129088506102562, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08484870940446854, 'eval_precision': 0.6539634146341463, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6718872357086924, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.2168, 'eval_samples_per_second': 682.519, 'eval_steps_per_second': 85.71, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 414.4245, 'train_samples_per_second': 248.151, 'train_steps_per_second': 7.76, 'total_flos': 3923819679226236.0, 'train_loss': 0.015196314424424623, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9882
  predict_f1                 =     0.6866
  predict_loss               =     0.0488
  predict_precision          =     0.6701
  predict_recall             =     0.7039
  predict_runtime            = 0:00:01.89
  predict_samples_per_second =    659.665
  predict_steps_per_second   =     82.722
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_303.json completed. F1: 0.6866310160427808
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_101.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6878.90 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:00, 6839.52 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6206.01 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6474.63 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6782.52 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7066.27 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7149.46 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7349.61 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6643.47 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7428.27 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4507.60 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 5708.04 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4061.57 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1562, 'grad_norm': 0.4062027633190155, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.055177588015794754, 'eval_precision': 0.5646687697160884, 'eval_recall': 0.5764895330112721, 'eval_f1': 0.5705179282868527, 'eval_accuracy': 0.9843670613983664, 'eval_runtime': 4.56, 'eval_samples_per_second': 331.796, 'eval_steps_per_second': 41.666, 'epoch': 1.0}
{'loss': 0.0461, 'grad_norm': 0.44747480750083923, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04854295775294304, 'eval_precision': 0.6330275229357798, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6494117647058824, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 4.5472, 'eval_samples_per_second': 332.731, 'eval_steps_per_second': 41.784, 'epoch': 2.0}
{'loss': 0.035, 'grad_norm': 0.34720104932785034, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.046911388635635376, 'eval_precision': 0.6609907120743034, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6740331491712707, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5376, 'eval_samples_per_second': 333.433, 'eval_steps_per_second': 41.872, 'epoch': 3.0}
{'loss': 0.0271, 'grad_norm': 0.48058292269706726, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.0454963855445385, 'eval_precision': 0.6960629921259842, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7038216560509554, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6057, 'eval_samples_per_second': 328.504, 'eval_steps_per_second': 41.253, 'epoch': 4.0}
{'loss': 0.0212, 'grad_norm': 0.335036963224411, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05021441727876663, 'eval_precision': 0.6595092024539877, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6755695208169678, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.5887, 'eval_samples_per_second': 329.724, 'eval_steps_per_second': 41.406, 'epoch': 5.0}
{'loss': 0.0165, 'grad_norm': 0.5317530035972595, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05071971192955971, 'eval_precision': 0.7262479871175523, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7262479871175523, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5466, 'eval_samples_per_second': 332.777, 'eval_steps_per_second': 41.79, 'epoch': 6.0}
{'loss': 0.0128, 'grad_norm': 0.27402088046073914, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.0537998266518116, 'eval_precision': 0.7067082683307332, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7179080824088748, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.5691, 'eval_samples_per_second': 331.137, 'eval_steps_per_second': 41.584, 'epoch': 7.0}
{'loss': 0.0104, 'grad_norm': 0.5934548377990723, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05604016035795212, 'eval_precision': 0.7133757961783439, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7173738991192954, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.5235, 'eval_samples_per_second': 334.479, 'eval_steps_per_second': 42.003, 'epoch': 8.0}
{'loss': 0.0089, 'grad_norm': 0.5981727838516235, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05792972072958946, 'eval_precision': 0.7061611374407583, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7129186602870814, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6483, 'eval_samples_per_second': 325.494, 'eval_steps_per_second': 40.875, 'epoch': 9.0}
{'loss': 0.0077, 'grad_norm': 0.24833175539970398, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06069617345929146, 'eval_precision': 0.6933962264150944, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7016706443914081, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.6072, 'eval_samples_per_second': 328.398, 'eval_steps_per_second': 41.24, 'epoch': 10.0}
{'loss': 0.0065, 'grad_norm': 0.28940388560295105, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06118153780698776, 'eval_precision': 0.712241653418124, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7168, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.5873, 'eval_samples_per_second': 329.823, 'eval_steps_per_second': 41.419, 'epoch': 11.0}
{'loss': 0.0061, 'grad_norm': 0.2525435984134674, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06171596422791481, 'eval_precision': 0.707740916271722, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7145135566188198, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.6582, 'eval_samples_per_second': 324.807, 'eval_steps_per_second': 40.789, 'epoch': 12.0}
{'train_runtime': 978.3038, 'train_samples_per_second': 105.121, 'train_steps_per_second': 1.644, 'train_loss': 0.02953991505192287, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0295
  train_runtime            = 0:16:18.30
  train_samples            =       8570
  train_samples_per_second =    105.121
  train_steps_per_second   =      1.644
[{'loss': 0.1562, 'grad_norm': 0.4062027633190155, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.055177588015794754, 'eval_precision': 0.5646687697160884, 'eval_recall': 0.5764895330112721, 'eval_f1': 0.5705179282868527, 'eval_accuracy': 0.9843670613983664, 'eval_runtime': 4.56, 'eval_samples_per_second': 331.796, 'eval_steps_per_second': 41.666, 'epoch': 1.0, 'step': 134}, {'loss': 0.0461, 'grad_norm': 0.44747480750083923, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04854295775294304, 'eval_precision': 0.6330275229357798, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6494117647058824, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 4.5472, 'eval_samples_per_second': 332.731, 'eval_steps_per_second': 41.784, 'epoch': 2.0, 'step': 268}, {'loss': 0.035, 'grad_norm': 0.34720104932785034, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.046911388635635376, 'eval_precision': 0.6609907120743034, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6740331491712707, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5376, 'eval_samples_per_second': 333.433, 'eval_steps_per_second': 41.872, 'epoch': 3.0, 'step': 402}, {'loss': 0.0271, 'grad_norm': 0.48058292269706726, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.0454963855445385, 'eval_precision': 0.6960629921259842, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7038216560509554, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6057, 'eval_samples_per_second': 328.504, 'eval_steps_per_second': 41.253, 'epoch': 4.0, 'step': 536}, {'loss': 0.0212, 'grad_norm': 0.335036963224411, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05021441727876663, 'eval_precision': 0.6595092024539877, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6755695208169678, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.5887, 'eval_samples_per_second': 329.724, 'eval_steps_per_second': 41.406, 'epoch': 5.0, 'step': 670}, {'loss': 0.0165, 'grad_norm': 0.5317530035972595, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05071971192955971, 'eval_precision': 0.7262479871175523, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7262479871175523, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5466, 'eval_samples_per_second': 332.777, 'eval_steps_per_second': 41.79, 'epoch': 6.0, 'step': 804}, {'loss': 0.0128, 'grad_norm': 0.27402088046073914, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.0537998266518116, 'eval_precision': 0.7067082683307332, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7179080824088748, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.5691, 'eval_samples_per_second': 331.137, 'eval_steps_per_second': 41.584, 'epoch': 7.0, 'step': 938}, {'loss': 0.0104, 'grad_norm': 0.5934548377990723, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05604016035795212, 'eval_precision': 0.7133757961783439, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7173738991192954, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.5235, 'eval_samples_per_second': 334.479, 'eval_steps_per_second': 42.003, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0089, 'grad_norm': 0.5981727838516235, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05792972072958946, 'eval_precision': 0.7061611374407583, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7129186602870814, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6483, 'eval_samples_per_second': 325.494, 'eval_steps_per_second': 40.875, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0077, 'grad_norm': 0.24833175539970398, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06069617345929146, 'eval_precision': 0.6933962264150944, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7016706443914081, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.6072, 'eval_samples_per_second': 328.398, 'eval_steps_per_second': 41.24, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0065, 'grad_norm': 0.28940388560295105, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06118153780698776, 'eval_precision': 0.712241653418124, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7168, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.5873, 'eval_samples_per_second': 329.823, 'eval_steps_per_second': 41.419, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0061, 'grad_norm': 0.2525435984134674, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06171596422791481, 'eval_precision': 0.707740916271722, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7145135566188198, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.6582, 'eval_samples_per_second': 324.807, 'eval_steps_per_second': 40.789, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 978.3038, 'train_samples_per_second': 105.121, 'train_steps_per_second': 1.644, 'total_flos': 1.2913056233591976e+16, 'train_loss': 0.02953991505192287, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9893
  predict_f1                 =     0.7135
  predict_loss               =     0.0419
  predict_precision          =     0.6841
  predict_recall             =     0.7456
  predict_runtime            = 0:00:03.86
  predict_samples_per_second =    323.676
  predict_steps_per_second   =     40.589
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_101.json completed. F1: 0.7135362014690452
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_404.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5799.57 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5514.12 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6153.77 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6228.56 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6573.33 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6920.05 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7005.83 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7209.60 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6316.72 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7344.37 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5446.00 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7319.89 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5121.49 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0738, 'grad_norm': 0.588546097278595, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.054721444845199585, 'eval_precision': 0.6062407132243685, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6306027820710974, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 4.8202, 'eval_samples_per_second': 313.887, 'eval_steps_per_second': 39.417, 'epoch': 1.0}
{'loss': 0.035, 'grad_norm': 1.104756474494934, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.05118727684020996, 'eval_precision': 0.6733333333333333, 'eval_recall': 0.6505636070853462, 'eval_f1': 0.6617526617526618, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 4.6831, 'eval_samples_per_second': 323.077, 'eval_steps_per_second': 40.571, 'epoch': 2.0}
{'loss': 0.0212, 'grad_norm': 0.38051432371139526, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05154646933078766, 'eval_precision': 0.6707692307692308, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6860739575137688, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 4.657, 'eval_samples_per_second': 324.886, 'eval_steps_per_second': 40.799, 'epoch': 3.0}
{'loss': 0.0121, 'grad_norm': 0.06122196838259697, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05756388604640961, 'eval_precision': 0.6498470948012233, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6666666666666667, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 4.6162, 'eval_samples_per_second': 327.761, 'eval_steps_per_second': 41.16, 'epoch': 4.0}
{'loss': 0.0079, 'grad_norm': 0.41647493839263916, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06309954822063446, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.5837, 'eval_samples_per_second': 330.081, 'eval_steps_per_second': 41.451, 'epoch': 5.0}
{'loss': 0.0037, 'grad_norm': 0.16447876393795013, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07001850754022598, 'eval_precision': 0.6789727126805778, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.680064308681672, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.5999, 'eval_samples_per_second': 328.921, 'eval_steps_per_second': 41.305, 'epoch': 6.0}
{'loss': 0.0019, 'grad_norm': 0.012815539725124836, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07253256440162659, 'eval_precision': 0.656832298136646, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6687747035573123, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 4.65, 'eval_samples_per_second': 325.379, 'eval_steps_per_second': 40.861, 'epoch': 7.0}
{'loss': 0.0012, 'grad_norm': 0.01661578007042408, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.0703236311674118, 'eval_precision': 0.6890625, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6994448850118954, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6274, 'eval_samples_per_second': 326.968, 'eval_steps_per_second': 41.06, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.016843071207404137, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07394499331712723, 'eval_precision': 0.6713836477987422, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6793953858392998, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 4.6127, 'eval_samples_per_second': 328.008, 'eval_steps_per_second': 41.191, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.0017478723311796784, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.0789821520447731, 'eval_precision': 0.6795665634674922, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6929755327545384, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.5784, 'eval_samples_per_second': 330.462, 'eval_steps_per_second': 41.499, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.011075974442064762, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.079864501953125, 'eval_precision': 0.6698113207547169, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6778042959427207, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.606, 'eval_samples_per_second': 328.482, 'eval_steps_per_second': 41.25, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.004953575786203146, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07975633442401886, 'eval_precision': 0.6728971962616822, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.684085510688836, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.5712, 'eval_samples_per_second': 330.983, 'eval_steps_per_second': 41.564, 'epoch': 12.0}
{'train_runtime': 988.2032, 'train_samples_per_second': 104.068, 'train_steps_per_second': 3.254, 'train_loss': 0.013167799020705013, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0132
  train_runtime            = 0:16:28.20
  train_samples            =       8570
  train_samples_per_second =    104.068
  train_steps_per_second   =      3.254
[{'loss': 0.0738, 'grad_norm': 0.588546097278595, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.054721444845199585, 'eval_precision': 0.6062407132243685, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6306027820710974, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 4.8202, 'eval_samples_per_second': 313.887, 'eval_steps_per_second': 39.417, 'epoch': 1.0, 'step': 268}, {'loss': 0.035, 'grad_norm': 1.104756474494934, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.05118727684020996, 'eval_precision': 0.6733333333333333, 'eval_recall': 0.6505636070853462, 'eval_f1': 0.6617526617526618, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 4.6831, 'eval_samples_per_second': 323.077, 'eval_steps_per_second': 40.571, 'epoch': 2.0, 'step': 536}, {'loss': 0.0212, 'grad_norm': 0.38051432371139526, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05154646933078766, 'eval_precision': 0.6707692307692308, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6860739575137688, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 4.657, 'eval_samples_per_second': 324.886, 'eval_steps_per_second': 40.799, 'epoch': 3.0, 'step': 804}, {'loss': 0.0121, 'grad_norm': 0.06122196838259697, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05756388604640961, 'eval_precision': 0.6498470948012233, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6666666666666667, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 4.6162, 'eval_samples_per_second': 327.761, 'eval_steps_per_second': 41.16, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0079, 'grad_norm': 0.41647493839263916, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06309954822063446, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.5837, 'eval_samples_per_second': 330.081, 'eval_steps_per_second': 41.451, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0037, 'grad_norm': 0.16447876393795013, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.07001850754022598, 'eval_precision': 0.6789727126805778, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.680064308681672, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.5999, 'eval_samples_per_second': 328.921, 'eval_steps_per_second': 41.305, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0019, 'grad_norm': 0.012815539725124836, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07253256440162659, 'eval_precision': 0.656832298136646, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6687747035573123, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 4.65, 'eval_samples_per_second': 325.379, 'eval_steps_per_second': 40.861, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0012, 'grad_norm': 0.01661578007042408, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.0703236311674118, 'eval_precision': 0.6890625, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6994448850118954, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6274, 'eval_samples_per_second': 326.968, 'eval_steps_per_second': 41.06, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0005, 'grad_norm': 0.016843071207404137, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07394499331712723, 'eval_precision': 0.6713836477987422, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6793953858392998, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 4.6127, 'eval_samples_per_second': 328.008, 'eval_steps_per_second': 41.191, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0003, 'grad_norm': 0.0017478723311796784, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.0789821520447731, 'eval_precision': 0.6795665634674922, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6929755327545384, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.5784, 'eval_samples_per_second': 330.462, 'eval_steps_per_second': 41.499, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0002, 'grad_norm': 0.011075974442064762, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.079864501953125, 'eval_precision': 0.6698113207547169, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6778042959427207, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.606, 'eval_samples_per_second': 328.482, 'eval_steps_per_second': 41.25, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0002, 'grad_norm': 0.004953575786203146, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.07975633442401886, 'eval_precision': 0.6728971962616822, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.684085510688836, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.5712, 'eval_samples_per_second': 330.983, 'eval_steps_per_second': 41.564, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 988.2032, 'train_samples_per_second': 104.068, 'train_steps_per_second': 3.254, 'total_flos': 1.1351569167583344e+16, 'train_loss': 0.013167799020705013, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9898
  predict_f1                 =     0.7178
  predict_loss               =     0.0416
  predict_precision          =     0.7275
  predict_recall             =     0.7083
  predict_runtime            = 0:00:03.85
  predict_samples_per_second =    325.025
  predict_steps_per_second   =     40.758
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_404.json completed. F1: 0.7177777777777777
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_202.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6211.98 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5173.99 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5201.37 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5888.21 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6308.18 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6678.86 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6801.12 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7029.91 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6088.75 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6196.34 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4606.09 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6786.61 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4448.70 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.222, 'grad_norm': 0.7305793166160583, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.06628446280956268, 'eval_precision': 0.5685358255451713, 'eval_recall': 0.5877616747181964, 'eval_f1': 0.5779889152810768, 'eval_accuracy': 0.9810450619455192, 'eval_runtime': 2.4037, 'eval_samples_per_second': 629.449, 'eval_steps_per_second': 79.045, 'epoch': 1.0}
{'loss': 0.0546, 'grad_norm': 0.508941650390625, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.05526989698410034, 'eval_precision': 0.5930408472012103, 'eval_recall': 0.6312399355877617, 'eval_f1': 0.6115444617784711, 'eval_accuracy': 0.9835072497752765, 'eval_runtime': 2.3321, 'eval_samples_per_second': 648.758, 'eval_steps_per_second': 81.47, 'epoch': 2.0}
{'loss': 0.0433, 'grad_norm': 0.4086766242980957, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.05221585929393768, 'eval_precision': 0.6393188854489165, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6519337016574585, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 2.3995, 'eval_samples_per_second': 630.536, 'eval_steps_per_second': 79.182, 'epoch': 3.0}
{'loss': 0.037, 'grad_norm': 0.5056350827217102, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.05312318727374077, 'eval_precision': 0.6153846153846154, 'eval_recall': 0.644122383252818, 'eval_f1': 0.6294256490952006, 'eval_accuracy': 0.9849532965959277, 'eval_runtime': 2.3721, 'eval_samples_per_second': 637.821, 'eval_steps_per_second': 80.097, 'epoch': 4.0}
{'loss': 0.0318, 'grad_norm': 0.5519037246704102, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05009540170431137, 'eval_precision': 0.6942277691107644, 'eval_recall': 0.71658615136876, 'eval_f1': 0.705229793977813, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.357, 'eval_samples_per_second': 641.921, 'eval_steps_per_second': 80.611, 'epoch': 5.0}
{'loss': 0.0278, 'grad_norm': 0.7906935811042786, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.052830517292022705, 'eval_precision': 0.6702954898911353, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6819620253164557, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.3943, 'eval_samples_per_second': 631.908, 'eval_steps_per_second': 79.354, 'epoch': 6.0}
{'loss': 0.0248, 'grad_norm': 0.37825992703437805, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.0541582815349102, 'eval_precision': 0.6739130434782609, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6861660079051384, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.3366, 'eval_samples_per_second': 647.524, 'eval_steps_per_second': 81.315, 'epoch': 7.0}
{'loss': 0.0221, 'grad_norm': 0.40995892882347107, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.055313706398010254, 'eval_precision': 0.6687211093990755, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6834645669291338, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.3953, 'eval_samples_per_second': 631.651, 'eval_steps_per_second': 79.322, 'epoch': 8.0}
{'loss': 0.0195, 'grad_norm': 0.786510705947876, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05803066864609718, 'eval_precision': 0.669195751138088, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6890625000000001, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3717, 'eval_samples_per_second': 637.936, 'eval_steps_per_second': 80.111, 'epoch': 9.0}
{'loss': 0.0179, 'grad_norm': 0.49613064527511597, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05778133124113083, 'eval_precision': 0.6731634182908546, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.6972049689440993, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.2695, 'eval_samples_per_second': 666.656, 'eval_steps_per_second': 83.718, 'epoch': 10.0}
{'loss': 0.0166, 'grad_norm': 0.6088994741439819, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.0594492107629776, 'eval_precision': 0.6712962962962963, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6855791962174941, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3293, 'eval_samples_per_second': 649.556, 'eval_steps_per_second': 81.57, 'epoch': 11.0}
{'loss': 0.0158, 'grad_norm': 0.4289453327655792, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05917854607105255, 'eval_precision': 0.6904024767801857, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7040252565114443, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.4281, 'eval_samples_per_second': 623.129, 'eval_steps_per_second': 78.251, 'epoch': 12.0}
{'train_runtime': 419.2697, 'train_samples_per_second': 245.284, 'train_steps_per_second': 3.835, 'train_loss': 0.04444893141884116, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0444
  train_runtime            = 0:06:59.26
  train_samples            =       8570
  train_samples_per_second =    245.284
  train_steps_per_second   =      3.835
[{'loss': 0.222, 'grad_norm': 0.7305793166160583, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.06628446280956268, 'eval_precision': 0.5685358255451713, 'eval_recall': 0.5877616747181964, 'eval_f1': 0.5779889152810768, 'eval_accuracy': 0.9810450619455192, 'eval_runtime': 2.4037, 'eval_samples_per_second': 629.449, 'eval_steps_per_second': 79.045, 'epoch': 1.0, 'step': 134}, {'loss': 0.0546, 'grad_norm': 0.508941650390625, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05526989698410034, 'eval_precision': 0.5930408472012103, 'eval_recall': 0.6312399355877617, 'eval_f1': 0.6115444617784711, 'eval_accuracy': 0.9835072497752765, 'eval_runtime': 2.3321, 'eval_samples_per_second': 648.758, 'eval_steps_per_second': 81.47, 'epoch': 2.0, 'step': 268}, {'loss': 0.0433, 'grad_norm': 0.4086766242980957, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.05221585929393768, 'eval_precision': 0.6393188854489165, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6519337016574585, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 2.3995, 'eval_samples_per_second': 630.536, 'eval_steps_per_second': 79.182, 'epoch': 3.0, 'step': 402}, {'loss': 0.037, 'grad_norm': 0.5056350827217102, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05312318727374077, 'eval_precision': 0.6153846153846154, 'eval_recall': 0.644122383252818, 'eval_f1': 0.6294256490952006, 'eval_accuracy': 0.9849532965959277, 'eval_runtime': 2.3721, 'eval_samples_per_second': 637.821, 'eval_steps_per_second': 80.097, 'epoch': 4.0, 'step': 536}, {'loss': 0.0318, 'grad_norm': 0.5519037246704102, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05009540170431137, 'eval_precision': 0.6942277691107644, 'eval_recall': 0.71658615136876, 'eval_f1': 0.705229793977813, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.357, 'eval_samples_per_second': 641.921, 'eval_steps_per_second': 80.611, 'epoch': 5.0, 'step': 670}, {'loss': 0.0278, 'grad_norm': 0.7906935811042786, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.052830517292022705, 'eval_precision': 0.6702954898911353, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6819620253164557, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.3943, 'eval_samples_per_second': 631.908, 'eval_steps_per_second': 79.354, 'epoch': 6.0, 'step': 804}, {'loss': 0.0248, 'grad_norm': 0.37825992703437805, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.0541582815349102, 'eval_precision': 0.6739130434782609, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6861660079051384, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.3366, 'eval_samples_per_second': 647.524, 'eval_steps_per_second': 81.315, 'epoch': 7.0, 'step': 938}, {'loss': 0.0221, 'grad_norm': 0.40995892882347107, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.055313706398010254, 'eval_precision': 0.6687211093990755, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6834645669291338, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.3953, 'eval_samples_per_second': 631.651, 'eval_steps_per_second': 79.322, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0195, 'grad_norm': 0.786510705947876, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05803066864609718, 'eval_precision': 0.669195751138088, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6890625000000001, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3717, 'eval_samples_per_second': 637.936, 'eval_steps_per_second': 80.111, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0179, 'grad_norm': 0.49613064527511597, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05778133124113083, 'eval_precision': 0.6731634182908546, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.6972049689440993, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.2695, 'eval_samples_per_second': 666.656, 'eval_steps_per_second': 83.718, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0166, 'grad_norm': 0.6088994741439819, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.0594492107629776, 'eval_precision': 0.6712962962962963, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6855791962174941, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3293, 'eval_samples_per_second': 649.556, 'eval_steps_per_second': 81.57, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0158, 'grad_norm': 0.4289453327655792, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05917854607105255, 'eval_precision': 0.6904024767801857, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7040252565114443, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.4281, 'eval_samples_per_second': 623.129, 'eval_steps_per_second': 78.251, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 419.2697, 'train_samples_per_second': 245.284, 'train_steps_per_second': 3.835, 'total_flos': 4417653570020508.0, 'train_loss': 0.04444893141884116, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9895
  predict_f1                 =     0.7129
  predict_loss               =     0.0422
  predict_precision          =     0.6944
  predict_recall             =     0.7325
  predict_runtime            = 0:00:01.95
  predict_samples_per_second =    640.444
  predict_steps_per_second   =     80.311
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_202.json completed. F1: 0.7129135538954108
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_303.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7198.16 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:00, 6648.67 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6804.54 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6214.07 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6561.46 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6867.25 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6927.44 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7132.19 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6663.00 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6960.03 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3619.05 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7177.81 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4800.24 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0731, 'grad_norm': 0.3111572265625, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.048013634979724884, 'eval_precision': 0.649390243902439, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6671887235708693, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 4.6312, 'eval_samples_per_second': 326.697, 'eval_steps_per_second': 41.026, 'epoch': 1.0}
{'loss': 0.0365, 'grad_norm': 0.41003742814064026, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04513728246092796, 'eval_precision': 0.6160583941605839, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6462480857580398, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 4.6136, 'eval_samples_per_second': 327.941, 'eval_steps_per_second': 41.182, 'epoch': 2.0}
{'loss': 0.021, 'grad_norm': 0.3203759491443634, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04730789363384247, 'eval_precision': 0.641566265060241, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.663035019455253, 'eval_accuracy': 0.9872200726931645, 'eval_runtime': 4.6084, 'eval_samples_per_second': 328.312, 'eval_steps_per_second': 41.229, 'epoch': 3.0}
{'loss': 0.0122, 'grad_norm': 1.558394432067871, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05634378641843796, 'eval_precision': 0.694006309148265, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.701195219123506, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.6609, 'eval_samples_per_second': 324.613, 'eval_steps_per_second': 40.764, 'epoch': 4.0}
{'loss': 0.0075, 'grad_norm': 0.04637211933732033, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05996362864971161, 'eval_precision': 0.6726726726726727, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6961926961926962, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.6601, 'eval_samples_per_second': 324.67, 'eval_steps_per_second': 40.771, 'epoch': 5.0}
{'loss': 0.0033, 'grad_norm': 0.04664003476500511, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06775632500648499, 'eval_precision': 0.6393188854489165, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6519337016574585, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 4.6333, 'eval_samples_per_second': 326.551, 'eval_steps_per_second': 41.008, 'epoch': 6.0}
{'loss': 0.0013, 'grad_norm': 0.012591439299285412, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07612539082765579, 'eval_precision': 0.6832, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6853932584269663, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6102, 'eval_samples_per_second': 328.187, 'eval_steps_per_second': 41.213, 'epoch': 7.0}
{'loss': 0.0007, 'grad_norm': 0.016182519495487213, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07471964508295059, 'eval_precision': 0.677115987460815, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6862589356632247, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6067, 'eval_samples_per_second': 328.436, 'eval_steps_per_second': 41.244, 'epoch': 8.0}
{'loss': 0.0003, 'grad_norm': 0.010356903076171875, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.0773397907614708, 'eval_precision': 0.6968253968253968, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7018385291766587, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.6238, 'eval_samples_per_second': 327.217, 'eval_steps_per_second': 41.091, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.002029428258538246, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08023381233215332, 'eval_precision': 0.6927899686520376, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7021445591739476, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.6169, 'eval_samples_per_second': 327.709, 'eval_steps_per_second': 41.153, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.00577682675793767, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08022170513868332, 'eval_precision': 0.6987381703470031, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7059760956175298, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.605, 'eval_samples_per_second': 328.559, 'eval_steps_per_second': 41.26, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.002198877278715372, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08081214874982834, 'eval_precision': 0.6929133858267716, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7006369426751592, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.6258, 'eval_samples_per_second': 327.076, 'eval_steps_per_second': 41.074, 'epoch': 12.0}
{'train_runtime': 955.466, 'train_samples_per_second': 107.633, 'train_steps_per_second': 3.366, 'train_loss': 0.013034361529289119, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =      0.013
  train_runtime            = 0:15:55.46
  train_samples            =       8570
  train_samples_per_second =    107.633
  train_steps_per_second   =      3.366
[{'loss': 0.0731, 'grad_norm': 0.3111572265625, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.048013634979724884, 'eval_precision': 0.649390243902439, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6671887235708693, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 4.6312, 'eval_samples_per_second': 326.697, 'eval_steps_per_second': 41.026, 'epoch': 1.0, 'step': 268}, {'loss': 0.0365, 'grad_norm': 0.41003742814064026, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04513728246092796, 'eval_precision': 0.6160583941605839, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6462480857580398, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 4.6136, 'eval_samples_per_second': 327.941, 'eval_steps_per_second': 41.182, 'epoch': 2.0, 'step': 536}, {'loss': 0.021, 'grad_norm': 0.3203759491443634, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04730789363384247, 'eval_precision': 0.641566265060241, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.663035019455253, 'eval_accuracy': 0.9872200726931645, 'eval_runtime': 4.6084, 'eval_samples_per_second': 328.312, 'eval_steps_per_second': 41.229, 'epoch': 3.0, 'step': 804}, {'loss': 0.0122, 'grad_norm': 1.558394432067871, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05634378641843796, 'eval_precision': 0.694006309148265, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.701195219123506, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.6609, 'eval_samples_per_second': 324.613, 'eval_steps_per_second': 40.764, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0075, 'grad_norm': 0.04637211933732033, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05996362864971161, 'eval_precision': 0.6726726726726727, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6961926961926962, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.6601, 'eval_samples_per_second': 324.67, 'eval_steps_per_second': 40.771, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0033, 'grad_norm': 0.04664003476500511, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06775632500648499, 'eval_precision': 0.6393188854489165, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6519337016574585, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 4.6333, 'eval_samples_per_second': 326.551, 'eval_steps_per_second': 41.008, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0013, 'grad_norm': 0.012591439299285412, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07612539082765579, 'eval_precision': 0.6832, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6853932584269663, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6102, 'eval_samples_per_second': 328.187, 'eval_steps_per_second': 41.213, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0007, 'grad_norm': 0.016182519495487213, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07471964508295059, 'eval_precision': 0.677115987460815, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6862589356632247, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6067, 'eval_samples_per_second': 328.436, 'eval_steps_per_second': 41.244, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0003, 'grad_norm': 0.010356903076171875, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.0773397907614708, 'eval_precision': 0.6968253968253968, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7018385291766587, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.6238, 'eval_samples_per_second': 327.217, 'eval_steps_per_second': 41.091, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.002029428258538246, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.08023381233215332, 'eval_precision': 0.6927899686520376, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7021445591739476, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.6169, 'eval_samples_per_second': 327.709, 'eval_steps_per_second': 41.153, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0002, 'grad_norm': 0.00577682675793767, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08022170513868332, 'eval_precision': 0.6987381703470031, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7059760956175298, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.605, 'eval_samples_per_second': 328.559, 'eval_steps_per_second': 41.26, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0002, 'grad_norm': 0.002198877278715372, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08081214874982834, 'eval_precision': 0.6929133858267716, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7006369426751592, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.6258, 'eval_samples_per_second': 327.076, 'eval_steps_per_second': 41.074, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 955.466, 'train_samples_per_second': 107.633, 'train_steps_per_second': 3.366, 'total_flos': 1.1349918485646804e+16, 'train_loss': 0.013034361529289119, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9881
  predict_f1                 =     0.6528
  predict_loss               =     0.0424
  predict_precision          =     0.6189
  predict_recall             =     0.6908
  predict_runtime            = 0:00:03.91
  predict_samples_per_second =    319.445
  predict_steps_per_second   =     40.058
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_303.json completed. F1: 0.6528497409326426
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_505.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7016.27 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 6042.36 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5057.70 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5743.69 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6163.11 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6534.88 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6679.66 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6886.27 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5913.18 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7091.78 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4963.90 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6658.76 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4525.36 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-670 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2201, 'grad_norm': 0.45036742091178894, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.06372136622667313, 'eval_precision': 0.5842696629213483, 'eval_recall': 0.5861513687600645, 'eval_f1': 0.5852090032154341, 'eval_accuracy': 0.9815922147965764, 'eval_runtime': 2.4913, 'eval_samples_per_second': 607.307, 'eval_steps_per_second': 76.265, 'epoch': 1.0}
{'loss': 0.0546, 'grad_norm': 0.6595871448516846, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.05450291186571121, 'eval_precision': 0.5907692307692308, 'eval_recall': 0.6183574879227053, 'eval_f1': 0.6042486231313926, 'eval_accuracy': 0.9842498143588541, 'eval_runtime': 2.3386, 'eval_samples_per_second': 646.982, 'eval_steps_per_second': 81.247, 'epoch': 2.0}
{'loss': 0.0431, 'grad_norm': 0.41279610991477966, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.050148606300354004, 'eval_precision': 0.6203288490284006, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6434108527131782, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 2.3265, 'eval_samples_per_second': 650.343, 'eval_steps_per_second': 81.669, 'epoch': 3.0}
{'loss': 0.0363, 'grad_norm': 0.48746538162231445, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.052026696503162384, 'eval_precision': 0.6757624398073836, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6768488745980707, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3166, 'eval_samples_per_second': 653.11, 'eval_steps_per_second': 82.016, 'epoch': 4.0}
{'loss': 0.032, 'grad_norm': 0.2688318192958832, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05010435730218887, 'eval_precision': 0.6724137931034483, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6814932486100079, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3075, 'eval_samples_per_second': 655.679, 'eval_steps_per_second': 82.339, 'epoch': 5.0}
{'loss': 0.0282, 'grad_norm': 0.5151277780532837, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05253152921795845, 'eval_precision': 0.6751592356687898, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6789431545236189, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.303, 'eval_samples_per_second': 656.977, 'eval_steps_per_second': 82.502, 'epoch': 6.0}
{'loss': 0.0248, 'grad_norm': 0.5214818716049194, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05392344295978546, 'eval_precision': 0.6813186813186813, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6899841017488076, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 2.3875, 'eval_samples_per_second': 633.716, 'eval_steps_per_second': 79.581, 'epoch': 7.0}
{'loss': 0.0222, 'grad_norm': 0.32153090834617615, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.0551890954375267, 'eval_precision': 0.649923896499239, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6682316118935836, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3586, 'eval_samples_per_second': 641.486, 'eval_steps_per_second': 80.557, 'epoch': 8.0}
{'loss': 0.0202, 'grad_norm': 0.33476391434669495, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05612513795495033, 'eval_precision': 0.6646153846153846, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6797797010228167, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3564, 'eval_samples_per_second': 642.077, 'eval_steps_per_second': 80.631, 'epoch': 9.0}
{'loss': 0.0183, 'grad_norm': 0.5317121148109436, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05806277319788933, 'eval_precision': 0.6514459665144596, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6697965571205008, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3663, 'eval_samples_per_second': 639.383, 'eval_steps_per_second': 80.293, 'epoch': 10.0}
{'loss': 0.0174, 'grad_norm': 0.27690017223358154, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05795837193727493, 'eval_precision': 0.6565349544072948, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6755277560594214, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.2929, 'eval_samples_per_second': 659.865, 'eval_steps_per_second': 82.865, 'epoch': 11.0}
{'loss': 0.0164, 'grad_norm': 0.5034694075584412, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05859718844294548, 'eval_precision': 0.649016641452345, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6692667706708268, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.2893, 'eval_samples_per_second': 660.893, 'eval_steps_per_second': 82.994, 'epoch': 12.0}
{'train_runtime': 393.4168, 'train_samples_per_second': 261.402, 'train_steps_per_second': 4.087, 'train_loss': 0.044458035954195466, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0445
  train_runtime            = 0:06:33.41
  train_samples            =       8570
  train_samples_per_second =    261.402
  train_steps_per_second   =      4.087
[{'loss': 0.2201, 'grad_norm': 0.45036742091178894, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.06372136622667313, 'eval_precision': 0.5842696629213483, 'eval_recall': 0.5861513687600645, 'eval_f1': 0.5852090032154341, 'eval_accuracy': 0.9815922147965764, 'eval_runtime': 2.4913, 'eval_samples_per_second': 607.307, 'eval_steps_per_second': 76.265, 'epoch': 1.0, 'step': 134}, {'loss': 0.0546, 'grad_norm': 0.6595871448516846, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05450291186571121, 'eval_precision': 0.5907692307692308, 'eval_recall': 0.6183574879227053, 'eval_f1': 0.6042486231313926, 'eval_accuracy': 0.9842498143588541, 'eval_runtime': 2.3386, 'eval_samples_per_second': 646.982, 'eval_steps_per_second': 81.247, 'epoch': 2.0, 'step': 268}, {'loss': 0.0431, 'grad_norm': 0.41279610991477966, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.050148606300354004, 'eval_precision': 0.6203288490284006, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6434108527131782, 'eval_accuracy': 0.9854613671004807, 'eval_runtime': 2.3265, 'eval_samples_per_second': 650.343, 'eval_steps_per_second': 81.669, 'epoch': 3.0, 'step': 402}, {'loss': 0.0363, 'grad_norm': 0.48746538162231445, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.052026696503162384, 'eval_precision': 0.6757624398073836, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6768488745980707, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3166, 'eval_samples_per_second': 653.11, 'eval_steps_per_second': 82.016, 'epoch': 4.0, 'step': 536}, {'loss': 0.032, 'grad_norm': 0.2688318192958832, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05010435730218887, 'eval_precision': 0.6724137931034483, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6814932486100079, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3075, 'eval_samples_per_second': 655.679, 'eval_steps_per_second': 82.339, 'epoch': 5.0, 'step': 670}, {'loss': 0.0282, 'grad_norm': 0.5151277780532837, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05253152921795845, 'eval_precision': 0.6751592356687898, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6789431545236189, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.303, 'eval_samples_per_second': 656.977, 'eval_steps_per_second': 82.502, 'epoch': 6.0, 'step': 804}, {'loss': 0.0248, 'grad_norm': 0.5214818716049194, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05392344295978546, 'eval_precision': 0.6813186813186813, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6899841017488076, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 2.3875, 'eval_samples_per_second': 633.716, 'eval_steps_per_second': 79.581, 'epoch': 7.0, 'step': 938}, {'loss': 0.0222, 'grad_norm': 0.32153090834617615, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.0551890954375267, 'eval_precision': 0.649923896499239, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6682316118935836, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3586, 'eval_samples_per_second': 641.486, 'eval_steps_per_second': 80.557, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0202, 'grad_norm': 0.33476391434669495, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05612513795495033, 'eval_precision': 0.6646153846153846, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6797797010228167, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3564, 'eval_samples_per_second': 642.077, 'eval_steps_per_second': 80.631, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0183, 'grad_norm': 0.5317121148109436, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05806277319788933, 'eval_precision': 0.6514459665144596, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6697965571205008, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3663, 'eval_samples_per_second': 639.383, 'eval_steps_per_second': 80.293, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0174, 'grad_norm': 0.27690017223358154, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05795837193727493, 'eval_precision': 0.6565349544072948, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6755277560594214, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.2929, 'eval_samples_per_second': 659.865, 'eval_steps_per_second': 82.865, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0164, 'grad_norm': 0.5034694075584412, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05859718844294548, 'eval_precision': 0.649016641452345, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6692667706708268, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 2.2893, 'eval_samples_per_second': 660.893, 'eval_steps_per_second': 82.994, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 393.4168, 'train_samples_per_second': 261.402, 'train_steps_per_second': 4.087, 'total_flos': 4435051633690860.0, 'train_loss': 0.044458035954195466, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9885
  predict_f1                 =     0.6923
  predict_loss               =     0.0442
  predict_precision          =      0.675
  predict_recall             =     0.7105
  predict_runtime            = 0:00:02.03
  predict_samples_per_second =    615.928
  predict_steps_per_second   =     77.237
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_505.json completed. F1: 0.6923076923076923
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_202.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7148.27 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:00, 6853.32 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6896.96 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 7042.51 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 7056.79 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7180.65 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:00<00:00, 7128.63 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7247.06 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6847.91 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7154.85 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5420.57 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 4953.88 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4460.91 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-670 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1061, 'grad_norm': 0.9170430898666382, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.054118528962135315, 'eval_precision': 0.5234042553191489, 'eval_recall': 0.5942028985507246, 'eval_f1': 0.5565610859728507, 'eval_accuracy': 0.9833509203892602, 'eval_runtime': 2.3341, 'eval_samples_per_second': 648.229, 'eval_steps_per_second': 81.404, 'epoch': 1.0}
{'loss': 0.0421, 'grad_norm': 0.4416520893573761, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.05047805234789848, 'eval_precision': 0.6452119309262166, 'eval_recall': 0.6618357487922706, 'eval_f1': 0.6534181240063593, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3083, 'eval_samples_per_second': 655.464, 'eval_steps_per_second': 82.312, 'epoch': 2.0}
{'loss': 0.0294, 'grad_norm': 0.274800181388855, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05725981295108795, 'eval_precision': 0.6554621848739496, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.6414473684210527, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3113, 'eval_samples_per_second': 654.603, 'eval_steps_per_second': 82.204, 'epoch': 3.0}
{'loss': 0.0195, 'grad_norm': 0.3888172209262848, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05932645499706268, 'eval_precision': 0.638680659670165, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6614906832298137, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 2.3268, 'eval_samples_per_second': 650.24, 'eval_steps_per_second': 81.656, 'epoch': 4.0}
{'loss': 0.0129, 'grad_norm': 0.5731501579284668, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06859194487333298, 'eval_precision': 0.6501547987616099, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6629834254143646, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.3015, 'eval_samples_per_second': 657.395, 'eval_steps_per_second': 82.555, 'epoch': 5.0}
{'loss': 0.0088, 'grad_norm': 0.7961053252220154, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07249800860881805, 'eval_precision': 0.6328125, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6423473433782713, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3275, 'eval_samples_per_second': 650.063, 'eval_steps_per_second': 81.634, 'epoch': 6.0}
{'loss': 0.0067, 'grad_norm': 0.4677841067314148, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07835308462381363, 'eval_precision': 0.6376582278481012, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6432561851556265, 'eval_accuracy': 0.9848751319029194, 'eval_runtime': 2.3137, 'eval_samples_per_second': 653.933, 'eval_steps_per_second': 82.12, 'epoch': 7.0}
{'loss': 0.0042, 'grad_norm': 0.11755815148353577, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07378526777029037, 'eval_precision': 0.660828025477707, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6645316253002402, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3376, 'eval_samples_per_second': 647.24, 'eval_steps_per_second': 81.279, 'epoch': 8.0}
{'loss': 0.0027, 'grad_norm': 0.2238919734954834, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.0800117626786232, 'eval_precision': 0.6428571428571429, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6614542611415168, 'eval_accuracy': 0.9856567788330012, 'eval_runtime': 2.3271, 'eval_samples_per_second': 650.153, 'eval_steps_per_second': 81.645, 'epoch': 9.0}
{'loss': 0.0019, 'grad_norm': 0.2631736397743225, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.0816313847899437, 'eval_precision': 0.6593059936908517, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6661354581673307, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 2.3322, 'eval_samples_per_second': 648.752, 'eval_steps_per_second': 81.469, 'epoch': 10.0}
{'loss': 0.0014, 'grad_norm': 0.18198469281196594, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08519595861434937, 'eval_precision': 0.6439628482972136, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6566692975532754, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 2.3579, 'eval_samples_per_second': 641.678, 'eval_steps_per_second': 80.581, 'epoch': 11.0}
{'loss': 0.0012, 'grad_norm': 0.19006621837615967, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08443936705589294, 'eval_precision': 0.6614420062695925, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6703733121525021, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.4217, 'eval_samples_per_second': 624.774, 'eval_steps_per_second': 78.458, 'epoch': 12.0}
{'train_runtime': 390.7228, 'train_samples_per_second': 263.205, 'train_steps_per_second': 4.115, 'train_loss': 0.019743600795369836, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0197
  train_runtime            = 0:06:30.72
  train_samples            =       8570
  train_samples_per_second =    263.205
  train_steps_per_second   =      4.115
[{'loss': 0.1061, 'grad_norm': 0.9170430898666382, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.054118528962135315, 'eval_precision': 0.5234042553191489, 'eval_recall': 0.5942028985507246, 'eval_f1': 0.5565610859728507, 'eval_accuracy': 0.9833509203892602, 'eval_runtime': 2.3341, 'eval_samples_per_second': 648.229, 'eval_steps_per_second': 81.404, 'epoch': 1.0, 'step': 134}, {'loss': 0.0421, 'grad_norm': 0.4416520893573761, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05047805234789848, 'eval_precision': 0.6452119309262166, 'eval_recall': 0.6618357487922706, 'eval_f1': 0.6534181240063593, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3083, 'eval_samples_per_second': 655.464, 'eval_steps_per_second': 82.312, 'epoch': 2.0, 'step': 268}, {'loss': 0.0294, 'grad_norm': 0.274800181388855, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.05725981295108795, 'eval_precision': 0.6554621848739496, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.6414473684210527, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3113, 'eval_samples_per_second': 654.603, 'eval_steps_per_second': 82.204, 'epoch': 3.0, 'step': 402}, {'loss': 0.0195, 'grad_norm': 0.3888172209262848, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05932645499706268, 'eval_precision': 0.638680659670165, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6614906832298137, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 2.3268, 'eval_samples_per_second': 650.24, 'eval_steps_per_second': 81.656, 'epoch': 4.0, 'step': 536}, {'loss': 0.0129, 'grad_norm': 0.5731501579284668, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.06859194487333298, 'eval_precision': 0.6501547987616099, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6629834254143646, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.3015, 'eval_samples_per_second': 657.395, 'eval_steps_per_second': 82.555, 'epoch': 5.0, 'step': 670}, {'loss': 0.0088, 'grad_norm': 0.7961053252220154, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.07249800860881805, 'eval_precision': 0.6328125, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6423473433782713, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3275, 'eval_samples_per_second': 650.063, 'eval_steps_per_second': 81.634, 'epoch': 6.0, 'step': 804}, {'loss': 0.0067, 'grad_norm': 0.4677841067314148, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.07835308462381363, 'eval_precision': 0.6376582278481012, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6432561851556265, 'eval_accuracy': 0.9848751319029194, 'eval_runtime': 2.3137, 'eval_samples_per_second': 653.933, 'eval_steps_per_second': 82.12, 'epoch': 7.0, 'step': 938}, {'loss': 0.0042, 'grad_norm': 0.11755815148353577, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07378526777029037, 'eval_precision': 0.660828025477707, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6645316253002402, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3376, 'eval_samples_per_second': 647.24, 'eval_steps_per_second': 81.279, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0027, 'grad_norm': 0.2238919734954834, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.0800117626786232, 'eval_precision': 0.6428571428571429, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6614542611415168, 'eval_accuracy': 0.9856567788330012, 'eval_runtime': 2.3271, 'eval_samples_per_second': 650.153, 'eval_steps_per_second': 81.645, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0019, 'grad_norm': 0.2631736397743225, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.0816313847899437, 'eval_precision': 0.6593059936908517, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6661354581673307, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 2.3322, 'eval_samples_per_second': 648.752, 'eval_steps_per_second': 81.469, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0014, 'grad_norm': 0.18198469281196594, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.08519595861434937, 'eval_precision': 0.6439628482972136, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6566692975532754, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 2.3579, 'eval_samples_per_second': 641.678, 'eval_steps_per_second': 80.581, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0012, 'grad_norm': 0.19006621837615967, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.08443936705589294, 'eval_precision': 0.6614420062695925, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6703733121525021, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.4217, 'eval_samples_per_second': 624.774, 'eval_steps_per_second': 78.458, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 390.7228, 'train_samples_per_second': 263.205, 'train_steps_per_second': 4.115, 'total_flos': 4417653570020508.0, 'train_loss': 0.019743600795369836, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9868
  predict_f1                 =     0.6479
  predict_loss               =     0.0447
  predict_precision          =     0.6383
  predict_recall             =     0.6579
  predict_runtime            = 0:00:01.95
  predict_samples_per_second =    639.542
  predict_steps_per_second   =     80.198
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_202.json completed. F1: 0.6479481641468683
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_505.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5754.80 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4993.57 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5416.20 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6016.67 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6368.25 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6680.19 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6782.94 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6998.31 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5977.62 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7232.97 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5487.27 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6342.39 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4817.86 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1057, 'grad_norm': 0.45268335938453674, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.051437508314847946, 'eval_precision': 0.6024844720496895, 'eval_recall': 0.6247987117552335, 'eval_f1': 0.6134387351778656, 'eval_accuracy': 0.9847969672099113, 'eval_runtime': 2.376, 'eval_samples_per_second': 636.793, 'eval_steps_per_second': 79.967, 'epoch': 1.0}
{'loss': 0.0403, 'grad_norm': 0.34168726205825806, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.05037146061658859, 'eval_precision': 0.6525157232704403, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6603023070803501, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3207, 'eval_samples_per_second': 651.964, 'eval_steps_per_second': 81.873, 'epoch': 2.0}
{'loss': 0.0284, 'grad_norm': 0.2879590690135956, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.0458005927503109, 'eval_precision': 0.6806853582554517, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6920031670625495, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 2.334, 'eval_samples_per_second': 648.247, 'eval_steps_per_second': 81.406, 'epoch': 3.0}
{'loss': 0.0194, 'grad_norm': 0.8587613701820374, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05028955638408661, 'eval_precision': 0.6548148148148148, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6820987654320988, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3453, 'eval_samples_per_second': 645.122, 'eval_steps_per_second': 81.013, 'epoch': 4.0}
{'loss': 0.0128, 'grad_norm': 0.10261209309101105, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.056799862533807755, 'eval_precision': 0.6526479750778816, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6634996041171813, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 2.3179, 'eval_samples_per_second': 652.751, 'eval_steps_per_second': 81.971, 'epoch': 5.0}
{'loss': 0.0093, 'grad_norm': 0.16895420849323273, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06003623455762863, 'eval_precision': 0.6470588235294118, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6764027671022291, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.3163, 'eval_samples_per_second': 653.202, 'eval_steps_per_second': 82.028, 'epoch': 6.0}
{'loss': 0.0061, 'grad_norm': 0.11497636884450912, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06600208580493927, 'eval_precision': 0.6289592760180995, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6495327102803738, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.5242, 'eval_samples_per_second': 599.387, 'eval_steps_per_second': 75.27, 'epoch': 7.0}
{'loss': 0.0041, 'grad_norm': 0.4122104346752167, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06877586245536804, 'eval_precision': 0.6461538461538462, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6608969315499607, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3178, 'eval_samples_per_second': 652.774, 'eval_steps_per_second': 81.974, 'epoch': 8.0}
{'loss': 0.0023, 'grad_norm': 0.005729821044951677, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07030519843101501, 'eval_precision': 0.6611195158850227, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6817472698907957, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 2.3223, 'eval_samples_per_second': 651.507, 'eval_steps_per_second': 81.815, 'epoch': 9.0}
{'loss': 0.0018, 'grad_norm': 0.037050843238830566, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07284364849328995, 'eval_precision': 0.6527777777777778, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6666666666666667, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.4984, 'eval_samples_per_second': 605.578, 'eval_steps_per_second': 76.048, 'epoch': 10.0}
{'loss': 0.0011, 'grad_norm': 0.008754194714128971, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07289862632751465, 'eval_precision': 0.6594761171032357, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6740157480314961, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.3071, 'eval_samples_per_second': 655.801, 'eval_steps_per_second': 82.354, 'epoch': 11.0}
{'loss': 0.001, 'grad_norm': 0.15233978629112244, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0734642818570137, 'eval_precision': 0.6569230769230769, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6719118804091266, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3936, 'eval_samples_per_second': 632.104, 'eval_steps_per_second': 79.379, 'epoch': 12.0}
{'train_runtime': 393.1928, 'train_samples_per_second': 261.551, 'train_steps_per_second': 4.09, 'train_loss': 0.01936912875789315, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0194
  train_runtime            = 0:06:33.19
  train_samples            =       8570
  train_samples_per_second =    261.551
  train_steps_per_second   =       4.09
[{'loss': 0.1057, 'grad_norm': 0.45268335938453674, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.051437508314847946, 'eval_precision': 0.6024844720496895, 'eval_recall': 0.6247987117552335, 'eval_f1': 0.6134387351778656, 'eval_accuracy': 0.9847969672099113, 'eval_runtime': 2.376, 'eval_samples_per_second': 636.793, 'eval_steps_per_second': 79.967, 'epoch': 1.0, 'step': 134}, {'loss': 0.0403, 'grad_norm': 0.34168726205825806, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05037146061658859, 'eval_precision': 0.6525157232704403, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6603023070803501, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3207, 'eval_samples_per_second': 651.964, 'eval_steps_per_second': 81.873, 'epoch': 2.0, 'step': 268}, {'loss': 0.0284, 'grad_norm': 0.2879590690135956, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.0458005927503109, 'eval_precision': 0.6806853582554517, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6920031670625495, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 2.334, 'eval_samples_per_second': 648.247, 'eval_steps_per_second': 81.406, 'epoch': 3.0, 'step': 402}, {'loss': 0.0194, 'grad_norm': 0.8587613701820374, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05028955638408661, 'eval_precision': 0.6548148148148148, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6820987654320988, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3453, 'eval_samples_per_second': 645.122, 'eval_steps_per_second': 81.013, 'epoch': 4.0, 'step': 536}, {'loss': 0.0128, 'grad_norm': 0.10261209309101105, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.056799862533807755, 'eval_precision': 0.6526479750778816, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6634996041171813, 'eval_accuracy': 0.9865947551490991, 'eval_runtime': 2.3179, 'eval_samples_per_second': 652.751, 'eval_steps_per_second': 81.971, 'epoch': 5.0, 'step': 670}, {'loss': 0.0093, 'grad_norm': 0.16895420849323273, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06003623455762863, 'eval_precision': 0.6470588235294118, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6764027671022291, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.3163, 'eval_samples_per_second': 653.202, 'eval_steps_per_second': 82.028, 'epoch': 6.0, 'step': 804}, {'loss': 0.0061, 'grad_norm': 0.11497636884450912, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06600208580493927, 'eval_precision': 0.6289592760180995, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6495327102803738, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.5242, 'eval_samples_per_second': 599.387, 'eval_steps_per_second': 75.27, 'epoch': 7.0, 'step': 938}, {'loss': 0.0041, 'grad_norm': 0.4122104346752167, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.06877586245536804, 'eval_precision': 0.6461538461538462, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6608969315499607, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3178, 'eval_samples_per_second': 652.774, 'eval_steps_per_second': 81.974, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0023, 'grad_norm': 0.005729821044951677, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.07030519843101501, 'eval_precision': 0.6611195158850227, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6817472698907957, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 2.3223, 'eval_samples_per_second': 651.507, 'eval_steps_per_second': 81.815, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0018, 'grad_norm': 0.037050843238830566, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07284364849328995, 'eval_precision': 0.6527777777777778, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6666666666666667, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.4984, 'eval_samples_per_second': 605.578, 'eval_steps_per_second': 76.048, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0011, 'grad_norm': 0.008754194714128971, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07289862632751465, 'eval_precision': 0.6594761171032357, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6740157480314961, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.3071, 'eval_samples_per_second': 655.801, 'eval_steps_per_second': 82.354, 'epoch': 11.0, 'step': 1474}, {'loss': 0.001, 'grad_norm': 0.15233978629112244, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.0734642818570137, 'eval_precision': 0.6569230769230769, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6719118804091266, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3936, 'eval_samples_per_second': 632.104, 'eval_steps_per_second': 79.379, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 393.1928, 'train_samples_per_second': 261.551, 'train_steps_per_second': 4.09, 'total_flos': 4435051633690860.0, 'train_loss': 0.01936912875789315, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9896
  predict_f1                 =     0.7189
  predict_loss               =     0.0439
  predict_precision          =     0.7038
  predict_recall             =     0.7346
  predict_runtime            = 0:00:01.94
  predict_samples_per_second =    642.865
  predict_steps_per_second   =     80.615
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_505.json completed. F1: 0.7188841201716738
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_202.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6754.12 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5713.99 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6046.88 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5837.44 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6313.48 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6720.76 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6840.67 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7037.41 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6307.83 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7205.84 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4938.85 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7090.82 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2826.53 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0863, 'grad_norm': 0.7202567458152771, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05194402486085892, 'eval_precision': 0.6568144499178982, 'eval_recall': 0.644122383252818, 'eval_f1': 0.6504065040650406, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 4.6125, 'eval_samples_per_second': 328.024, 'eval_steps_per_second': 41.193, 'epoch': 1.0}
{'loss': 0.0356, 'grad_norm': 0.498329758644104, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.047959305346012115, 'eval_precision': 0.6751188589540412, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6805111821086263, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.6045, 'eval_samples_per_second': 328.591, 'eval_steps_per_second': 41.264, 'epoch': 2.0}
{'loss': 0.0222, 'grad_norm': 0.20843660831451416, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.051260918378829956, 'eval_precision': 0.7038834951456311, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.7021791767554479, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.6041, 'eval_samples_per_second': 328.621, 'eval_steps_per_second': 41.268, 'epoch': 3.0}
{'loss': 0.0127, 'grad_norm': 0.2977316677570343, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05300576239824295, 'eval_precision': 0.6481481481481481, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6619385342789598, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6124, 'eval_samples_per_second': 328.03, 'eval_steps_per_second': 41.194, 'epoch': 4.0}
{'loss': 0.0071, 'grad_norm': 0.21729065477848053, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05639380216598511, 'eval_precision': 0.6792452830188679, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6873508353221958, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.5968, 'eval_samples_per_second': 329.142, 'eval_steps_per_second': 41.333, 'epoch': 5.0}
{'loss': 0.004, 'grad_norm': 0.06378750503063202, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06379778683185577, 'eval_precision': 0.7115689381933439, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7172523961661342, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.6143, 'eval_samples_per_second': 327.893, 'eval_steps_per_second': 41.176, 'epoch': 6.0}
{'loss': 0.002, 'grad_norm': 0.26467710733413696, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07118996232748032, 'eval_precision': 0.6646058732612056, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6782334384858044, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.6274, 'eval_samples_per_second': 326.966, 'eval_steps_per_second': 41.06, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.01615414209663868, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06960152089595795, 'eval_precision': 0.6729559748427673, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.680986475735879, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6213, 'eval_samples_per_second': 327.394, 'eval_steps_per_second': 41.114, 'epoch': 8.0}
{'loss': 0.0009, 'grad_norm': 0.4235033690929413, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07248203456401825, 'eval_precision': 0.6702954898911353, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6819620253164557, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.6101, 'eval_samples_per_second': 328.19, 'eval_steps_per_second': 41.214, 'epoch': 9.0}
{'loss': 0.0004, 'grad_norm': 0.06096704676747322, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07350713014602661, 'eval_precision': 0.6641337386018237, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6833463643471461, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 4.6378, 'eval_samples_per_second': 326.231, 'eval_steps_per_second': 40.968, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.0076914578676223755, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07670731842517853, 'eval_precision': 0.6806853582554517, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6920031670625495, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.609, 'eval_samples_per_second': 328.271, 'eval_steps_per_second': 41.224, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.007384743075817823, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07762584835290909, 'eval_precision': 0.6817472698907956, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6925515055467512, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6125, 'eval_samples_per_second': 328.021, 'eval_steps_per_second': 41.192, 'epoch': 12.0}
{'train_runtime': 999.5311, 'train_samples_per_second': 102.888, 'train_steps_per_second': 1.609, 'train_loss': 0.014381657537554776, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0144
  train_runtime            = 0:16:39.53
  train_samples            =       8570
  train_samples_per_second =    102.888
  train_steps_per_second   =      1.609
[{'loss': 0.0863, 'grad_norm': 0.7202567458152771, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05194402486085892, 'eval_precision': 0.6568144499178982, 'eval_recall': 0.644122383252818, 'eval_f1': 0.6504065040650406, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 4.6125, 'eval_samples_per_second': 328.024, 'eval_steps_per_second': 41.193, 'epoch': 1.0, 'step': 134}, {'loss': 0.0356, 'grad_norm': 0.498329758644104, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.047959305346012115, 'eval_precision': 0.6751188589540412, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6805111821086263, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.6045, 'eval_samples_per_second': 328.591, 'eval_steps_per_second': 41.264, 'epoch': 2.0, 'step': 268}, {'loss': 0.0222, 'grad_norm': 0.20843660831451416, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.051260918378829956, 'eval_precision': 0.7038834951456311, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.7021791767554479, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.6041, 'eval_samples_per_second': 328.621, 'eval_steps_per_second': 41.268, 'epoch': 3.0, 'step': 402}, {'loss': 0.0127, 'grad_norm': 0.2977316677570343, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05300576239824295, 'eval_precision': 0.6481481481481481, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6619385342789598, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6124, 'eval_samples_per_second': 328.03, 'eval_steps_per_second': 41.194, 'epoch': 4.0, 'step': 536}, {'loss': 0.0071, 'grad_norm': 0.21729065477848053, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05639380216598511, 'eval_precision': 0.6792452830188679, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6873508353221958, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.5968, 'eval_samples_per_second': 329.142, 'eval_steps_per_second': 41.333, 'epoch': 5.0, 'step': 670}, {'loss': 0.004, 'grad_norm': 0.06378750503063202, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06379778683185577, 'eval_precision': 0.7115689381933439, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7172523961661342, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.6143, 'eval_samples_per_second': 327.893, 'eval_steps_per_second': 41.176, 'epoch': 6.0, 'step': 804}, {'loss': 0.002, 'grad_norm': 0.26467710733413696, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.07118996232748032, 'eval_precision': 0.6646058732612056, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6782334384858044, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.6274, 'eval_samples_per_second': 326.966, 'eval_steps_per_second': 41.06, 'epoch': 7.0, 'step': 938}, {'loss': 0.001, 'grad_norm': 0.01615414209663868, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.06960152089595795, 'eval_precision': 0.6729559748427673, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.680986475735879, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6213, 'eval_samples_per_second': 327.394, 'eval_steps_per_second': 41.114, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0009, 'grad_norm': 0.4235033690929413, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.07248203456401825, 'eval_precision': 0.6702954898911353, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6819620253164557, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.6101, 'eval_samples_per_second': 328.19, 'eval_steps_per_second': 41.214, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0004, 'grad_norm': 0.06096704676747322, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07350713014602661, 'eval_precision': 0.6641337386018237, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6833463643471461, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 4.6378, 'eval_samples_per_second': 326.231, 'eval_steps_per_second': 40.968, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0002, 'grad_norm': 0.0076914578676223755, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07670731842517853, 'eval_precision': 0.6806853582554517, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6920031670625495, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.609, 'eval_samples_per_second': 328.271, 'eval_steps_per_second': 41.224, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0002, 'grad_norm': 0.007384743075817823, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07762584835290909, 'eval_precision': 0.6817472698907956, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6925515055467512, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6125, 'eval_samples_per_second': 328.021, 'eval_steps_per_second': 41.192, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 999.5311, 'train_samples_per_second': 102.888, 'train_steps_per_second': 1.609, 'total_flos': 1.2881744067011148e+16, 'train_loss': 0.014381657537554776, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =      0.989
  predict_f1                 =     0.6939
  predict_loss               =     0.0418
  predict_precision          =       0.68
  predict_recall             =     0.7083
  predict_runtime            = 0:00:03.90
  predict_samples_per_second =    320.817
  predict_steps_per_second   =      40.23
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_202.json completed. F1: 0.6938775510204083
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_101.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6541.63 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4404.35 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5301.46 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5194.53 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5789.60 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6331.96 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6570.74 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6878.83 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5747.66 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6766.01 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4380.08 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7317.09 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5872.99 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1112, 'grad_norm': 0.8980017900466919, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04918194189667702, 'eval_precision': 0.6484751203852327, 'eval_recall': 0.6505636070853462, 'eval_f1': 0.6495176848874598, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 4.6674, 'eval_samples_per_second': 324.161, 'eval_steps_per_second': 40.708, 'epoch': 1.0}
{'loss': 0.0396, 'grad_norm': 0.5960665345191956, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.041929084807634354, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 5.1203, 'eval_samples_per_second': 295.49, 'eval_steps_per_second': 37.107, 'epoch': 2.0}
{'loss': 0.0276, 'grad_norm': 0.23673562705516815, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04321936517953873, 'eval_precision': 0.7003058103975535, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7184313725490195, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6163, 'eval_samples_per_second': 327.753, 'eval_steps_per_second': 41.159, 'epoch': 3.0}
{'loss': 0.0202, 'grad_norm': 0.6991223096847534, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.044020574539899826, 'eval_precision': 0.6952526799387443, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7127158555729984, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6115, 'eval_samples_per_second': 328.091, 'eval_steps_per_second': 41.201, 'epoch': 4.0}
{'loss': 0.0143, 'grad_norm': 0.6190193891525269, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.045648545026779175, 'eval_precision': 0.6973058637083994, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7028753993610224, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.614, 'eval_samples_per_second': 327.913, 'eval_steps_per_second': 41.179, 'epoch': 5.0}
{'loss': 0.0102, 'grad_norm': 0.9071446061134338, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.048782914876937866, 'eval_precision': 0.7169517884914464, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7294303797468354, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.6169, 'eval_samples_per_second': 327.71, 'eval_steps_per_second': 41.153, 'epoch': 6.0}
{'loss': 0.0074, 'grad_norm': 0.2503450810909271, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05081969499588013, 'eval_precision': 0.6953125, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7057890563045202, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6095, 'eval_samples_per_second': 328.236, 'eval_steps_per_second': 41.219, 'epoch': 7.0}
{'loss': 0.0057, 'grad_norm': 0.14396622776985168, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05371612310409546, 'eval_precision': 0.7025316455696202, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7086991221069434, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.5952, 'eval_samples_per_second': 329.256, 'eval_steps_per_second': 41.347, 'epoch': 8.0}
{'loss': 0.0045, 'grad_norm': 0.41628286242485046, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.056471895426511765, 'eval_precision': 0.6848673946957878, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6957210776545167, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6494, 'eval_samples_per_second': 325.417, 'eval_steps_per_second': 40.865, 'epoch': 9.0}
{'loss': 0.0038, 'grad_norm': 0.26417434215545654, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.055682841688394547, 'eval_precision': 0.706436420722135, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7154213036565977, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6061, 'eval_samples_per_second': 328.477, 'eval_steps_per_second': 41.25, 'epoch': 10.0}
{'loss': 0.003, 'grad_norm': 0.2600628137588501, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.0588892437517643, 'eval_precision': 0.68125, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6915146708961143, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.6166, 'eval_samples_per_second': 327.73, 'eval_steps_per_second': 41.156, 'epoch': 11.0}
{'loss': 0.0025, 'grad_norm': 0.32706061005592346, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05770585685968399, 'eval_precision': 0.6920684292379471, 'eval_recall': 0.71658615136876, 'eval_f1': 0.704113924050633, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.6099, 'eval_samples_per_second': 328.209, 'eval_steps_per_second': 41.216, 'epoch': 12.0}
{'train_runtime': 966.7438, 'train_samples_per_second': 106.378, 'train_steps_per_second': 3.327, 'train_loss': 0.020848697486949796, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0208
  train_runtime            = 0:16:06.74
  train_samples            =       8570
  train_samples_per_second =    106.378
  train_steps_per_second   =      3.327
[{'loss': 0.1112, 'grad_norm': 0.8980017900466919, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04918194189667702, 'eval_precision': 0.6484751203852327, 'eval_recall': 0.6505636070853462, 'eval_f1': 0.6495176848874598, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 4.6674, 'eval_samples_per_second': 324.161, 'eval_steps_per_second': 40.708, 'epoch': 1.0, 'step': 268}, {'loss': 0.0396, 'grad_norm': 0.5960665345191956, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.041929084807634354, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 5.1203, 'eval_samples_per_second': 295.49, 'eval_steps_per_second': 37.107, 'epoch': 2.0, 'step': 536}, {'loss': 0.0276, 'grad_norm': 0.23673562705516815, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04321936517953873, 'eval_precision': 0.7003058103975535, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7184313725490195, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6163, 'eval_samples_per_second': 327.753, 'eval_steps_per_second': 41.159, 'epoch': 3.0, 'step': 804}, {'loss': 0.0202, 'grad_norm': 0.6991223096847534, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.044020574539899826, 'eval_precision': 0.6952526799387443, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7127158555729984, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6115, 'eval_samples_per_second': 328.091, 'eval_steps_per_second': 41.201, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0143, 'grad_norm': 0.6190193891525269, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.045648545026779175, 'eval_precision': 0.6973058637083994, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7028753993610224, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.614, 'eval_samples_per_second': 327.913, 'eval_steps_per_second': 41.179, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0102, 'grad_norm': 0.9071446061134338, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.048782914876937866, 'eval_precision': 0.7169517884914464, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7294303797468354, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.6169, 'eval_samples_per_second': 327.71, 'eval_steps_per_second': 41.153, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0074, 'grad_norm': 0.2503450810909271, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05081969499588013, 'eval_precision': 0.6953125, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7057890563045202, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6095, 'eval_samples_per_second': 328.236, 'eval_steps_per_second': 41.219, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0057, 'grad_norm': 0.14396622776985168, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05371612310409546, 'eval_precision': 0.7025316455696202, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7086991221069434, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.5952, 'eval_samples_per_second': 329.256, 'eval_steps_per_second': 41.347, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0045, 'grad_norm': 0.41628286242485046, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.056471895426511765, 'eval_precision': 0.6848673946957878, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6957210776545167, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6494, 'eval_samples_per_second': 325.417, 'eval_steps_per_second': 40.865, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0038, 'grad_norm': 0.26417434215545654, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.055682841688394547, 'eval_precision': 0.706436420722135, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7154213036565977, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6061, 'eval_samples_per_second': 328.477, 'eval_steps_per_second': 41.25, 'epoch': 10.0, 'step': 2680}, {'loss': 0.003, 'grad_norm': 0.2600628137588501, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.0588892437517643, 'eval_precision': 0.68125, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6915146708961143, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.6166, 'eval_samples_per_second': 327.73, 'eval_steps_per_second': 41.156, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0025, 'grad_norm': 0.32706061005592346, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.05770585685968399, 'eval_precision': 0.6920684292379471, 'eval_recall': 0.71658615136876, 'eval_f1': 0.704113924050633, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.6099, 'eval_samples_per_second': 328.209, 'eval_steps_per_second': 41.216, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 966.7438, 'train_samples_per_second': 106.378, 'train_steps_per_second': 3.327, 'total_flos': 1.1352534181638552e+16, 'train_loss': 0.020848697486949796, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9897
  predict_f1                 =     0.7034
  predict_loss               =     0.0381
  predict_precision          =     0.6803
  predict_recall             =     0.7281
  predict_runtime            = 0:00:03.94
  predict_samples_per_second =    317.724
  predict_steps_per_second   =     39.842
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_101.json completed. F1: 0.7033898305084746
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_505.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5869.71 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5357.71 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6257.13 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5679.10 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6165.65 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6605.04 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6755.67 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7008.96 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6174.71 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7282.37 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4140.38 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7240.95 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3769.56 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.09, 'grad_norm': 0.5520592331886292, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05254201218485832, 'eval_precision': 0.6055312954876274, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.636085626911315, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 4.6444, 'eval_samples_per_second': 325.768, 'eval_steps_per_second': 40.909, 'epoch': 1.0}
{'loss': 0.0374, 'grad_norm': 0.5720602869987488, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.047502148896455765, 'eval_precision': 0.6772046589018302, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6661211129296235, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 4.6295, 'eval_samples_per_second': 326.82, 'eval_steps_per_second': 41.042, 'epoch': 2.0}
{'loss': 0.0222, 'grad_norm': 0.30846014618873596, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04473058134317398, 'eval_precision': 0.7006369426751592, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7045636509207367, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6431, 'eval_samples_per_second': 325.861, 'eval_steps_per_second': 40.921, 'epoch': 3.0}
{'loss': 0.0133, 'grad_norm': 0.41188758611679077, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.054575011134147644, 'eval_precision': 0.696969696969697, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.7003205128205129, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6212, 'eval_samples_per_second': 327.402, 'eval_steps_per_second': 41.115, 'epoch': 4.0}
{'loss': 0.0076, 'grad_norm': 0.015515459701418877, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.0565989650785923, 'eval_precision': 0.668693009118541, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6880375293197811, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 5.8721, 'eval_samples_per_second': 257.66, 'eval_steps_per_second': 32.357, 'epoch': 5.0}
{'loss': 0.0037, 'grad_norm': 0.7651618123054504, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06328466534614563, 'eval_precision': 0.6970633693972179, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7113564668769717, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6266, 'eval_samples_per_second': 327.025, 'eval_steps_per_second': 41.067, 'epoch': 6.0}
{'loss': 0.0021, 'grad_norm': 0.028242088854312897, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06678158044815063, 'eval_precision': 0.6961414790996785, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6967015285599356, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6114, 'eval_samples_per_second': 328.097, 'eval_steps_per_second': 41.202, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.04180285707116127, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07120511680841446, 'eval_precision': 0.6873015873015873, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6922462030375699, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6383, 'eval_samples_per_second': 326.195, 'eval_steps_per_second': 40.963, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.004957854747772217, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07513854652643204, 'eval_precision': 0.6928, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6950240770465489, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.613, 'eval_samples_per_second': 327.987, 'eval_steps_per_second': 41.188, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.22308818995952606, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07490387558937073, 'eval_precision': 0.7003205128205128, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.702008032128514, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6052, 'eval_samples_per_second': 328.539, 'eval_steps_per_second': 41.257, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.010269483551383018, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07605215907096863, 'eval_precision': 0.7006369426751592, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7045636509207367, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6116, 'eval_samples_per_second': 328.087, 'eval_steps_per_second': 41.201, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.018054217100143433, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07664895057678223, 'eval_precision': 0.7019230769230769, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7036144578313253, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6763, 'eval_samples_per_second': 323.546, 'eval_steps_per_second': 40.63, 'epoch': 12.0}
{'train_runtime': 986.0539, 'train_samples_per_second': 104.295, 'train_steps_per_second': 1.631, 'train_loss': 0.014870728440212077, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0149
  train_runtime            = 0:16:26.05
  train_samples            =       8570
  train_samples_per_second =    104.295
  train_steps_per_second   =      1.631
[{'loss': 0.09, 'grad_norm': 0.5520592331886292, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05254201218485832, 'eval_precision': 0.6055312954876274, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.636085626911315, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 4.6444, 'eval_samples_per_second': 325.768, 'eval_steps_per_second': 40.909, 'epoch': 1.0, 'step': 134}, {'loss': 0.0374, 'grad_norm': 0.5720602869987488, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.047502148896455765, 'eval_precision': 0.6772046589018302, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6661211129296235, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 4.6295, 'eval_samples_per_second': 326.82, 'eval_steps_per_second': 41.042, 'epoch': 2.0, 'step': 268}, {'loss': 0.0222, 'grad_norm': 0.30846014618873596, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04473058134317398, 'eval_precision': 0.7006369426751592, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7045636509207367, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6431, 'eval_samples_per_second': 325.861, 'eval_steps_per_second': 40.921, 'epoch': 3.0, 'step': 402}, {'loss': 0.0133, 'grad_norm': 0.41188758611679077, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.054575011134147644, 'eval_precision': 0.696969696969697, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.7003205128205129, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6212, 'eval_samples_per_second': 327.402, 'eval_steps_per_second': 41.115, 'epoch': 4.0, 'step': 536}, {'loss': 0.0076, 'grad_norm': 0.015515459701418877, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.0565989650785923, 'eval_precision': 0.668693009118541, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6880375293197811, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 5.8721, 'eval_samples_per_second': 257.66, 'eval_steps_per_second': 32.357, 'epoch': 5.0, 'step': 670}, {'loss': 0.0037, 'grad_norm': 0.7651618123054504, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06328466534614563, 'eval_precision': 0.6970633693972179, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7113564668769717, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6266, 'eval_samples_per_second': 327.025, 'eval_steps_per_second': 41.067, 'epoch': 6.0, 'step': 804}, {'loss': 0.0021, 'grad_norm': 0.028242088854312897, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06678158044815063, 'eval_precision': 0.6961414790996785, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6967015285599356, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6114, 'eval_samples_per_second': 328.097, 'eval_steps_per_second': 41.202, 'epoch': 7.0, 'step': 938}, {'loss': 0.001, 'grad_norm': 0.04180285707116127, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07120511680841446, 'eval_precision': 0.6873015873015873, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6922462030375699, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6383, 'eval_samples_per_second': 326.195, 'eval_steps_per_second': 40.963, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0005, 'grad_norm': 0.004957854747772217, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.07513854652643204, 'eval_precision': 0.6928, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6950240770465489, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.613, 'eval_samples_per_second': 327.987, 'eval_steps_per_second': 41.188, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0003, 'grad_norm': 0.22308818995952606, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07490387558937073, 'eval_precision': 0.7003205128205128, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.702008032128514, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6052, 'eval_samples_per_second': 328.539, 'eval_steps_per_second': 41.257, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0002, 'grad_norm': 0.010269483551383018, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07605215907096863, 'eval_precision': 0.7006369426751592, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.7045636509207367, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6116, 'eval_samples_per_second': 328.087, 'eval_steps_per_second': 41.201, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0002, 'grad_norm': 0.018054217100143433, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07664895057678223, 'eval_precision': 0.7019230769230769, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7036144578313253, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6763, 'eval_samples_per_second': 323.546, 'eval_steps_per_second': 40.63, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 986.0539, 'train_samples_per_second': 104.295, 'train_steps_per_second': 1.631, 'total_flos': 1.2900609003428748e+16, 'train_loss': 0.014870728440212077, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9899
  predict_f1                 =     0.7189
  predict_loss               =     0.0396
  predict_precision          =     0.7038
  predict_recall             =     0.7346
  predict_runtime            = 0:00:03.91
  predict_samples_per_second =     319.53
  predict_steps_per_second   =     40.069
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_505.json completed. F1: 0.7188841201716738
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_101.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4824.66 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4033.25 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4530.27 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5347.69 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5873.88 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6333.26 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6549.07 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6837.40 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5732.99 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7126.09 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4084.55 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7136.01 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3851.52 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2198, 'grad_norm': 0.2938629984855652, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.06352610141038895, 'eval_precision': 0.5666156202143952, 'eval_recall': 0.5958132045088567, 'eval_f1': 0.5808477237048667, 'eval_accuracy': 0.981396803064056, 'eval_runtime': 2.3735, 'eval_samples_per_second': 637.448, 'eval_steps_per_second': 80.05, 'epoch': 1.0}
{'loss': 0.0542, 'grad_norm': 0.30807289481163025, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.05387299135327339, 'eval_precision': 0.5437853107344632, 'eval_recall': 0.6199677938808373, 'eval_f1': 0.5793829947328818, 'eval_accuracy': 0.9839762379333256, 'eval_runtime': 2.3161, 'eval_samples_per_second': 653.244, 'eval_steps_per_second': 82.033, 'epoch': 2.0}
{'loss': 0.0435, 'grad_norm': 0.46016085147857666, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.049212418496608734, 'eval_precision': 0.6407914764079148, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6588419405320813, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.324, 'eval_samples_per_second': 651.021, 'eval_steps_per_second': 81.754, 'epoch': 3.0}
{'loss': 0.0368, 'grad_norm': 0.5866543650627136, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04848209396004677, 'eval_precision': 0.661993769470405, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6730007917656374, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 2.3197, 'eval_samples_per_second': 652.229, 'eval_steps_per_second': 81.906, 'epoch': 4.0}
{'loss': 0.0325, 'grad_norm': 0.30115535855293274, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.049094025045633316, 'eval_precision': 0.6426380368098159, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6582875098193245, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3348, 'eval_samples_per_second': 648.029, 'eval_steps_per_second': 81.378, 'epoch': 5.0}
{'loss': 0.0287, 'grad_norm': 0.3012583553791046, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.051751576364040375, 'eval_precision': 0.6505295007564297, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6708268330733228, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.3157, 'eval_samples_per_second': 653.365, 'eval_steps_per_second': 82.048, 'epoch': 6.0}
{'loss': 0.0257, 'grad_norm': 0.245159313082695, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.049747586250305176, 'eval_precision': 0.672360248447205, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6845849802371542, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 2.3218, 'eval_samples_per_second': 651.653, 'eval_steps_per_second': 81.834, 'epoch': 7.0}
{'loss': 0.0226, 'grad_norm': 0.6301508545875549, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05152660608291626, 'eval_precision': 0.6854460093896714, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6952380952380952, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 2.3254, 'eval_samples_per_second': 650.647, 'eval_steps_per_second': 81.707, 'epoch': 8.0}
{'loss': 0.0208, 'grad_norm': 0.48437607288360596, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05328461900353432, 'eval_precision': 0.6564885496183206, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6739811912225705, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3082, 'eval_samples_per_second': 655.498, 'eval_steps_per_second': 82.316, 'epoch': 9.0}
{'loss': 0.0189, 'grad_norm': 0.32311633229255676, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.054707422852516174, 'eval_precision': 0.6568778979907264, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6703470031545742, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3152, 'eval_samples_per_second': 653.497, 'eval_steps_per_second': 82.065, 'epoch': 10.0}
{'loss': 0.0176, 'grad_norm': 0.25024789571762085, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05649025738239288, 'eval_precision': 0.6614906832298136, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6735177865612648, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3184, 'eval_samples_per_second': 652.607, 'eval_steps_per_second': 81.953, 'epoch': 11.0}
{'loss': 0.017, 'grad_norm': 0.2894366681575775, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05601116642355919, 'eval_precision': 0.6702619414483821, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6850393700787402, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.4135, 'eval_samples_per_second': 626.885, 'eval_steps_per_second': 78.723, 'epoch': 12.0}
{'train_runtime': 394.7389, 'train_samples_per_second': 260.527, 'train_steps_per_second': 4.074, 'train_loss': 0.04483474398133767, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0448
  train_runtime            = 0:06:34.73
  train_samples            =       8570
  train_samples_per_second =    260.527
  train_steps_per_second   =      4.074
[{'loss': 0.2198, 'grad_norm': 0.2938629984855652, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.06352610141038895, 'eval_precision': 0.5666156202143952, 'eval_recall': 0.5958132045088567, 'eval_f1': 0.5808477237048667, 'eval_accuracy': 0.981396803064056, 'eval_runtime': 2.3735, 'eval_samples_per_second': 637.448, 'eval_steps_per_second': 80.05, 'epoch': 1.0, 'step': 134}, {'loss': 0.0542, 'grad_norm': 0.30807289481163025, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05387299135327339, 'eval_precision': 0.5437853107344632, 'eval_recall': 0.6199677938808373, 'eval_f1': 0.5793829947328818, 'eval_accuracy': 0.9839762379333256, 'eval_runtime': 2.3161, 'eval_samples_per_second': 653.244, 'eval_steps_per_second': 82.033, 'epoch': 2.0, 'step': 268}, {'loss': 0.0435, 'grad_norm': 0.46016085147857666, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.049212418496608734, 'eval_precision': 0.6407914764079148, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6588419405320813, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.324, 'eval_samples_per_second': 651.021, 'eval_steps_per_second': 81.754, 'epoch': 3.0, 'step': 402}, {'loss': 0.0368, 'grad_norm': 0.5866543650627136, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04848209396004677, 'eval_precision': 0.661993769470405, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6730007917656374, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 2.3197, 'eval_samples_per_second': 652.229, 'eval_steps_per_second': 81.906, 'epoch': 4.0, 'step': 536}, {'loss': 0.0325, 'grad_norm': 0.30115535855293274, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.049094025045633316, 'eval_precision': 0.6426380368098159, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6582875098193245, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3348, 'eval_samples_per_second': 648.029, 'eval_steps_per_second': 81.378, 'epoch': 5.0, 'step': 670}, {'loss': 0.0287, 'grad_norm': 0.3012583553791046, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.051751576364040375, 'eval_precision': 0.6505295007564297, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6708268330733228, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.3157, 'eval_samples_per_second': 653.365, 'eval_steps_per_second': 82.048, 'epoch': 6.0, 'step': 804}, {'loss': 0.0257, 'grad_norm': 0.245159313082695, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.049747586250305176, 'eval_precision': 0.672360248447205, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6845849802371542, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 2.3218, 'eval_samples_per_second': 651.653, 'eval_steps_per_second': 81.834, 'epoch': 7.0, 'step': 938}, {'loss': 0.0226, 'grad_norm': 0.6301508545875549, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05152660608291626, 'eval_precision': 0.6854460093896714, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6952380952380952, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 2.3254, 'eval_samples_per_second': 650.647, 'eval_steps_per_second': 81.707, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0208, 'grad_norm': 0.48437607288360596, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05328461900353432, 'eval_precision': 0.6564885496183206, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6739811912225705, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3082, 'eval_samples_per_second': 655.498, 'eval_steps_per_second': 82.316, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0189, 'grad_norm': 0.32311633229255676, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.054707422852516174, 'eval_precision': 0.6568778979907264, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6703470031545742, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3152, 'eval_samples_per_second': 653.497, 'eval_steps_per_second': 82.065, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0176, 'grad_norm': 0.25024789571762085, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05649025738239288, 'eval_precision': 0.6614906832298136, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6735177865612648, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3184, 'eval_samples_per_second': 652.607, 'eval_steps_per_second': 81.953, 'epoch': 11.0, 'step': 1474}, {'loss': 0.017, 'grad_norm': 0.2894366681575775, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05601116642355919, 'eval_precision': 0.6702619414483821, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6850393700787402, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.4135, 'eval_samples_per_second': 626.885, 'eval_steps_per_second': 78.723, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 394.7389, 'train_samples_per_second': 260.527, 'train_steps_per_second': 4.074, 'total_flos': 4438763166165948.0, 'train_loss': 0.04483474398133767, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9885
  predict_f1                 =     0.6957
  predict_loss               =     0.0448
  predict_precision          =     0.6735
  predict_recall             =     0.7193
  predict_runtime            = 0:00:01.96
  predict_samples_per_second =    637.448
  predict_steps_per_second   =     79.936
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_101.json completed. F1: 0.6956521739130435
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_202.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7207.83 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:00, 7272.46 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 7552.81 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6275.65 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6615.23 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6955.15 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6996.66 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7226.40 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6476.28 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6113.82 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4681.49 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7293.27 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4506.97 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1461, 'grad_norm': 0.6536840796470642, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05358082428574562, 'eval_precision': 0.5516717325227963, 'eval_recall': 0.5845410628019324, 'eval_f1': 0.5676309616888194, 'eval_accuracy': 0.98421073201235, 'eval_runtime': 4.628, 'eval_samples_per_second': 326.92, 'eval_steps_per_second': 41.054, 'epoch': 1.0}
{'loss': 0.0454, 'grad_norm': 0.442613810300827, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.045030079782009125, 'eval_precision': 0.6703296703296703, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6788553259141494, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.7518, 'eval_samples_per_second': 318.404, 'eval_steps_per_second': 39.985, 'epoch': 2.0}
{'loss': 0.0333, 'grad_norm': 0.5637587308883667, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.043189141899347305, 'eval_precision': 0.6773675762439807, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6784565916398714, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6249, 'eval_samples_per_second': 327.139, 'eval_steps_per_second': 41.082, 'epoch': 3.0}
{'loss': 0.0252, 'grad_norm': 0.372321754693985, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.043735429644584656, 'eval_precision': 0.7227564102564102, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7244979919678715, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6061, 'eval_samples_per_second': 328.479, 'eval_steps_per_second': 41.25, 'epoch': 4.0}
{'loss': 0.0188, 'grad_norm': 0.7091186046600342, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.047753747552633286, 'eval_precision': 0.6996951219512195, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7188723570869224, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6113, 'eval_samples_per_second': 328.106, 'eval_steps_per_second': 41.203, 'epoch': 5.0}
{'loss': 0.0152, 'grad_norm': 0.521572470664978, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.0486263781785965, 'eval_precision': 0.7109004739336493, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7177033492822965, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6206, 'eval_samples_per_second': 327.448, 'eval_steps_per_second': 41.12, 'epoch': 6.0}
{'loss': 0.0119, 'grad_norm': 0.5335208177566528, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.050013218075037, 'eval_precision': 0.7239747634069401, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7314741035856572, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.6326, 'eval_samples_per_second': 326.6, 'eval_steps_per_second': 41.014, 'epoch': 7.0}
{'loss': 0.0098, 'grad_norm': 0.2710111737251282, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.054306283593177795, 'eval_precision': 0.7262479871175523, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7262479871175523, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6479, 'eval_samples_per_second': 325.522, 'eval_steps_per_second': 40.878, 'epoch': 8.0}
{'loss': 0.0077, 'grad_norm': 0.6848440170288086, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.055450599640607834, 'eval_precision': 0.720125786163522, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7287191726332537, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6194, 'eval_samples_per_second': 327.528, 'eval_steps_per_second': 41.13, 'epoch': 9.0}
{'loss': 0.0068, 'grad_norm': 0.3103373050689697, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05787738412618637, 'eval_precision': 0.7296, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7319422150882825, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.6272, 'eval_samples_per_second': 326.976, 'eval_steps_per_second': 41.061, 'epoch': 10.0}
{'loss': 0.0058, 'grad_norm': 0.165609210729599, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05840716511011124, 'eval_precision': 0.7149606299212599, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7229299363057327, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.7044, 'eval_samples_per_second': 321.614, 'eval_steps_per_second': 40.388, 'epoch': 11.0}
{'loss': 0.0055, 'grad_norm': 0.07491851598024368, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05859686806797981, 'eval_precision': 0.722488038277512, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7259615384615384, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6312, 'eval_samples_per_second': 326.695, 'eval_steps_per_second': 41.026, 'epoch': 12.0}
{'train_runtime': 982.3084, 'train_samples_per_second': 104.692, 'train_steps_per_second': 1.637, 'train_loss': 0.027627885119238898, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0276
  train_runtime            = 0:16:22.30
  train_samples            =       8570
  train_samples_per_second =    104.692
  train_steps_per_second   =      1.637
[{'loss': 0.1461, 'grad_norm': 0.6536840796470642, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05358082428574562, 'eval_precision': 0.5516717325227963, 'eval_recall': 0.5845410628019324, 'eval_f1': 0.5676309616888194, 'eval_accuracy': 0.98421073201235, 'eval_runtime': 4.628, 'eval_samples_per_second': 326.92, 'eval_steps_per_second': 41.054, 'epoch': 1.0, 'step': 134}, {'loss': 0.0454, 'grad_norm': 0.442613810300827, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.045030079782009125, 'eval_precision': 0.6703296703296703, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6788553259141494, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.7518, 'eval_samples_per_second': 318.404, 'eval_steps_per_second': 39.985, 'epoch': 2.0, 'step': 268}, {'loss': 0.0333, 'grad_norm': 0.5637587308883667, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.043189141899347305, 'eval_precision': 0.6773675762439807, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6784565916398714, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6249, 'eval_samples_per_second': 327.139, 'eval_steps_per_second': 41.082, 'epoch': 3.0, 'step': 402}, {'loss': 0.0252, 'grad_norm': 0.372321754693985, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.043735429644584656, 'eval_precision': 0.7227564102564102, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7244979919678715, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6061, 'eval_samples_per_second': 328.479, 'eval_steps_per_second': 41.25, 'epoch': 4.0, 'step': 536}, {'loss': 0.0188, 'grad_norm': 0.7091186046600342, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.047753747552633286, 'eval_precision': 0.6996951219512195, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7188723570869224, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6113, 'eval_samples_per_second': 328.106, 'eval_steps_per_second': 41.203, 'epoch': 5.0, 'step': 670}, {'loss': 0.0152, 'grad_norm': 0.521572470664978, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.0486263781785965, 'eval_precision': 0.7109004739336493, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7177033492822965, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6206, 'eval_samples_per_second': 327.448, 'eval_steps_per_second': 41.12, 'epoch': 6.0, 'step': 804}, {'loss': 0.0119, 'grad_norm': 0.5335208177566528, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.050013218075037, 'eval_precision': 0.7239747634069401, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7314741035856572, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.6326, 'eval_samples_per_second': 326.6, 'eval_steps_per_second': 41.014, 'epoch': 7.0, 'step': 938}, {'loss': 0.0098, 'grad_norm': 0.2710111737251282, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.054306283593177795, 'eval_precision': 0.7262479871175523, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7262479871175523, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6479, 'eval_samples_per_second': 325.522, 'eval_steps_per_second': 40.878, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0077, 'grad_norm': 0.6848440170288086, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.055450599640607834, 'eval_precision': 0.720125786163522, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7287191726332537, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6194, 'eval_samples_per_second': 327.528, 'eval_steps_per_second': 41.13, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0068, 'grad_norm': 0.3103373050689697, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05787738412618637, 'eval_precision': 0.7296, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7319422150882825, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.6272, 'eval_samples_per_second': 326.976, 'eval_steps_per_second': 41.061, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0058, 'grad_norm': 0.165609210729599, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05840716511011124, 'eval_precision': 0.7149606299212599, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7229299363057327, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.7044, 'eval_samples_per_second': 321.614, 'eval_steps_per_second': 40.388, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0055, 'grad_norm': 0.07491851598024368, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05859686806797981, 'eval_precision': 0.722488038277512, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7259615384615384, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6312, 'eval_samples_per_second': 326.695, 'eval_steps_per_second': 41.026, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 982.3084, 'train_samples_per_second': 104.692, 'train_steps_per_second': 1.637, 'total_flos': 1.2881744067011148e+16, 'train_loss': 0.027627885119238898, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9893
  predict_f1                 =     0.7019
  predict_loss               =     0.0395
  predict_precision          =     0.6776
  predict_recall             =     0.7281
  predict_runtime            = 0:00:03.91
  predict_samples_per_second =    319.753
  predict_steps_per_second   =     40.097
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_202.json completed. F1: 0.7019027484143764
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_505.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5676.09 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5662.10 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6475.31 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6474.19 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6706.34 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6988.17 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7010.52 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7166.85 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6108.17 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6817.83 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5260.39 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7198.88 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5124.53 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1705, 'grad_norm': 0.6539888381958008, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05284443497657776, 'eval_precision': 0.5932721712538226, 'eval_recall': 0.6247987117552335, 'eval_f1': 0.6086274509803922, 'eval_accuracy': 0.985148708328448, 'eval_runtime': 4.6694, 'eval_samples_per_second': 324.022, 'eval_steps_per_second': 40.69, 'epoch': 1.0}
{'loss': 0.0462, 'grad_norm': 0.47801685333251953, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04533536359667778, 'eval_precision': 0.6724683544303798, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6783719074221867, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6281, 'eval_samples_per_second': 326.915, 'eval_steps_per_second': 41.053, 'epoch': 2.0}
{'loss': 0.0352, 'grad_norm': 0.5370784401893616, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04487469792366028, 'eval_precision': 0.6842948717948718, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6859437751004016, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6432, 'eval_samples_per_second': 325.85, 'eval_steps_per_second': 40.92, 'epoch': 3.0}
{'loss': 0.0268, 'grad_norm': 0.42546963691711426, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04493996873497963, 'eval_precision': 0.6989079563182528, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7099841521394612, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.6455, 'eval_samples_per_second': 325.691, 'eval_steps_per_second': 40.9, 'epoch': 4.0}
{'loss': 0.0211, 'grad_norm': 0.06622771918773651, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04593054950237274, 'eval_precision': 0.706436420722135, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7154213036565977, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.6641, 'eval_samples_per_second': 324.391, 'eval_steps_per_second': 40.736, 'epoch': 5.0}
{'loss': 0.0159, 'grad_norm': 0.3514152765274048, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05093725025653839, 'eval_precision': 0.7259380097879282, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7212317666126418, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.6405, 'eval_samples_per_second': 326.039, 'eval_steps_per_second': 40.943, 'epoch': 6.0}
{'loss': 0.0129, 'grad_norm': 0.2325037717819214, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05118478462100029, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7194244604316546, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6238, 'eval_samples_per_second': 327.221, 'eval_steps_per_second': 41.092, 'epoch': 7.0}
{'loss': 0.0106, 'grad_norm': 0.4752825200557709, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05317208543419838, 'eval_precision': 0.7104430379746836, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.716679968076616, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6412, 'eval_samples_per_second': 325.994, 'eval_steps_per_second': 40.938, 'epoch': 8.0}
{'loss': 0.0086, 'grad_norm': 0.23265591263771057, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05545387417078018, 'eval_precision': 0.6975683890577508, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7177482408131353, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6445, 'eval_samples_per_second': 325.759, 'eval_steps_per_second': 40.908, 'epoch': 9.0}
{'loss': 0.0072, 'grad_norm': 0.2584485113620758, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05835999548435211, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6179, 'eval_samples_per_second': 327.638, 'eval_steps_per_second': 41.144, 'epoch': 10.0}
{'loss': 0.0061, 'grad_norm': 0.21036122739315033, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05783737078309059, 'eval_precision': 0.7083993660855784, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7140575079872205, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6276, 'eval_samples_per_second': 326.951, 'eval_steps_per_second': 41.058, 'epoch': 11.0}
{'loss': 0.0058, 'grad_norm': 0.4301660656929016, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0581837072968483, 'eval_precision': 0.7048665620094191, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7138314785373608, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5896, 'eval_samples_per_second': 329.657, 'eval_steps_per_second': 41.398, 'epoch': 12.0}
{'train_runtime': 976.7126, 'train_samples_per_second': 105.292, 'train_steps_per_second': 1.646, 'train_loss': 0.03056796176813135, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0306
  train_runtime            = 0:16:16.71
  train_samples            =       8570
  train_samples_per_second =    105.292
  train_steps_per_second   =      1.646
[{'loss': 0.1705, 'grad_norm': 0.6539888381958008, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05284443497657776, 'eval_precision': 0.5932721712538226, 'eval_recall': 0.6247987117552335, 'eval_f1': 0.6086274509803922, 'eval_accuracy': 0.985148708328448, 'eval_runtime': 4.6694, 'eval_samples_per_second': 324.022, 'eval_steps_per_second': 40.69, 'epoch': 1.0, 'step': 134}, {'loss': 0.0462, 'grad_norm': 0.47801685333251953, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04533536359667778, 'eval_precision': 0.6724683544303798, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6783719074221867, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6281, 'eval_samples_per_second': 326.915, 'eval_steps_per_second': 41.053, 'epoch': 2.0, 'step': 268}, {'loss': 0.0352, 'grad_norm': 0.5370784401893616, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04487469792366028, 'eval_precision': 0.6842948717948718, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6859437751004016, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6432, 'eval_samples_per_second': 325.85, 'eval_steps_per_second': 40.92, 'epoch': 3.0, 'step': 402}, {'loss': 0.0268, 'grad_norm': 0.42546963691711426, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04493996873497963, 'eval_precision': 0.6989079563182528, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7099841521394612, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.6455, 'eval_samples_per_second': 325.691, 'eval_steps_per_second': 40.9, 'epoch': 4.0, 'step': 536}, {'loss': 0.0211, 'grad_norm': 0.06622771918773651, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04593054950237274, 'eval_precision': 0.706436420722135, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7154213036565977, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.6641, 'eval_samples_per_second': 324.391, 'eval_steps_per_second': 40.736, 'epoch': 5.0, 'step': 670}, {'loss': 0.0159, 'grad_norm': 0.3514152765274048, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05093725025653839, 'eval_precision': 0.7259380097879282, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7212317666126418, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.6405, 'eval_samples_per_second': 326.039, 'eval_steps_per_second': 40.943, 'epoch': 6.0, 'step': 804}, {'loss': 0.0129, 'grad_norm': 0.2325037717819214, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05118478462100029, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7194244604316546, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6238, 'eval_samples_per_second': 327.221, 'eval_steps_per_second': 41.092, 'epoch': 7.0, 'step': 938}, {'loss': 0.0106, 'grad_norm': 0.4752825200557709, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05317208543419838, 'eval_precision': 0.7104430379746836, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.716679968076616, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6412, 'eval_samples_per_second': 325.994, 'eval_steps_per_second': 40.938, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0086, 'grad_norm': 0.23265591263771057, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05545387417078018, 'eval_precision': 0.6975683890577508, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7177482408131353, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6445, 'eval_samples_per_second': 325.759, 'eval_steps_per_second': 40.908, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0072, 'grad_norm': 0.2584485113620758, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05835999548435211, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6179, 'eval_samples_per_second': 327.638, 'eval_steps_per_second': 41.144, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0061, 'grad_norm': 0.21036122739315033, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05783737078309059, 'eval_precision': 0.7083993660855784, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7140575079872205, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6276, 'eval_samples_per_second': 326.951, 'eval_steps_per_second': 41.058, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0058, 'grad_norm': 0.4301660656929016, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.0581837072968483, 'eval_precision': 0.7048665620094191, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7138314785373608, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5896, 'eval_samples_per_second': 329.657, 'eval_steps_per_second': 41.398, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 976.7126, 'train_samples_per_second': 105.292, 'train_steps_per_second': 1.646, 'total_flos': 1.2900609003428748e+16, 'train_loss': 0.03056796176813135, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9888
  predict_f1                 =     0.6951
  predict_loss               =     0.0405
  predict_precision          =     0.6763
  predict_recall             =     0.7149
  predict_runtime            = 0:00:03.87
  predict_samples_per_second =     323.18
  predict_steps_per_second   =     40.527
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_505.json completed. F1: 0.6950959488272921
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_303.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6493.60 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5844.06 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5770.45 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6258.25 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6501.13 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6766.58 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6804.40 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6977.66 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6249.24 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7098.86 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3609.61 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7148.23 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4046.45 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2147, 'grad_norm': 0.35815709829330444, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.06291255354881287, 'eval_precision': 0.5844961240310077, 'eval_recall': 0.607085346215781, 'eval_f1': 0.5955766192733016, 'eval_accuracy': 0.9814749677570641, 'eval_runtime': 2.3773, 'eval_samples_per_second': 636.434, 'eval_steps_per_second': 79.922, 'epoch': 1.0}
{'loss': 0.0543, 'grad_norm': 0.4378858208656311, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.05402291938662529, 'eval_precision': 0.5699248120300752, 'eval_recall': 0.6103059581320451, 'eval_f1': 0.5894245723172629, 'eval_accuracy': 0.9838199085473092, 'eval_runtime': 2.3419, 'eval_samples_per_second': 646.047, 'eval_steps_per_second': 81.129, 'epoch': 2.0}
{'loss': 0.044, 'grad_norm': 0.3239763081073761, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.05055833235383034, 'eval_precision': 0.6024279210925645, 'eval_recall': 0.6392914653784219, 'eval_f1': 0.6203125, 'eval_accuracy': 0.9849923789424317, 'eval_runtime': 2.3341, 'eval_samples_per_second': 648.226, 'eval_steps_per_second': 81.403, 'epoch': 3.0}
{'loss': 0.0372, 'grad_norm': 0.6002882122993469, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04946824908256531, 'eval_precision': 0.6584234930448223, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6719242902208202, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3283, 'eval_samples_per_second': 649.842, 'eval_steps_per_second': 81.606, 'epoch': 4.0}
{'loss': 0.0323, 'grad_norm': 0.2582583725452423, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05021032691001892, 'eval_precision': 0.6573643410852713, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.669826224328594, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.2696, 'eval_samples_per_second': 666.641, 'eval_steps_per_second': 83.716, 'epoch': 5.0}
{'loss': 0.029, 'grad_norm': 0.33212852478027344, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05199669301509857, 'eval_precision': 0.6594427244582043, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6724546172059984, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3336, 'eval_samples_per_second': 648.354, 'eval_steps_per_second': 81.419, 'epoch': 6.0}
{'loss': 0.0262, 'grad_norm': 1.1568559408187866, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05328943952918053, 'eval_precision': 0.6492307692307693, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6640440597954366, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.3439, 'eval_samples_per_second': 645.505, 'eval_steps_per_second': 81.061, 'epoch': 7.0}
{'loss': 0.024, 'grad_norm': 0.3690941631793976, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.055939994752407074, 'eval_precision': 0.6588785046728972, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.669833729216152, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 2.3284, 'eval_samples_per_second': 649.793, 'eval_steps_per_second': 81.6, 'epoch': 8.0}
{'loss': 0.021, 'grad_norm': 0.3647750914096832, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05648388713598251, 'eval_precision': 0.654434250764526, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6713725490196077, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 2.3394, 'eval_samples_per_second': 646.756, 'eval_steps_per_second': 81.219, 'epoch': 9.0}
{'loss': 0.0198, 'grad_norm': 0.6108932495117188, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.057740259915590286, 'eval_precision': 0.6687598116169545, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6772655007949127, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.4453, 'eval_samples_per_second': 618.739, 'eval_steps_per_second': 77.7, 'epoch': 10.0}
{'loss': 0.0183, 'grad_norm': 0.2686268389225006, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.058524731546640396, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.679304897314376, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2823, 'eval_samples_per_second': 662.927, 'eval_steps_per_second': 83.249, 'epoch': 11.0}
{'loss': 0.0178, 'grad_norm': 0.5277508497238159, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05885636806488037, 'eval_precision': 0.672386895475819, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6830427892234548, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.4402, 'eval_samples_per_second': 620.026, 'eval_steps_per_second': 77.862, 'epoch': 12.0}
{'train_runtime': 393.0368, 'train_samples_per_second': 261.655, 'train_steps_per_second': 4.091, 'train_loss': 0.04487405428245886, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0449
  train_runtime            = 0:06:33.03
  train_samples            =       8570
  train_samples_per_second =    261.655
  train_steps_per_second   =      4.091
[{'loss': 0.2147, 'grad_norm': 0.35815709829330444, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.06291255354881287, 'eval_precision': 0.5844961240310077, 'eval_recall': 0.607085346215781, 'eval_f1': 0.5955766192733016, 'eval_accuracy': 0.9814749677570641, 'eval_runtime': 2.3773, 'eval_samples_per_second': 636.434, 'eval_steps_per_second': 79.922, 'epoch': 1.0, 'step': 134}, {'loss': 0.0543, 'grad_norm': 0.4378858208656311, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05402291938662529, 'eval_precision': 0.5699248120300752, 'eval_recall': 0.6103059581320451, 'eval_f1': 0.5894245723172629, 'eval_accuracy': 0.9838199085473092, 'eval_runtime': 2.3419, 'eval_samples_per_second': 646.047, 'eval_steps_per_second': 81.129, 'epoch': 2.0, 'step': 268}, {'loss': 0.044, 'grad_norm': 0.3239763081073761, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.05055833235383034, 'eval_precision': 0.6024279210925645, 'eval_recall': 0.6392914653784219, 'eval_f1': 0.6203125, 'eval_accuracy': 0.9849923789424317, 'eval_runtime': 2.3341, 'eval_samples_per_second': 648.226, 'eval_steps_per_second': 81.403, 'epoch': 3.0, 'step': 402}, {'loss': 0.0372, 'grad_norm': 0.6002882122993469, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04946824908256531, 'eval_precision': 0.6584234930448223, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6719242902208202, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3283, 'eval_samples_per_second': 649.842, 'eval_steps_per_second': 81.606, 'epoch': 4.0, 'step': 536}, {'loss': 0.0323, 'grad_norm': 0.2582583725452423, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05021032691001892, 'eval_precision': 0.6573643410852713, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.669826224328594, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.2696, 'eval_samples_per_second': 666.641, 'eval_steps_per_second': 83.716, 'epoch': 5.0, 'step': 670}, {'loss': 0.029, 'grad_norm': 0.33212852478027344, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05199669301509857, 'eval_precision': 0.6594427244582043, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6724546172059984, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3336, 'eval_samples_per_second': 648.354, 'eval_steps_per_second': 81.419, 'epoch': 6.0, 'step': 804}, {'loss': 0.0262, 'grad_norm': 1.1568559408187866, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05328943952918053, 'eval_precision': 0.6492307692307693, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6640440597954366, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.3439, 'eval_samples_per_second': 645.505, 'eval_steps_per_second': 81.061, 'epoch': 7.0, 'step': 938}, {'loss': 0.024, 'grad_norm': 0.3690941631793976, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.055939994752407074, 'eval_precision': 0.6588785046728972, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.669833729216152, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 2.3284, 'eval_samples_per_second': 649.793, 'eval_steps_per_second': 81.6, 'epoch': 8.0, 'step': 1072}, {'loss': 0.021, 'grad_norm': 0.3647750914096832, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05648388713598251, 'eval_precision': 0.654434250764526, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6713725490196077, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 2.3394, 'eval_samples_per_second': 646.756, 'eval_steps_per_second': 81.219, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0198, 'grad_norm': 0.6108932495117188, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.057740259915590286, 'eval_precision': 0.6687598116169545, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6772655007949127, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.4453, 'eval_samples_per_second': 618.739, 'eval_steps_per_second': 77.7, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0183, 'grad_norm': 0.2686268389225006, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.058524731546640396, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.679304897314376, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2823, 'eval_samples_per_second': 662.927, 'eval_steps_per_second': 83.249, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0178, 'grad_norm': 0.5277508497238159, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05885636806488037, 'eval_precision': 0.672386895475819, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6830427892234548, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.4402, 'eval_samples_per_second': 620.026, 'eval_steps_per_second': 77.862, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 393.0368, 'train_samples_per_second': 261.655, 'train_steps_per_second': 4.091, 'total_flos': 4434832167386640.0, 'train_loss': 0.04487405428245886, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =      0.989
  predict_f1                 =     0.7007
  predict_loss               =     0.0446
  predict_precision          =     0.6812
  predict_recall             =     0.7215
  predict_runtime            = 0:00:01.97
  predict_samples_per_second =    633.472
  predict_steps_per_second   =     79.437
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_303.json completed. F1: 0.7007454739084131
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_505.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7132.71 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4812.91 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5477.13 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5969.19 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6360.76 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6744.95 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6855.43 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7069.59 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6104.69 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7242.73 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5055.90 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7141.15 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4344.20 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0716, 'grad_norm': 1.0814200639724731, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04332046955823898, 'eval_precision': 0.6456456456456456, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6682206682206682, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.6577, 'eval_samples_per_second': 324.837, 'eval_steps_per_second': 40.792, 'epoch': 1.0}
{'loss': 0.0336, 'grad_norm': 0.44668498635292053, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04734174907207489, 'eval_precision': 0.7219512195121951, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7184466019417475, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.64, 'eval_samples_per_second': 326.078, 'eval_steps_per_second': 40.948, 'epoch': 2.0}
{'loss': 0.0189, 'grad_norm': 0.28312933444976807, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.0557330846786499, 'eval_precision': 0.7114427860696517, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.7009803921568627, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.6202, 'eval_samples_per_second': 327.477, 'eval_steps_per_second': 41.124, 'epoch': 3.0}
{'loss': 0.0109, 'grad_norm': 0.5454559922218323, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.06126721575856209, 'eval_precision': 0.64171974522293, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6453162530024019, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 4.6041, 'eval_samples_per_second': 328.623, 'eval_steps_per_second': 41.268, 'epoch': 4.0}
{'loss': 0.0078, 'grad_norm': 0.04327752813696861, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.061135511845350266, 'eval_precision': 0.6867088607594937, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6927374301675977, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6619, 'eval_samples_per_second': 324.544, 'eval_steps_per_second': 40.756, 'epoch': 5.0}
{'loss': 0.0041, 'grad_norm': 0.026770804077386856, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07257037609815598, 'eval_precision': 0.6962843295638126, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6951612903225807, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6008, 'eval_samples_per_second': 328.853, 'eval_steps_per_second': 41.297, 'epoch': 6.0}
{'loss': 0.002, 'grad_norm': 0.018106065690517426, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07009956240653992, 'eval_precision': 0.6645569620253164, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.670391061452514, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6051, 'eval_samples_per_second': 328.545, 'eval_steps_per_second': 41.258, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.2055269032716751, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07130996137857437, 'eval_precision': 0.7035256410256411, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7052208835341367, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6173, 'eval_samples_per_second': 327.681, 'eval_steps_per_second': 41.15, 'epoch': 8.0}
{'loss': 0.0003, 'grad_norm': 0.002034837147220969, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07661143690347672, 'eval_precision': 0.7147435897435898, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7164658634538152, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.6411, 'eval_samples_per_second': 326.004, 'eval_steps_per_second': 40.939, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.0015407407190650702, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07624722272157669, 'eval_precision': 0.7067307692307693, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.708433734939759, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6055, 'eval_samples_per_second': 328.517, 'eval_steps_per_second': 41.255, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.0010805448982864618, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.0770733654499054, 'eval_precision': 0.7124600638977636, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7153167602245389, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.6146, 'eval_samples_per_second': 327.871, 'eval_steps_per_second': 41.173, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.02321653999388218, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07735021412372589, 'eval_precision': 0.7152, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7174959871589085, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.6228, 'eval_samples_per_second': 327.291, 'eval_steps_per_second': 41.101, 'epoch': 12.0}
{'train_runtime': 959.6933, 'train_samples_per_second': 107.159, 'train_steps_per_second': 3.351, 'train_loss': 0.012532225641449775, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0125
  train_runtime            = 0:15:59.69
  train_samples            =       8570
  train_samples_per_second =    107.159
  train_steps_per_second   =      3.351
[{'loss': 0.0716, 'grad_norm': 1.0814200639724731, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04332046955823898, 'eval_precision': 0.6456456456456456, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6682206682206682, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.6577, 'eval_samples_per_second': 324.837, 'eval_steps_per_second': 40.792, 'epoch': 1.0, 'step': 268}, {'loss': 0.0336, 'grad_norm': 0.44668498635292053, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04734174907207489, 'eval_precision': 0.7219512195121951, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7184466019417475, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.64, 'eval_samples_per_second': 326.078, 'eval_steps_per_second': 40.948, 'epoch': 2.0, 'step': 536}, {'loss': 0.0189, 'grad_norm': 0.28312933444976807, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.0557330846786499, 'eval_precision': 0.7114427860696517, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.7009803921568627, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.6202, 'eval_samples_per_second': 327.477, 'eval_steps_per_second': 41.124, 'epoch': 3.0, 'step': 804}, {'loss': 0.0109, 'grad_norm': 0.5454559922218323, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.06126721575856209, 'eval_precision': 0.64171974522293, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6453162530024019, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 4.6041, 'eval_samples_per_second': 328.623, 'eval_steps_per_second': 41.268, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0078, 'grad_norm': 0.04327752813696861, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.061135511845350266, 'eval_precision': 0.6867088607594937, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6927374301675977, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6619, 'eval_samples_per_second': 324.544, 'eval_steps_per_second': 40.756, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0041, 'grad_norm': 0.026770804077386856, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.07257037609815598, 'eval_precision': 0.6962843295638126, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6951612903225807, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6008, 'eval_samples_per_second': 328.853, 'eval_steps_per_second': 41.297, 'epoch': 6.0, 'step': 1608}, {'loss': 0.002, 'grad_norm': 0.018106065690517426, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07009956240653992, 'eval_precision': 0.6645569620253164, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.670391061452514, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6051, 'eval_samples_per_second': 328.545, 'eval_steps_per_second': 41.258, 'epoch': 7.0, 'step': 1876}, {'loss': 0.001, 'grad_norm': 0.2055269032716751, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07130996137857437, 'eval_precision': 0.7035256410256411, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7052208835341367, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6173, 'eval_samples_per_second': 327.681, 'eval_steps_per_second': 41.15, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0003, 'grad_norm': 0.002034837147220969, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07661143690347672, 'eval_precision': 0.7147435897435898, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7164658634538152, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.6411, 'eval_samples_per_second': 326.004, 'eval_steps_per_second': 40.939, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.0015407407190650702, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07624722272157669, 'eval_precision': 0.7067307692307693, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.708433734939759, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6055, 'eval_samples_per_second': 328.517, 'eval_steps_per_second': 41.255, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0001, 'grad_norm': 0.0010805448982864618, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.0770733654499054, 'eval_precision': 0.7124600638977636, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7153167602245389, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.6146, 'eval_samples_per_second': 327.871, 'eval_steps_per_second': 41.173, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0001, 'grad_norm': 0.02321653999388218, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.07735021412372589, 'eval_precision': 0.7152, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7174959871589085, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.6228, 'eval_samples_per_second': 327.291, 'eval_steps_per_second': 41.101, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 959.6933, 'train_samples_per_second': 107.159, 'train_steps_per_second': 3.351, 'total_flos': 1.138674864612978e+16, 'train_loss': 0.012532225641449775, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =      0.989
  predict_f1                 =     0.6794
  predict_loss               =      0.038
  predict_precision          =     0.6584
  predict_recall             =     0.7018
  predict_runtime            = 0:00:03.89
  predict_samples_per_second =    321.372
  predict_steps_per_second   =       40.3
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_505.json completed. F1: 0.6794055201698513
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_404.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6003.87 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:00, 6718.28 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5048.96 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5746.77 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6158.44 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6560.51 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6667.84 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6902.71 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6210.55 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7143.83 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4410.50 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6595.85 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3122.23 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2256, 'grad_norm': 0.5191226005554199, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.062060534954071045, 'eval_precision': 0.6059602649006622, 'eval_recall': 0.5893719806763285, 'eval_f1': 0.5975510204081632, 'eval_accuracy': 0.9823738617266581, 'eval_runtime': 2.4339, 'eval_samples_per_second': 621.632, 'eval_steps_per_second': 78.063, 'epoch': 1.0}
{'loss': 0.0545, 'grad_norm': 0.22497142851352692, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.05547347292304039, 'eval_precision': 0.55, 'eval_recall': 0.6022544283413849, 'eval_f1': 0.5749423520368947, 'eval_accuracy': 0.983702661507797, 'eval_runtime': 2.3214, 'eval_samples_per_second': 651.749, 'eval_steps_per_second': 81.845, 'epoch': 2.0}
{'loss': 0.0435, 'grad_norm': 0.42941510677337646, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.053314484655857086, 'eval_precision': 0.612540192926045, 'eval_recall': 0.6135265700483091, 'eval_f1': 0.6130329847144006, 'eval_accuracy': 0.9848751319029194, 'eval_runtime': 2.3235, 'eval_samples_per_second': 651.183, 'eval_steps_per_second': 81.774, 'epoch': 3.0}
{'loss': 0.0379, 'grad_norm': 0.3529091775417328, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.05073743686079979, 'eval_precision': 0.6467817896389325, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6550079491255961, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3365, 'eval_samples_per_second': 647.552, 'eval_steps_per_second': 81.319, 'epoch': 4.0}
{'loss': 0.0326, 'grad_norm': 0.45705944299697876, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.053008176386356354, 'eval_precision': 0.6640378548895899, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6709163346613546, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.32, 'eval_samples_per_second': 652.164, 'eval_steps_per_second': 81.898, 'epoch': 5.0}
{'loss': 0.0283, 'grad_norm': 0.37073785066604614, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05313294753432274, 'eval_precision': 0.6682539682539682, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6730615507593924, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3116, 'eval_samples_per_second': 654.53, 'eval_steps_per_second': 82.195, 'epoch': 6.0}
{'loss': 0.0256, 'grad_norm': 0.1501389592885971, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05360166355967522, 'eval_precision': 0.6781045751633987, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6731549067315491, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3233, 'eval_samples_per_second': 651.234, 'eval_steps_per_second': 81.781, 'epoch': 7.0}
{'loss': 0.0229, 'grad_norm': 0.519728422164917, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.0540393702685833, 'eval_precision': 0.6813186813186813, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6899841017488076, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.3094, 'eval_samples_per_second': 655.144, 'eval_steps_per_second': 82.272, 'epoch': 8.0}
{'loss': 0.0205, 'grad_norm': 0.7013022303581238, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.055309660732746124, 'eval_precision': 0.6849529780564263, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6942017474185862, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.3079, 'eval_samples_per_second': 655.569, 'eval_steps_per_second': 82.325, 'epoch': 9.0}
{'loss': 0.019, 'grad_norm': 0.47353196144104004, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05633120983839035, 'eval_precision': 0.6692426584234931, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6829652996845426, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3386, 'eval_samples_per_second': 646.96, 'eval_steps_per_second': 81.244, 'epoch': 10.0}
{'loss': 0.0179, 'grad_norm': 0.2116166055202484, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05780705064535141, 'eval_precision': 0.6728971962616822, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.684085510688836, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3515, 'eval_samples_per_second': 643.42, 'eval_steps_per_second': 80.8, 'epoch': 11.0}
{'loss': 0.0165, 'grad_norm': 1.0499780178070068, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05827787518501282, 'eval_precision': 0.676056338028169, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6857142857142857, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.3123, 'eval_samples_per_second': 654.329, 'eval_steps_per_second': 82.17, 'epoch': 12.0}
{'train_runtime': 391.7632, 'train_samples_per_second': 262.505, 'train_steps_per_second': 4.105, 'train_loss': 0.045398787034684744, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0454
  train_runtime            = 0:06:31.76
  train_samples            =       8570
  train_samples_per_second =    262.505
  train_steps_per_second   =      4.105
[{'loss': 0.2256, 'grad_norm': 0.5191226005554199, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.062060534954071045, 'eval_precision': 0.6059602649006622, 'eval_recall': 0.5893719806763285, 'eval_f1': 0.5975510204081632, 'eval_accuracy': 0.9823738617266581, 'eval_runtime': 2.4339, 'eval_samples_per_second': 621.632, 'eval_steps_per_second': 78.063, 'epoch': 1.0, 'step': 134}, {'loss': 0.0545, 'grad_norm': 0.22497142851352692, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05547347292304039, 'eval_precision': 0.55, 'eval_recall': 0.6022544283413849, 'eval_f1': 0.5749423520368947, 'eval_accuracy': 0.983702661507797, 'eval_runtime': 2.3214, 'eval_samples_per_second': 651.749, 'eval_steps_per_second': 81.845, 'epoch': 2.0, 'step': 268}, {'loss': 0.0435, 'grad_norm': 0.42941510677337646, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.053314484655857086, 'eval_precision': 0.612540192926045, 'eval_recall': 0.6135265700483091, 'eval_f1': 0.6130329847144006, 'eval_accuracy': 0.9848751319029194, 'eval_runtime': 2.3235, 'eval_samples_per_second': 651.183, 'eval_steps_per_second': 81.774, 'epoch': 3.0, 'step': 402}, {'loss': 0.0379, 'grad_norm': 0.3529091775417328, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05073743686079979, 'eval_precision': 0.6467817896389325, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6550079491255961, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3365, 'eval_samples_per_second': 647.552, 'eval_steps_per_second': 81.319, 'epoch': 4.0, 'step': 536}, {'loss': 0.0326, 'grad_norm': 0.45705944299697876, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.053008176386356354, 'eval_precision': 0.6640378548895899, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6709163346613546, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.32, 'eval_samples_per_second': 652.164, 'eval_steps_per_second': 81.898, 'epoch': 5.0, 'step': 670}, {'loss': 0.0283, 'grad_norm': 0.37073785066604614, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05313294753432274, 'eval_precision': 0.6682539682539682, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6730615507593924, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3116, 'eval_samples_per_second': 654.53, 'eval_steps_per_second': 82.195, 'epoch': 6.0, 'step': 804}, {'loss': 0.0256, 'grad_norm': 0.1501389592885971, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05360166355967522, 'eval_precision': 0.6781045751633987, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6731549067315491, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3233, 'eval_samples_per_second': 651.234, 'eval_steps_per_second': 81.781, 'epoch': 7.0, 'step': 938}, {'loss': 0.0229, 'grad_norm': 0.519728422164917, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.0540393702685833, 'eval_precision': 0.6813186813186813, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6899841017488076, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.3094, 'eval_samples_per_second': 655.144, 'eval_steps_per_second': 82.272, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0205, 'grad_norm': 0.7013022303581238, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.055309660732746124, 'eval_precision': 0.6849529780564263, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6942017474185862, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.3079, 'eval_samples_per_second': 655.569, 'eval_steps_per_second': 82.325, 'epoch': 9.0, 'step': 1206}, {'loss': 0.019, 'grad_norm': 0.47353196144104004, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05633120983839035, 'eval_precision': 0.6692426584234931, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6829652996845426, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3386, 'eval_samples_per_second': 646.96, 'eval_steps_per_second': 81.244, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0179, 'grad_norm': 0.2116166055202484, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05780705064535141, 'eval_precision': 0.6728971962616822, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.684085510688836, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3515, 'eval_samples_per_second': 643.42, 'eval_steps_per_second': 80.8, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0165, 'grad_norm': 1.0499780178070068, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05827787518501282, 'eval_precision': 0.676056338028169, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6857142857142857, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.3123, 'eval_samples_per_second': 654.329, 'eval_steps_per_second': 82.17, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 391.7632, 'train_samples_per_second': 262.505, 'train_steps_per_second': 4.105, 'total_flos': 4419828838366056.0, 'train_loss': 0.045398787034684744, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9887
  predict_f1                 =     0.6972
  predict_loss               =     0.0447
  predict_precision          =     0.6784
  predict_recall             =     0.7171
  predict_runtime            = 0:00:02.05
  predict_samples_per_second =    608.208
  predict_steps_per_second   =     76.269
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_08_404.json completed. F1: 0.697228144989339
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_202.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7238.05 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 6374.00 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6311.36 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5893.39 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6304.25 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6672.22 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6761.77 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7022.52 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6394.23 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7259.38 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 6462.83 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7152.43 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3803.65 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0759, 'grad_norm': 0.3180719017982483, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.0500163808465004, 'eval_precision': 0.6145833333333334, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6388244392884764, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 4.6048, 'eval_samples_per_second': 328.571, 'eval_steps_per_second': 41.261, 'epoch': 1.0}
{'loss': 0.0343, 'grad_norm': 0.6637760996818542, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04798368364572525, 'eval_precision': 0.6868852459016394, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6807473598700243, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.6085, 'eval_samples_per_second': 328.303, 'eval_steps_per_second': 41.228, 'epoch': 2.0}
{'loss': 0.0199, 'grad_norm': 0.0831204280257225, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.0537528358399868, 'eval_precision': 0.6558966074313409, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6548387096774195, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 4.6265, 'eval_samples_per_second': 327.03, 'eval_steps_per_second': 41.068, 'epoch': 3.0}
{'loss': 0.0116, 'grad_norm': 0.21185851097106934, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.06743728369474411, 'eval_precision': 0.678407350689127, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6954474097331239, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.6138, 'eval_samples_per_second': 327.931, 'eval_steps_per_second': 41.181, 'epoch': 4.0}
{'loss': 0.0073, 'grad_norm': 0.5253591537475586, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.060467615723609924, 'eval_precision': 0.6743119266055045, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.691764705882353, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.599, 'eval_samples_per_second': 328.988, 'eval_steps_per_second': 41.314, 'epoch': 5.0}
{'loss': 0.0041, 'grad_norm': 0.157546266913414, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.062234263867139816, 'eval_precision': 0.6952380952380952, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7002398081534772, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6222, 'eval_samples_per_second': 327.333, 'eval_steps_per_second': 41.106, 'epoch': 6.0}
{'loss': 0.002, 'grad_norm': 0.1020369902253151, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07024150341749191, 'eval_precision': 0.672926447574335, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6825396825396827, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5956, 'eval_samples_per_second': 329.23, 'eval_steps_per_second': 41.344, 'epoch': 7.0}
{'loss': 0.0011, 'grad_norm': 0.0030639003962278366, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07197606563568115, 'eval_precision': 0.6589506172839507, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6729708431836091, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.7305, 'eval_samples_per_second': 319.838, 'eval_steps_per_second': 40.165, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.04665389284491539, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07717996090650558, 'eval_precision': 0.6894904458598726, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6933546837469975, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.6154, 'eval_samples_per_second': 327.817, 'eval_steps_per_second': 41.167, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.031506940722465515, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07587262243032455, 'eval_precision': 0.6692913385826772, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6767515923566879, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6052, 'eval_samples_per_second': 328.545, 'eval_steps_per_second': 41.258, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.0020003621466457844, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07834816724061966, 'eval_precision': 0.6765625, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6867565424266455, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.6, 'eval_samples_per_second': 328.91, 'eval_steps_per_second': 41.304, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.007433949038386345, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07905455678701401, 'eval_precision': 0.6968253968253968, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7018385291766587, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.6184, 'eval_samples_per_second': 327.602, 'eval_steps_per_second': 41.14, 'epoch': 12.0}
{'train_runtime': 960.2838, 'train_samples_per_second': 107.093, 'train_steps_per_second': 3.349, 'train_loss': 0.013109064252644, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0131
  train_runtime            = 0:16:00.28
  train_samples            =       8570
  train_samples_per_second =    107.093
  train_steps_per_second   =      3.349
[{'loss': 0.0759, 'grad_norm': 0.3180719017982483, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.0500163808465004, 'eval_precision': 0.6145833333333334, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6388244392884764, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 4.6048, 'eval_samples_per_second': 328.571, 'eval_steps_per_second': 41.261, 'epoch': 1.0, 'step': 268}, {'loss': 0.0343, 'grad_norm': 0.6637760996818542, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04798368364572525, 'eval_precision': 0.6868852459016394, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6807473598700243, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 4.6085, 'eval_samples_per_second': 328.303, 'eval_steps_per_second': 41.228, 'epoch': 2.0, 'step': 536}, {'loss': 0.0199, 'grad_norm': 0.0831204280257225, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.0537528358399868, 'eval_precision': 0.6558966074313409, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6548387096774195, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 4.6265, 'eval_samples_per_second': 327.03, 'eval_steps_per_second': 41.068, 'epoch': 3.0, 'step': 804}, {'loss': 0.0116, 'grad_norm': 0.21185851097106934, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.06743728369474411, 'eval_precision': 0.678407350689127, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6954474097331239, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.6138, 'eval_samples_per_second': 327.931, 'eval_steps_per_second': 41.181, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0073, 'grad_norm': 0.5253591537475586, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.060467615723609924, 'eval_precision': 0.6743119266055045, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.691764705882353, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.599, 'eval_samples_per_second': 328.988, 'eval_steps_per_second': 41.314, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0041, 'grad_norm': 0.157546266913414, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.062234263867139816, 'eval_precision': 0.6952380952380952, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7002398081534772, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6222, 'eval_samples_per_second': 327.333, 'eval_steps_per_second': 41.106, 'epoch': 6.0, 'step': 1608}, {'loss': 0.002, 'grad_norm': 0.1020369902253151, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07024150341749191, 'eval_precision': 0.672926447574335, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6825396825396827, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5956, 'eval_samples_per_second': 329.23, 'eval_steps_per_second': 41.344, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0011, 'grad_norm': 0.0030639003962278366, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07197606563568115, 'eval_precision': 0.6589506172839507, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6729708431836091, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.7305, 'eval_samples_per_second': 319.838, 'eval_steps_per_second': 40.165, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0005, 'grad_norm': 0.04665389284491539, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07717996090650558, 'eval_precision': 0.6894904458598726, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6933546837469975, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.6154, 'eval_samples_per_second': 327.817, 'eval_steps_per_second': 41.167, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0003, 'grad_norm': 0.031506940722465515, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07587262243032455, 'eval_precision': 0.6692913385826772, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6767515923566879, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6052, 'eval_samples_per_second': 328.545, 'eval_steps_per_second': 41.258, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0001, 'grad_norm': 0.0020003621466457844, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.07834816724061966, 'eval_precision': 0.6765625, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6867565424266455, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.6, 'eval_samples_per_second': 328.91, 'eval_steps_per_second': 41.304, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0001, 'grad_norm': 0.007433949038386345, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.07905455678701401, 'eval_precision': 0.6968253968253968, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7018385291766587, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.6184, 'eval_samples_per_second': 327.602, 'eval_steps_per_second': 41.14, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 960.2838, 'train_samples_per_second': 107.093, 'train_steps_per_second': 3.349, 'total_flos': 1.1358393195545172e+16, 'train_loss': 0.013109064252644, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9894
  predict_f1                 =     0.6958
  predict_loss               =     0.0398
  predict_precision          =     0.7126
  predict_recall             =     0.6798
  predict_runtime            = 0:00:03.88
  predict_samples_per_second =    321.988
  predict_steps_per_second   =     40.377
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_202.json completed. F1: 0.6958473625140292
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_505.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7205.92 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4982.84 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5402.68 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5802.86 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6107.19 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6530.20 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6673.22 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6919.45 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6020.77 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6268.99 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3340.01 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6539.51 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3364.64 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0799, 'grad_norm': 1.1681153774261475, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.050225548446178436, 'eval_precision': 0.659375, 'eval_recall': 0.679549114331723, 'eval_f1': 0.669310071371927, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3588, 'eval_samples_per_second': 641.44, 'eval_steps_per_second': 80.551, 'epoch': 1.0}
{'loss': 0.0381, 'grad_norm': 0.5957604050636292, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.050895944237709045, 'eval_precision': 0.6550079491255962, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6592, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.3424, 'eval_samples_per_second': 645.906, 'eval_steps_per_second': 81.112, 'epoch': 2.0}
{'loss': 0.0243, 'grad_norm': 0.4437558352947235, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05991394445300102, 'eval_precision': 0.6704545454545454, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6677445432497979, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 2.4292, 'eval_samples_per_second': 622.846, 'eval_steps_per_second': 78.216, 'epoch': 3.0}
{'loss': 0.0162, 'grad_norm': 3.59848952293396, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.06139305606484413, 'eval_precision': 0.5427350427350427, 'eval_recall': 0.6135265700483091, 'eval_f1': 0.5759637188208616, 'eval_accuracy': 0.9832336733497479, 'eval_runtime': 2.3491, 'eval_samples_per_second': 644.082, 'eval_steps_per_second': 80.883, 'epoch': 4.0}
{'loss': 0.0096, 'grad_norm': 0.028094397857785225, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06855431199073792, 'eval_precision': 0.6055900621118012, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.616600790513834, 'eval_accuracy': 0.9848360495564153, 'eval_runtime': 2.3091, 'eval_samples_per_second': 655.234, 'eval_steps_per_second': 82.283, 'epoch': 5.0}
{'loss': 0.0067, 'grad_norm': 0.05864724516868591, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.08260535448789597, 'eval_precision': 0.6099397590361446, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6303501945525292, 'eval_accuracy': 0.9844843084378786, 'eval_runtime': 2.2977, 'eval_samples_per_second': 658.487, 'eval_steps_per_second': 82.692, 'epoch': 6.0}
{'loss': 0.0041, 'grad_norm': 0.07202805578708649, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07324456423521042, 'eval_precision': 0.6310240963855421, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6521400778210117, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 2.321, 'eval_samples_per_second': 651.879, 'eval_steps_per_second': 81.862, 'epoch': 7.0}
{'loss': 0.0019, 'grad_norm': 0.3720451593399048, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.08289451897144318, 'eval_precision': 0.6661466458658346, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.676703645007924, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3026, 'eval_samples_per_second': 657.07, 'eval_steps_per_second': 82.514, 'epoch': 8.0}
{'loss': 0.0011, 'grad_norm': 0.01685929112136364, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08636097609996796, 'eval_precision': 0.650381679389313, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6677115987460815, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.2981, 'eval_samples_per_second': 658.382, 'eval_steps_per_second': 82.678, 'epoch': 9.0}
{'loss': 0.0006, 'grad_norm': 0.008059843443334103, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08894790709018707, 'eval_precision': 0.6385542168674698, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.659922178988327, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.5626, 'eval_samples_per_second': 590.405, 'eval_steps_per_second': 74.142, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.0012512587709352374, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.09056878089904785, 'eval_precision': 0.6589506172839507, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6729708431836091, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 2.3038, 'eval_samples_per_second': 656.737, 'eval_steps_per_second': 82.472, 'epoch': 11.0}
{'loss': 0.0003, 'grad_norm': 0.05057273805141449, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.09115645289421082, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9856567788330012, 'eval_runtime': 2.312, 'eval_samples_per_second': 654.4, 'eval_steps_per_second': 82.179, 'epoch': 12.0}
{'train_runtime': 424.9356, 'train_samples_per_second': 242.013, 'train_steps_per_second': 7.568, 'train_loss': 0.015269988318395555, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0153
  train_runtime            = 0:07:04.93
  train_samples            =       8570
  train_samples_per_second =    242.013
  train_steps_per_second   =      7.568
[{'loss': 0.0799, 'grad_norm': 1.1681153774261475, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.050225548446178436, 'eval_precision': 0.659375, 'eval_recall': 0.679549114331723, 'eval_f1': 0.669310071371927, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3588, 'eval_samples_per_second': 641.44, 'eval_steps_per_second': 80.551, 'epoch': 1.0, 'step': 268}, {'loss': 0.0381, 'grad_norm': 0.5957604050636292, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.050895944237709045, 'eval_precision': 0.6550079491255962, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6592, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.3424, 'eval_samples_per_second': 645.906, 'eval_steps_per_second': 81.112, 'epoch': 2.0, 'step': 536}, {'loss': 0.0243, 'grad_norm': 0.4437558352947235, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05991394445300102, 'eval_precision': 0.6704545454545454, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6677445432497979, 'eval_accuracy': 0.9857740258725134, 'eval_runtime': 2.4292, 'eval_samples_per_second': 622.846, 'eval_steps_per_second': 78.216, 'epoch': 3.0, 'step': 804}, {'loss': 0.0162, 'grad_norm': 3.59848952293396, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.06139305606484413, 'eval_precision': 0.5427350427350427, 'eval_recall': 0.6135265700483091, 'eval_f1': 0.5759637188208616, 'eval_accuracy': 0.9832336733497479, 'eval_runtime': 2.3491, 'eval_samples_per_second': 644.082, 'eval_steps_per_second': 80.883, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0096, 'grad_norm': 0.028094397857785225, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06855431199073792, 'eval_precision': 0.6055900621118012, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.616600790513834, 'eval_accuracy': 0.9848360495564153, 'eval_runtime': 2.3091, 'eval_samples_per_second': 655.234, 'eval_steps_per_second': 82.283, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0067, 'grad_norm': 0.05864724516868591, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.08260535448789597, 'eval_precision': 0.6099397590361446, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6303501945525292, 'eval_accuracy': 0.9844843084378786, 'eval_runtime': 2.2977, 'eval_samples_per_second': 658.487, 'eval_steps_per_second': 82.692, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0041, 'grad_norm': 0.07202805578708649, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07324456423521042, 'eval_precision': 0.6310240963855421, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6521400778210117, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 2.321, 'eval_samples_per_second': 651.879, 'eval_steps_per_second': 81.862, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0019, 'grad_norm': 0.3720451593399048, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.08289451897144318, 'eval_precision': 0.6661466458658346, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.676703645007924, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3026, 'eval_samples_per_second': 657.07, 'eval_steps_per_second': 82.514, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0011, 'grad_norm': 0.01685929112136364, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.08636097609996796, 'eval_precision': 0.650381679389313, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6677115987460815, 'eval_accuracy': 0.9858912729120256, 'eval_runtime': 2.2981, 'eval_samples_per_second': 658.382, 'eval_steps_per_second': 82.678, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0006, 'grad_norm': 0.008059843443334103, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.08894790709018707, 'eval_precision': 0.6385542168674698, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.659922178988327, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.5626, 'eval_samples_per_second': 590.405, 'eval_steps_per_second': 74.142, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0004, 'grad_norm': 0.0012512587709352374, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.09056878089904785, 'eval_precision': 0.6589506172839507, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6729708431836091, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 2.3038, 'eval_samples_per_second': 656.737, 'eval_steps_per_second': 82.472, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0003, 'grad_norm': 0.05057273805141449, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.09115645289421082, 'eval_precision': 0.6512345679012346, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6650906225374311, 'eval_accuracy': 0.9856567788330012, 'eval_runtime': 2.312, 'eval_samples_per_second': 654.4, 'eval_steps_per_second': 82.179, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 424.9356, 'train_samples_per_second': 242.013, 'train_steps_per_second': 7.568, 'total_flos': 3932790236814540.0, 'train_loss': 0.015269988318395555, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9885
  predict_f1                 =      0.678
  predict_loss               =     0.0405
  predict_precision          =     0.6557
  predict_recall             =     0.7018
  predict_runtime            = 0:00:01.92
  predict_samples_per_second =    651.528
  predict_steps_per_second   =     81.701
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_505.json completed. F1: 0.6779661016949152
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_202.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6649.26 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4840.15 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5009.64 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5707.69 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6124.40 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6507.46 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6645.20 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6866.94 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5729.91 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6617.42 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3586.67 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7054.62 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4416.77 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0825, 'grad_norm': 0.23080295324325562, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05189942941069603, 'eval_precision': 0.5706134094151213, 'eval_recall': 0.644122383252818, 'eval_f1': 0.605143721633888, 'eval_accuracy': 0.9840934849728378, 'eval_runtime': 2.345, 'eval_samples_per_second': 645.209, 'eval_steps_per_second': 81.024, 'epoch': 1.0}
{'loss': 0.0386, 'grad_norm': 0.5750762224197388, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.048306189477443695, 'eval_precision': 0.6341463414634146, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.6310679611650485, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3355, 'eval_samples_per_second': 647.816, 'eval_steps_per_second': 81.352, 'epoch': 2.0}
{'loss': 0.0248, 'grad_norm': 0.17641641199588776, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05483134463429451, 'eval_precision': 0.625544267053701, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6580152671755726, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3039, 'eval_samples_per_second': 656.722, 'eval_steps_per_second': 82.47, 'epoch': 3.0}
{'loss': 0.016, 'grad_norm': 0.17835263907909393, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.06120697781443596, 'eval_precision': 0.6880877742946708, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6973788721207307, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3152, 'eval_samples_per_second': 653.509, 'eval_steps_per_second': 82.067, 'epoch': 4.0}
{'loss': 0.0101, 'grad_norm': 0.43603140115737915, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06422507017850876, 'eval_precision': 0.6527131782945736, 'eval_recall': 0.677938808373591, 'eval_f1': 0.665086887835703, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3246, 'eval_samples_per_second': 650.855, 'eval_steps_per_second': 81.733, 'epoch': 5.0}
{'loss': 0.0067, 'grad_norm': 0.07747312635183334, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06884200125932693, 'eval_precision': 0.6688206785137318, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6677419354838708, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3143, 'eval_samples_per_second': 653.771, 'eval_steps_per_second': 82.099, 'epoch': 6.0}
{'loss': 0.0035, 'grad_norm': 0.4456705152988434, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07555010914802551, 'eval_precision': 0.6367713004484304, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6604651162790698, 'eval_accuracy': 0.985148708328448, 'eval_runtime': 2.3583, 'eval_samples_per_second': 641.567, 'eval_steps_per_second': 80.567, 'epoch': 7.0}
{'loss': 0.0021, 'grad_norm': 0.335475891828537, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07504349946975708, 'eval_precision': 0.6542635658914728, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6666666666666666, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3313, 'eval_samples_per_second': 648.997, 'eval_steps_per_second': 81.5, 'epoch': 8.0}
{'loss': 0.0012, 'grad_norm': 0.00339013384655118, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07980199158191681, 'eval_precision': 0.6471471471471472, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6697746697746697, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.317, 'eval_samples_per_second': 653.012, 'eval_steps_per_second': 82.004, 'epoch': 9.0}
{'loss': 0.0007, 'grad_norm': 0.08276207745075226, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08135926723480225, 'eval_precision': 0.665144596651446, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6838810641627544, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3102, 'eval_samples_per_second': 654.926, 'eval_steps_per_second': 82.245, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.004831980913877487, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08185772597789764, 'eval_precision': 0.6574074074074074, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6713947990543736, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.321, 'eval_samples_per_second': 651.881, 'eval_steps_per_second': 81.862, 'epoch': 11.0}
{'loss': 0.0003, 'grad_norm': 0.0978895053267479, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08223740756511688, 'eval_precision': 0.6671875, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6772402854877081, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.3264, 'eval_samples_per_second': 650.368, 'eval_steps_per_second': 81.672, 'epoch': 12.0}
{'train_runtime': 395.1187, 'train_samples_per_second': 260.276, 'train_steps_per_second': 8.139, 'train_loss': 0.015572199262029941, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0156
  train_runtime            = 0:06:35.11
  train_samples            =       8570
  train_samples_per_second =    260.276
  train_steps_per_second   =      8.139
[{'loss': 0.0825, 'grad_norm': 0.23080295324325562, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05189942941069603, 'eval_precision': 0.5706134094151213, 'eval_recall': 0.644122383252818, 'eval_f1': 0.605143721633888, 'eval_accuracy': 0.9840934849728378, 'eval_runtime': 2.345, 'eval_samples_per_second': 645.209, 'eval_steps_per_second': 81.024, 'epoch': 1.0, 'step': 268}, {'loss': 0.0386, 'grad_norm': 0.5750762224197388, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.048306189477443695, 'eval_precision': 0.6341463414634146, 'eval_recall': 0.6280193236714976, 'eval_f1': 0.6310679611650485, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3355, 'eval_samples_per_second': 647.816, 'eval_steps_per_second': 81.352, 'epoch': 2.0, 'step': 536}, {'loss': 0.0248, 'grad_norm': 0.17641641199588776, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05483134463429451, 'eval_precision': 0.625544267053701, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6580152671755726, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3039, 'eval_samples_per_second': 656.722, 'eval_steps_per_second': 82.47, 'epoch': 3.0, 'step': 804}, {'loss': 0.016, 'grad_norm': 0.17835263907909393, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.06120697781443596, 'eval_precision': 0.6880877742946708, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6973788721207307, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3152, 'eval_samples_per_second': 653.509, 'eval_steps_per_second': 82.067, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0101, 'grad_norm': 0.43603140115737915, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06422507017850876, 'eval_precision': 0.6527131782945736, 'eval_recall': 0.677938808373591, 'eval_f1': 0.665086887835703, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3246, 'eval_samples_per_second': 650.855, 'eval_steps_per_second': 81.733, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0067, 'grad_norm': 0.07747312635183334, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06884200125932693, 'eval_precision': 0.6688206785137318, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6677419354838708, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3143, 'eval_samples_per_second': 653.771, 'eval_steps_per_second': 82.099, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0035, 'grad_norm': 0.4456705152988434, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07555010914802551, 'eval_precision': 0.6367713004484304, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6604651162790698, 'eval_accuracy': 0.985148708328448, 'eval_runtime': 2.3583, 'eval_samples_per_second': 641.567, 'eval_steps_per_second': 80.567, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0021, 'grad_norm': 0.335475891828537, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07504349946975708, 'eval_precision': 0.6542635658914728, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6666666666666666, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3313, 'eval_samples_per_second': 648.997, 'eval_steps_per_second': 81.5, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0012, 'grad_norm': 0.00339013384655118, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07980199158191681, 'eval_precision': 0.6471471471471472, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6697746697746697, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.317, 'eval_samples_per_second': 653.012, 'eval_steps_per_second': 82.004, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0007, 'grad_norm': 0.08276207745075226, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.08135926723480225, 'eval_precision': 0.665144596651446, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6838810641627544, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3102, 'eval_samples_per_second': 654.926, 'eval_steps_per_second': 82.245, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0004, 'grad_norm': 0.004831980913877487, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08185772597789764, 'eval_precision': 0.6574074074074074, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6713947990543736, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.321, 'eval_samples_per_second': 651.881, 'eval_steps_per_second': 81.862, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0003, 'grad_norm': 0.0978895053267479, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08223740756511688, 'eval_precision': 0.6671875, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6772402854877081, 'eval_accuracy': 0.986555672802595, 'eval_runtime': 2.3264, 'eval_samples_per_second': 650.368, 'eval_steps_per_second': 81.672, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 395.1187, 'train_samples_per_second': 260.276, 'train_steps_per_second': 8.139, 'total_flos': 3923015309701932.0, 'train_loss': 0.015572199262029941, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9883
  predict_f1                 =     0.6565
  predict_loss               =     0.0413
  predict_precision          =     0.6529
  predict_recall             =     0.6601
  predict_runtime            = 0:00:01.95
  predict_samples_per_second =    639.138
  predict_steps_per_second   =     80.148
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_202.json completed. F1: 0.6564885496183205
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_505.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_505.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7141.96 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5107.40 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5654.69 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6227.56 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6539.03 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6846.96 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6928.39 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7120.62 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6253.02 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7188.48 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3611.08 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7145.91 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3656.08 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1402, 'grad_norm': 0.8770455121994019, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05599434673786163, 'eval_precision': 0.5852895148669797, 'eval_recall': 0.6022544283413849, 'eval_f1': 0.5936507936507937, 'eval_accuracy': 0.9826474381521867, 'eval_runtime': 2.3442, 'eval_samples_per_second': 645.429, 'eval_steps_per_second': 81.052, 'epoch': 1.0}
{'loss': 0.047, 'grad_norm': 0.5055254697799683, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04920119419693947, 'eval_precision': 0.6314984709480123, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.647843137254902, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.3096, 'eval_samples_per_second': 655.086, 'eval_steps_per_second': 82.265, 'epoch': 2.0}
{'loss': 0.0371, 'grad_norm': 0.7374404072761536, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04774100333452225, 'eval_precision': 0.7029702970297029, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6943765281173594, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.322, 'eval_samples_per_second': 651.586, 'eval_steps_per_second': 81.825, 'epoch': 3.0}
{'loss': 0.0307, 'grad_norm': 0.592485249042511, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04830145835876465, 'eval_precision': 0.6856702619414484, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7007874015748031, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.3222, 'eval_samples_per_second': 651.536, 'eval_steps_per_second': 81.819, 'epoch': 4.0}
{'loss': 0.0248, 'grad_norm': 0.1891346573829651, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.048876095563173294, 'eval_precision': 0.6965408805031447, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7048528241845664, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 2.3079, 'eval_samples_per_second': 655.561, 'eval_steps_per_second': 82.324, 'epoch': 5.0}
{'loss': 0.0196, 'grad_norm': 0.3560402989387512, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.0509379617869854, 'eval_precision': 0.6758832565284179, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6918238993710693, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.3122, 'eval_samples_per_second': 654.353, 'eval_steps_per_second': 82.173, 'epoch': 6.0}
{'loss': 0.0157, 'grad_norm': 0.24176037311553955, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05393388494849205, 'eval_precision': 0.6838006230529595, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6951702296120348, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 2.3202, 'eval_samples_per_second': 652.085, 'eval_steps_per_second': 81.888, 'epoch': 7.0}
{'loss': 0.0129, 'grad_norm': 0.5165274143218994, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05562533438205719, 'eval_precision': 0.6818873668188736, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7010954616588418, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 2.3033, 'eval_samples_per_second': 656.896, 'eval_steps_per_second': 82.492, 'epoch': 8.0}
{'loss': 0.0111, 'grad_norm': 0.4729663133621216, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.0591154620051384, 'eval_precision': 0.6825153374233128, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6991358994501177, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.3237, 'eval_samples_per_second': 651.121, 'eval_steps_per_second': 81.767, 'epoch': 9.0}
{'loss': 0.01, 'grad_norm': 0.21159785985946655, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.058742474764585495, 'eval_precision': 0.6943164362519201, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7106918238993709, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 2.3357, 'eval_samples_per_second': 647.771, 'eval_steps_per_second': 81.346, 'epoch': 10.0}
{'loss': 0.0088, 'grad_norm': 0.03379540145397186, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06006248667836189, 'eval_precision': 0.691717791411043, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7085624509033779, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 2.3227, 'eval_samples_per_second': 651.396, 'eval_steps_per_second': 81.801, 'epoch': 11.0}
{'loss': 0.0081, 'grad_norm': 1.1897482872009277, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06095212697982788, 'eval_precision': 0.6793313069908815, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6989835809225957, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 2.325, 'eval_samples_per_second': 650.754, 'eval_steps_per_second': 81.721, 'epoch': 12.0}
{'train_runtime': 394.1021, 'train_samples_per_second': 260.948, 'train_steps_per_second': 8.16, 'train_loss': 0.030498224852690055, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0305
  train_runtime            = 0:06:34.10
  train_samples            =       8570
  train_samples_per_second =    260.948
  train_steps_per_second   =       8.16
[{'loss': 0.1402, 'grad_norm': 0.8770455121994019, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05599434673786163, 'eval_precision': 0.5852895148669797, 'eval_recall': 0.6022544283413849, 'eval_f1': 0.5936507936507937, 'eval_accuracy': 0.9826474381521867, 'eval_runtime': 2.3442, 'eval_samples_per_second': 645.429, 'eval_steps_per_second': 81.052, 'epoch': 1.0, 'step': 268}, {'loss': 0.047, 'grad_norm': 0.5055254697799683, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04920119419693947, 'eval_precision': 0.6314984709480123, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.647843137254902, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.3096, 'eval_samples_per_second': 655.086, 'eval_steps_per_second': 82.265, 'epoch': 2.0, 'step': 536}, {'loss': 0.0371, 'grad_norm': 0.7374404072761536, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04774100333452225, 'eval_precision': 0.7029702970297029, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6943765281173594, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.322, 'eval_samples_per_second': 651.586, 'eval_steps_per_second': 81.825, 'epoch': 3.0, 'step': 804}, {'loss': 0.0307, 'grad_norm': 0.592485249042511, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04830145835876465, 'eval_precision': 0.6856702619414484, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7007874015748031, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.3222, 'eval_samples_per_second': 651.536, 'eval_steps_per_second': 81.819, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0248, 'grad_norm': 0.1891346573829651, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.048876095563173294, 'eval_precision': 0.6965408805031447, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7048528241845664, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 2.3079, 'eval_samples_per_second': 655.561, 'eval_steps_per_second': 82.324, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0196, 'grad_norm': 0.3560402989387512, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.0509379617869854, 'eval_precision': 0.6758832565284179, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6918238993710693, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.3122, 'eval_samples_per_second': 654.353, 'eval_steps_per_second': 82.173, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0157, 'grad_norm': 0.24176037311553955, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05393388494849205, 'eval_precision': 0.6838006230529595, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6951702296120348, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 2.3202, 'eval_samples_per_second': 652.085, 'eval_steps_per_second': 81.888, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0129, 'grad_norm': 0.5165274143218994, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05562533438205719, 'eval_precision': 0.6818873668188736, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7010954616588418, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 2.3033, 'eval_samples_per_second': 656.896, 'eval_steps_per_second': 82.492, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0111, 'grad_norm': 0.4729663133621216, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.0591154620051384, 'eval_precision': 0.6825153374233128, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6991358994501177, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.3237, 'eval_samples_per_second': 651.121, 'eval_steps_per_second': 81.767, 'epoch': 9.0, 'step': 2412}, {'loss': 0.01, 'grad_norm': 0.21159785985946655, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.058742474764585495, 'eval_precision': 0.6943164362519201, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7106918238993709, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 2.3357, 'eval_samples_per_second': 647.771, 'eval_steps_per_second': 81.346, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0088, 'grad_norm': 0.03379540145397186, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06006248667836189, 'eval_precision': 0.691717791411043, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7085624509033779, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 2.3227, 'eval_samples_per_second': 651.396, 'eval_steps_per_second': 81.801, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0081, 'grad_norm': 1.1897482872009277, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06095212697982788, 'eval_precision': 0.6793313069908815, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6989835809225957, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 2.325, 'eval_samples_per_second': 650.754, 'eval_steps_per_second': 81.721, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 394.1021, 'train_samples_per_second': 260.948, 'train_steps_per_second': 8.16, 'total_flos': 3932790236814540.0, 'train_loss': 0.030498224852690055, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9888
  predict_f1                 =     0.6968
  predict_loss               =     0.0428
  predict_precision          =     0.7007
  predict_recall             =      0.693
  predict_runtime            = 0:00:01.96
  predict_samples_per_second =    637.495
  predict_steps_per_second   =     79.941
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_505.json completed. F1: 0.6968026460859977
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_303.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7186.89 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5009.40 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5723.97 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6085.44 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6414.12 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6740.61 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6815.11 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7038.32 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6192.67 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7216.18 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4044.91 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6774.60 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3272.19 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1153, 'grad_norm': 0.377887487411499, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04657124727964401, 'eval_precision': 0.6108663729809104, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6390168970814132, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 4.6228, 'eval_samples_per_second': 327.292, 'eval_steps_per_second': 41.101, 'epoch': 1.0}
{'loss': 0.0403, 'grad_norm': 0.7993024587631226, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04133043810725212, 'eval_precision': 0.6961832061068702, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7147335423197492, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6134, 'eval_samples_per_second': 327.956, 'eval_steps_per_second': 41.184, 'epoch': 2.0}
{'loss': 0.0276, 'grad_norm': 0.5644283294677734, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03972649574279785, 'eval_precision': 0.7292993630573248, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.733386709367494, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6024, 'eval_samples_per_second': 328.744, 'eval_steps_per_second': 41.283, 'epoch': 3.0}
{'loss': 0.019, 'grad_norm': 0.5526136755943298, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04503040015697479, 'eval_precision': 0.7131782945736435, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7266982622432859, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6549, 'eval_samples_per_second': 325.037, 'eval_steps_per_second': 40.818, 'epoch': 4.0}
{'loss': 0.0137, 'grad_norm': 0.23817434906959534, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.047259919345378876, 'eval_precision': 0.6904400606980273, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7109375000000001, 'eval_accuracy': 0.988822448899832, 'eval_runtime': 4.5856, 'eval_samples_per_second': 329.948, 'eval_steps_per_second': 41.434, 'epoch': 5.0}
{'loss': 0.0103, 'grad_norm': 0.2068185657262802, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05387520417571068, 'eval_precision': 0.6921850079744817, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6955128205128205, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.6019, 'eval_samples_per_second': 328.778, 'eval_steps_per_second': 41.287, 'epoch': 6.0}
{'loss': 0.0076, 'grad_norm': 0.5117769241333008, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.058331266045570374, 'eval_precision': 0.6692546583850931, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6814229249011857, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6063, 'eval_samples_per_second': 328.461, 'eval_steps_per_second': 41.248, 'epoch': 7.0}
{'loss': 0.0058, 'grad_norm': 1.0785013437271118, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05859198421239853, 'eval_precision': 0.6671779141104295, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6834249803613511, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6009, 'eval_samples_per_second': 328.847, 'eval_steps_per_second': 41.296, 'epoch': 8.0}
{'loss': 0.0043, 'grad_norm': 0.7335448861122131, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06015542149543762, 'eval_precision': 0.7049689440993789, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7177865612648222, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.5965, 'eval_samples_per_second': 329.164, 'eval_steps_per_second': 41.336, 'epoch': 9.0}
{'loss': 0.0034, 'grad_norm': 0.3029117286205292, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06181688606739044, 'eval_precision': 0.6847826086956522, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6972332015810276, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.6163, 'eval_samples_per_second': 327.749, 'eval_steps_per_second': 41.158, 'epoch': 10.0}
{'loss': 0.0031, 'grad_norm': 0.4737187325954437, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06369613856077194, 'eval_precision': 0.6754250386398764, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6892744479495267, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 4.5984, 'eval_samples_per_second': 329.027, 'eval_steps_per_second': 41.319, 'epoch': 11.0}
{'loss': 0.0024, 'grad_norm': 0.35402363538742065, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06433029472827911, 'eval_precision': 0.6702453987730062, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6865671641791046, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.6173, 'eval_samples_per_second': 327.681, 'eval_steps_per_second': 41.15, 'epoch': 12.0}
{'train_runtime': 981.9754, 'train_samples_per_second': 104.728, 'train_steps_per_second': 3.275, 'train_loss': 0.02106812129269785, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0211
  train_runtime            = 0:16:21.97
  train_samples            =       8570
  train_samples_per_second =    104.728
  train_steps_per_second   =      3.275
[{'loss': 0.1153, 'grad_norm': 0.377887487411499, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04657124727964401, 'eval_precision': 0.6108663729809104, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6390168970814132, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 4.6228, 'eval_samples_per_second': 327.292, 'eval_steps_per_second': 41.101, 'epoch': 1.0, 'step': 268}, {'loss': 0.0403, 'grad_norm': 0.7993024587631226, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04133043810725212, 'eval_precision': 0.6961832061068702, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7147335423197492, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6134, 'eval_samples_per_second': 327.956, 'eval_steps_per_second': 41.184, 'epoch': 2.0, 'step': 536}, {'loss': 0.0276, 'grad_norm': 0.5644283294677734, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.03972649574279785, 'eval_precision': 0.7292993630573248, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.733386709367494, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6024, 'eval_samples_per_second': 328.744, 'eval_steps_per_second': 41.283, 'epoch': 3.0, 'step': 804}, {'loss': 0.019, 'grad_norm': 0.5526136755943298, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04503040015697479, 'eval_precision': 0.7131782945736435, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7266982622432859, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6549, 'eval_samples_per_second': 325.037, 'eval_steps_per_second': 40.818, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0137, 'grad_norm': 0.23817434906959534, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.047259919345378876, 'eval_precision': 0.6904400606980273, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7109375000000001, 'eval_accuracy': 0.988822448899832, 'eval_runtime': 4.5856, 'eval_samples_per_second': 329.948, 'eval_steps_per_second': 41.434, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0103, 'grad_norm': 0.2068185657262802, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05387520417571068, 'eval_precision': 0.6921850079744817, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6955128205128205, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.6019, 'eval_samples_per_second': 328.778, 'eval_steps_per_second': 41.287, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0076, 'grad_norm': 0.5117769241333008, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.058331266045570374, 'eval_precision': 0.6692546583850931, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6814229249011857, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6063, 'eval_samples_per_second': 328.461, 'eval_steps_per_second': 41.248, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0058, 'grad_norm': 1.0785013437271118, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05859198421239853, 'eval_precision': 0.6671779141104295, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6834249803613511, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6009, 'eval_samples_per_second': 328.847, 'eval_steps_per_second': 41.296, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0043, 'grad_norm': 0.7335448861122131, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06015542149543762, 'eval_precision': 0.7049689440993789, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7177865612648222, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.5965, 'eval_samples_per_second': 329.164, 'eval_steps_per_second': 41.336, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0034, 'grad_norm': 0.3029117286205292, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06181688606739044, 'eval_precision': 0.6847826086956522, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6972332015810276, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.6163, 'eval_samples_per_second': 327.749, 'eval_steps_per_second': 41.158, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0031, 'grad_norm': 0.4737187325954437, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06369613856077194, 'eval_precision': 0.6754250386398764, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6892744479495267, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 4.5984, 'eval_samples_per_second': 329.027, 'eval_steps_per_second': 41.319, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0024, 'grad_norm': 0.35402363538742065, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06433029472827911, 'eval_precision': 0.6702453987730062, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6865671641791046, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.6173, 'eval_samples_per_second': 327.681, 'eval_steps_per_second': 41.15, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 981.9754, 'train_samples_per_second': 104.728, 'train_steps_per_second': 3.275, 'total_flos': 1.1349918485646804e+16, 'train_loss': 0.02106812129269785, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9906
  predict_f1                 =     0.7545
  predict_loss               =     0.0372
  predict_precision          =      0.732
  predict_recall             =     0.7785
  predict_runtime            = 0:00:03.88
  predict_samples_per_second =    321.862
  predict_steps_per_second   =     40.361
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_303.json completed. F1: 0.7545164718384697
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_202.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6715.17 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4655.51 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4788.00 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5543.19 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5997.07 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6432.44 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6581.70 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6852.10 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5663.62 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7150.10 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4917.79 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7115.37 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3112.71 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1471, 'grad_norm': 0.41735607385635376, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05758153274655342, 'eval_precision': 0.5842349304482226, 'eval_recall': 0.6086956521739131, 'eval_f1': 0.5962145110410094, 'eval_accuracy': 0.9826083558056826, 'eval_runtime': 2.3486, 'eval_samples_per_second': 644.215, 'eval_steps_per_second': 80.9, 'epoch': 1.0}
{'loss': 0.0472, 'grad_norm': 0.6578511595726013, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04994509369134903, 'eval_precision': 0.6265432098765432, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6398739164696612, 'eval_accuracy': 0.9856958611795053, 'eval_runtime': 2.3356, 'eval_samples_per_second': 647.801, 'eval_steps_per_second': 81.35, 'epoch': 2.0}
{'loss': 0.0368, 'grad_norm': 0.7930163145065308, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.05207914859056473, 'eval_precision': 0.6296900489396411, 'eval_recall': 0.6215780998389694, 'eval_f1': 0.6256077795786061, 'eval_accuracy': 0.9850705436354399, 'eval_runtime': 2.3289, 'eval_samples_per_second': 649.652, 'eval_steps_per_second': 81.582, 'epoch': 3.0}
{'loss': 0.03, 'grad_norm': 0.29782599210739136, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.051187288016080856, 'eval_precision': 0.6919354838709677, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6913779210314263, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.315, 'eval_samples_per_second': 653.555, 'eval_steps_per_second': 82.072, 'epoch': 4.0}
{'loss': 0.0255, 'grad_norm': 0.7153788805007935, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05285672843456268, 'eval_precision': 0.6416791604197901, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6645962732919254, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.3062, 'eval_samples_per_second': 656.067, 'eval_steps_per_second': 82.388, 'epoch': 5.0}
{'loss': 0.0211, 'grad_norm': 0.6860106587409973, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05454431474208832, 'eval_precision': 0.6834645669291338, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6910828025477707, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.349, 'eval_samples_per_second': 644.105, 'eval_steps_per_second': 80.886, 'epoch': 6.0}
{'loss': 0.017, 'grad_norm': 0.541212797164917, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.057177335023880005, 'eval_precision': 0.6807511737089202, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6904761904761905, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.2998, 'eval_samples_per_second': 657.894, 'eval_steps_per_second': 82.617, 'epoch': 7.0}
{'loss': 0.0142, 'grad_norm': 0.3086452782154083, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.06195781007409096, 'eval_precision': 0.660436137071651, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6714172604908947, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3071, 'eval_samples_per_second': 655.793, 'eval_steps_per_second': 82.353, 'epoch': 8.0}
{'loss': 0.0123, 'grad_norm': 0.13412033021450043, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06368652731180191, 'eval_precision': 0.6656346749226006, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6787687450670875, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3192, 'eval_samples_per_second': 652.376, 'eval_steps_per_second': 81.924, 'epoch': 9.0}
{'loss': 0.0107, 'grad_norm': 0.3923902213573456, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06430720537900925, 'eval_precision': 0.6895475819032761, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7004754358161649, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.3084, 'eval_samples_per_second': 655.436, 'eval_steps_per_second': 82.309, 'epoch': 10.0}
{'loss': 0.0097, 'grad_norm': 0.1981162577867508, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06748360395431519, 'eval_precision': 0.6739130434782609, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6861660079051384, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3313, 'eval_samples_per_second': 649.007, 'eval_steps_per_second': 81.501, 'epoch': 11.0}
{'loss': 0.0089, 'grad_norm': 0.4473342001438141, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06615851074457169, 'eval_precision': 0.6791862284820032, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6888888888888889, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.3103, 'eval_samples_per_second': 654.887, 'eval_steps_per_second': 82.24, 'epoch': 12.0}
{'train_runtime': 394.3048, 'train_samples_per_second': 260.813, 'train_steps_per_second': 8.156, 'train_loss': 0.031709265797885494, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0317
  train_runtime            = 0:06:34.30
  train_samples            =       8570
  train_samples_per_second =    260.813
  train_steps_per_second   =      8.156
[{'loss': 0.1471, 'grad_norm': 0.41735607385635376, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05758153274655342, 'eval_precision': 0.5842349304482226, 'eval_recall': 0.6086956521739131, 'eval_f1': 0.5962145110410094, 'eval_accuracy': 0.9826083558056826, 'eval_runtime': 2.3486, 'eval_samples_per_second': 644.215, 'eval_steps_per_second': 80.9, 'epoch': 1.0, 'step': 268}, {'loss': 0.0472, 'grad_norm': 0.6578511595726013, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04994509369134903, 'eval_precision': 0.6265432098765432, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6398739164696612, 'eval_accuracy': 0.9856958611795053, 'eval_runtime': 2.3356, 'eval_samples_per_second': 647.801, 'eval_steps_per_second': 81.35, 'epoch': 2.0, 'step': 536}, {'loss': 0.0368, 'grad_norm': 0.7930163145065308, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05207914859056473, 'eval_precision': 0.6296900489396411, 'eval_recall': 0.6215780998389694, 'eval_f1': 0.6256077795786061, 'eval_accuracy': 0.9850705436354399, 'eval_runtime': 2.3289, 'eval_samples_per_second': 649.652, 'eval_steps_per_second': 81.582, 'epoch': 3.0, 'step': 804}, {'loss': 0.03, 'grad_norm': 0.29782599210739136, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.051187288016080856, 'eval_precision': 0.6919354838709677, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6913779210314263, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.315, 'eval_samples_per_second': 653.555, 'eval_steps_per_second': 82.072, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0255, 'grad_norm': 0.7153788805007935, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05285672843456268, 'eval_precision': 0.6416791604197901, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6645962732919254, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.3062, 'eval_samples_per_second': 656.067, 'eval_steps_per_second': 82.388, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0211, 'grad_norm': 0.6860106587409973, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05454431474208832, 'eval_precision': 0.6834645669291338, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6910828025477707, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.349, 'eval_samples_per_second': 644.105, 'eval_steps_per_second': 80.886, 'epoch': 6.0, 'step': 1608}, {'loss': 0.017, 'grad_norm': 0.541212797164917, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.057177335023880005, 'eval_precision': 0.6807511737089202, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6904761904761905, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.2998, 'eval_samples_per_second': 657.894, 'eval_steps_per_second': 82.617, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0142, 'grad_norm': 0.3086452782154083, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06195781007409096, 'eval_precision': 0.660436137071651, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6714172604908947, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3071, 'eval_samples_per_second': 655.793, 'eval_steps_per_second': 82.353, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0123, 'grad_norm': 0.13412033021450043, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06368652731180191, 'eval_precision': 0.6656346749226006, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6787687450670875, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3192, 'eval_samples_per_second': 652.376, 'eval_steps_per_second': 81.924, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0107, 'grad_norm': 0.3923902213573456, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06430720537900925, 'eval_precision': 0.6895475819032761, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7004754358161649, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.3084, 'eval_samples_per_second': 655.436, 'eval_steps_per_second': 82.309, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0097, 'grad_norm': 0.1981162577867508, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06748360395431519, 'eval_precision': 0.6739130434782609, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6861660079051384, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3313, 'eval_samples_per_second': 649.007, 'eval_steps_per_second': 81.501, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0089, 'grad_norm': 0.4473342001438141, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06615851074457169, 'eval_precision': 0.6791862284820032, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6888888888888889, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.3103, 'eval_samples_per_second': 654.887, 'eval_steps_per_second': 82.24, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 394.3048, 'train_samples_per_second': 260.813, 'train_steps_per_second': 8.156, 'total_flos': 3923015309701932.0, 'train_loss': 0.031709265797885494, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =      0.987
  predict_f1                 =     0.6416
  predict_loss               =      0.044
  predict_precision          =     0.6282
  predict_recall             =     0.6557
  predict_runtime            = 0:00:01.94
  predict_samples_per_second =    643.604
  predict_steps_per_second   =     80.707
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_202.json completed. F1: 0.6416309012875536
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_404.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7202.12 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5256.91 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5872.31 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6111.28 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6471.11 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6811.16 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6893.23 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7099.44 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5773.42 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6239.97 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4395.30 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7251.86 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3361.29 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.111, 'grad_norm': 0.6452871561050415, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04838665947318077, 'eval_precision': 0.6056338028169014, 'eval_recall': 0.6231884057971014, 'eval_f1': 0.6142857142857142, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 4.6019, 'eval_samples_per_second': 328.778, 'eval_steps_per_second': 41.287, 'epoch': 1.0}
{'loss': 0.041, 'grad_norm': 0.2660863995552063, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.044352296739816666, 'eval_precision': 0.6470588235294118, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6511999999999999, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 4.6565, 'eval_samples_per_second': 324.925, 'eval_steps_per_second': 40.804, 'epoch': 2.0}
{'loss': 0.0295, 'grad_norm': 0.7579064965248108, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04293646663427353, 'eval_precision': 0.6862123613312203, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6916932907348243, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6003, 'eval_samples_per_second': 328.892, 'eval_steps_per_second': 41.302, 'epoch': 3.0}
{'loss': 0.0219, 'grad_norm': 0.5030702352523804, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04375565052032471, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7027027027027027, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6103, 'eval_samples_per_second': 328.177, 'eval_steps_per_second': 41.212, 'epoch': 4.0}
{'loss': 0.0151, 'grad_norm': 0.8173990249633789, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04930116608738899, 'eval_precision': 0.7105263157894737, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7245461720599842, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6233, 'eval_samples_per_second': 327.255, 'eval_steps_per_second': 41.096, 'epoch': 5.0}
{'loss': 0.0106, 'grad_norm': 0.7637006640434265, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.0511724129319191, 'eval_precision': 0.7073552425665102, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7174603174603175, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6035, 'eval_samples_per_second': 328.663, 'eval_steps_per_second': 41.273, 'epoch': 6.0}
{'loss': 0.0079, 'grad_norm': 0.23087282478809357, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05639743059873581, 'eval_precision': 0.699685534591195, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7080350039777247, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6209, 'eval_samples_per_second': 327.426, 'eval_steps_per_second': 41.118, 'epoch': 7.0}
{'loss': 0.0062, 'grad_norm': 0.7000580430030823, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.056132540106773376, 'eval_precision': 0.7026194144838213, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7181102362204724, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6281, 'eval_samples_per_second': 326.918, 'eval_steps_per_second': 41.054, 'epoch': 8.0}
{'loss': 0.0043, 'grad_norm': 0.8906387686729431, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06072336807847023, 'eval_precision': 0.6681887366818874, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6870109546165885, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.5961, 'eval_samples_per_second': 329.189, 'eval_steps_per_second': 41.339, 'epoch': 9.0}
{'loss': 0.0035, 'grad_norm': 0.5100522637367249, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06185143068432808, 'eval_precision': 0.6779141104294478, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6944226237234878, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.626, 'eval_samples_per_second': 327.066, 'eval_steps_per_second': 41.072, 'epoch': 10.0}
{'loss': 0.0028, 'grad_norm': 0.1101706475019455, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06239797919988632, 'eval_precision': 0.705607476635514, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7173396674584324, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.6307, 'eval_samples_per_second': 326.729, 'eval_steps_per_second': 41.03, 'epoch': 11.0}
{'loss': 0.0025, 'grad_norm': 0.0166617538779974, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06218472495675087, 'eval_precision': 0.7111801242236024, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7241106719367588, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.5779, 'eval_samples_per_second': 330.498, 'eval_steps_per_second': 41.503, 'epoch': 12.0}
{'train_runtime': 959.863, 'train_samples_per_second': 107.14, 'train_steps_per_second': 3.35, 'train_loss': 0.021367238992037466, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0214
  train_runtime            = 0:15:59.86
  train_samples            =       8570
  train_samples_per_second =     107.14
  train_steps_per_second   =       3.35
[{'loss': 0.111, 'grad_norm': 0.6452871561050415, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04838665947318077, 'eval_precision': 0.6056338028169014, 'eval_recall': 0.6231884057971014, 'eval_f1': 0.6142857142857142, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 4.6019, 'eval_samples_per_second': 328.778, 'eval_steps_per_second': 41.287, 'epoch': 1.0, 'step': 268}, {'loss': 0.041, 'grad_norm': 0.2660863995552063, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.044352296739816666, 'eval_precision': 0.6470588235294118, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6511999999999999, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 4.6565, 'eval_samples_per_second': 324.925, 'eval_steps_per_second': 40.804, 'epoch': 2.0, 'step': 536}, {'loss': 0.0295, 'grad_norm': 0.7579064965248108, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04293646663427353, 'eval_precision': 0.6862123613312203, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6916932907348243, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6003, 'eval_samples_per_second': 328.892, 'eval_steps_per_second': 41.302, 'epoch': 3.0, 'step': 804}, {'loss': 0.0219, 'grad_norm': 0.5030702352523804, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04375565052032471, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7027027027027027, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6103, 'eval_samples_per_second': 328.177, 'eval_steps_per_second': 41.212, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0151, 'grad_norm': 0.8173990249633789, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.04930116608738899, 'eval_precision': 0.7105263157894737, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7245461720599842, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6233, 'eval_samples_per_second': 327.255, 'eval_steps_per_second': 41.096, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0106, 'grad_norm': 0.7637006640434265, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.0511724129319191, 'eval_precision': 0.7073552425665102, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7174603174603175, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6035, 'eval_samples_per_second': 328.663, 'eval_steps_per_second': 41.273, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0079, 'grad_norm': 0.23087282478809357, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05639743059873581, 'eval_precision': 0.699685534591195, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7080350039777247, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6209, 'eval_samples_per_second': 327.426, 'eval_steps_per_second': 41.118, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0062, 'grad_norm': 0.7000580430030823, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.056132540106773376, 'eval_precision': 0.7026194144838213, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7181102362204724, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6281, 'eval_samples_per_second': 326.918, 'eval_steps_per_second': 41.054, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0043, 'grad_norm': 0.8906387686729431, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06072336807847023, 'eval_precision': 0.6681887366818874, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6870109546165885, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.5961, 'eval_samples_per_second': 329.189, 'eval_steps_per_second': 41.339, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0035, 'grad_norm': 0.5100522637367249, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06185143068432808, 'eval_precision': 0.6779141104294478, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6944226237234878, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.626, 'eval_samples_per_second': 327.066, 'eval_steps_per_second': 41.072, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0028, 'grad_norm': 0.1101706475019455, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06239797919988632, 'eval_precision': 0.705607476635514, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7173396674584324, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.6307, 'eval_samples_per_second': 326.729, 'eval_steps_per_second': 41.03, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0025, 'grad_norm': 0.0166617538779974, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06218472495675087, 'eval_precision': 0.7111801242236024, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7241106719367588, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.5779, 'eval_samples_per_second': 330.498, 'eval_steps_per_second': 41.503, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 959.863, 'train_samples_per_second': 107.14, 'train_steps_per_second': 3.35, 'total_flos': 1.1351569167583344e+16, 'train_loss': 0.021367238992037466, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9895
  predict_f1                 =     0.6866
  predict_loss               =     0.0417
  predict_precision          =      0.668
  predict_recall             =     0.7061
  predict_runtime            = 0:00:03.87
  predict_samples_per_second =    323.476
  predict_steps_per_second   =     40.564
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_01_404.json completed. F1: 0.6865671641791045
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_101.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4921.04 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4350.21 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4809.40 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5492.27 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5923.73 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6309.98 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6464.41 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6701.07 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5629.26 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 5894.82 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3356.25 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6955.66 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3422.25 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0821, 'grad_norm': 0.625558078289032, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04673231393098831, 'eval_precision': 0.674565560821485, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6810207336523125, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.4197, 'eval_samples_per_second': 625.282, 'eval_steps_per_second': 78.522, 'epoch': 1.0}
{'loss': 0.0361, 'grad_norm': 0.3223745822906494, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04685540124773979, 'eval_precision': 0.6222547584187409, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.651840490797546, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3536, 'eval_samples_per_second': 642.858, 'eval_steps_per_second': 80.729, 'epoch': 2.0}
{'loss': 0.0242, 'grad_norm': 0.11975512653589249, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05366058275103569, 'eval_precision': 0.6646341463414634, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6828504306969461, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.344, 'eval_samples_per_second': 645.468, 'eval_steps_per_second': 81.057, 'epoch': 3.0}
{'loss': 0.0153, 'grad_norm': 0.34604766964912415, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05681779980659485, 'eval_precision': 0.6513353115727003, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.677992277992278, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.3267, 'eval_samples_per_second': 650.288, 'eval_steps_per_second': 81.662, 'epoch': 4.0}
{'loss': 0.01, 'grad_norm': 0.4783734977245331, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.0665380209684372, 'eval_precision': 0.6642228739002932, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.6953184957789716, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3508, 'eval_samples_per_second': 643.602, 'eval_steps_per_second': 80.822, 'epoch': 5.0}
{'loss': 0.006, 'grad_norm': 0.8217271566390991, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07371070981025696, 'eval_precision': 0.6763285024154589, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6763285024154589, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3337, 'eval_samples_per_second': 648.336, 'eval_steps_per_second': 81.417, 'epoch': 6.0}
{'loss': 0.004, 'grad_norm': 0.20928384363651276, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07795441150665283, 'eval_precision': 0.6459909228441755, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6661466458658345, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3421, 'eval_samples_per_second': 646.003, 'eval_steps_per_second': 81.124, 'epoch': 7.0}
{'loss': 0.0025, 'grad_norm': 0.05365155637264252, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.08119795471429825, 'eval_precision': 0.6479514415781487, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6671874999999999, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3493, 'eval_samples_per_second': 644.011, 'eval_steps_per_second': 80.874, 'epoch': 8.0}
{'loss': 0.0013, 'grad_norm': 0.06003516912460327, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08660543709993362, 'eval_precision': 0.6267496111975117, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6376582278481012, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.3532, 'eval_samples_per_second': 642.957, 'eval_steps_per_second': 80.741, 'epoch': 9.0}
{'loss': 0.0007, 'grad_norm': 0.007670963648706675, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.09083263576030731, 'eval_precision': 0.6240601503759399, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6454121306376361, 'eval_accuracy': 0.9851877906749521, 'eval_runtime': 2.3626, 'eval_samples_per_second': 640.384, 'eval_steps_per_second': 80.418, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.002518847119063139, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.09135551750659943, 'eval_precision': 0.644916540212443, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6640625, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.3469, 'eval_samples_per_second': 644.685, 'eval_steps_per_second': 80.958, 'epoch': 11.0}
{'loss': 0.0004, 'grad_norm': 0.01018079835921526, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.09069336205720901, 'eval_precision': 0.6457055214723927, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6614296936370778, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3573, 'eval_samples_per_second': 641.833, 'eval_steps_per_second': 80.6, 'epoch': 12.0}
{'train_runtime': 398.1385, 'train_samples_per_second': 258.302, 'train_steps_per_second': 8.078, 'train_loss': 0.015239235365865243, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0152
  train_runtime            = 0:06:38.13
  train_samples            =       8570
  train_samples_per_second =    258.302
  train_steps_per_second   =      8.078
[{'loss': 0.0821, 'grad_norm': 0.625558078289032, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04673231393098831, 'eval_precision': 0.674565560821485, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6810207336523125, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.4197, 'eval_samples_per_second': 625.282, 'eval_steps_per_second': 78.522, 'epoch': 1.0, 'step': 268}, {'loss': 0.0361, 'grad_norm': 0.3223745822906494, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04685540124773979, 'eval_precision': 0.6222547584187409, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.651840490797546, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3536, 'eval_samples_per_second': 642.858, 'eval_steps_per_second': 80.729, 'epoch': 2.0, 'step': 536}, {'loss': 0.0242, 'grad_norm': 0.11975512653589249, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05366058275103569, 'eval_precision': 0.6646341463414634, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6828504306969461, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.344, 'eval_samples_per_second': 645.468, 'eval_steps_per_second': 81.057, 'epoch': 3.0, 'step': 804}, {'loss': 0.0153, 'grad_norm': 0.34604766964912415, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05681779980659485, 'eval_precision': 0.6513353115727003, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.677992277992278, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.3267, 'eval_samples_per_second': 650.288, 'eval_steps_per_second': 81.662, 'epoch': 4.0, 'step': 1072}, {'loss': 0.01, 'grad_norm': 0.4783734977245331, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.0665380209684372, 'eval_precision': 0.6642228739002932, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.6953184957789716, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.3508, 'eval_samples_per_second': 643.602, 'eval_steps_per_second': 80.822, 'epoch': 5.0, 'step': 1340}, {'loss': 0.006, 'grad_norm': 0.8217271566390991, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.07371070981025696, 'eval_precision': 0.6763285024154589, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6763285024154589, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3337, 'eval_samples_per_second': 648.336, 'eval_steps_per_second': 81.417, 'epoch': 6.0, 'step': 1608}, {'loss': 0.004, 'grad_norm': 0.20928384363651276, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07795441150665283, 'eval_precision': 0.6459909228441755, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6661466458658345, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3421, 'eval_samples_per_second': 646.003, 'eval_steps_per_second': 81.124, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0025, 'grad_norm': 0.05365155637264252, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.08119795471429825, 'eval_precision': 0.6479514415781487, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6671874999999999, 'eval_accuracy': 0.9862039316840583, 'eval_runtime': 2.3493, 'eval_samples_per_second': 644.011, 'eval_steps_per_second': 80.874, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0013, 'grad_norm': 0.06003516912460327, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.08660543709993362, 'eval_precision': 0.6267496111975117, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6376582278481012, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.3532, 'eval_samples_per_second': 642.957, 'eval_steps_per_second': 80.741, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0007, 'grad_norm': 0.007670963648706675, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.09083263576030731, 'eval_precision': 0.6240601503759399, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6454121306376361, 'eval_accuracy': 0.9851877906749521, 'eval_runtime': 2.3626, 'eval_samples_per_second': 640.384, 'eval_steps_per_second': 80.418, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0004, 'grad_norm': 0.002518847119063139, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.09135551750659943, 'eval_precision': 0.644916540212443, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6640625, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.3469, 'eval_samples_per_second': 644.685, 'eval_steps_per_second': 80.958, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0004, 'grad_norm': 0.01018079835921526, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.09069336205720901, 'eval_precision': 0.6457055214723927, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6614296936370778, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3573, 'eval_samples_per_second': 641.833, 'eval_steps_per_second': 80.6, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 398.1385, 'train_samples_per_second': 258.302, 'train_steps_per_second': 8.078, 'total_flos': 3923517530267868.0, 'train_loss': 0.015239235365865243, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9884
  predict_f1                 =      0.694
  predict_loss               =     0.0417
  predict_precision          =     0.6822
  predict_recall             =     0.7061
  predict_runtime            = 0:00:02.01
  predict_samples_per_second =     621.89
  predict_steps_per_second   =     77.985
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_05_101.json completed. F1: 0.6939655172413793
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_303.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6658.44 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5288.85 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5756.54 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5946.75 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6344.82 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6726.02 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6842.38 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7075.12 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6152.92 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7256.56 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3532.48 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7220.91 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2844.43 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1612, 'grad_norm': 0.634760320186615, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05708969384431839, 'eval_precision': 0.5311526479750779, 'eval_recall': 0.5491143317230274, 'eval_f1': 0.5399841646872525, 'eval_accuracy': 0.9831164263102357, 'eval_runtime': 4.6433, 'eval_samples_per_second': 325.845, 'eval_steps_per_second': 40.919, 'epoch': 1.0}
{'loss': 0.0476, 'grad_norm': 0.5518796443939209, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04658982902765274, 'eval_precision': 0.6491499227202473, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.662460567823344, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6138, 'eval_samples_per_second': 327.932, 'eval_steps_per_second': 41.181, 'epoch': 2.0}
{'loss': 0.0373, 'grad_norm': 0.44569095969200134, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04447637498378754, 'eval_precision': 0.6861198738170347, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6932270916334662, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6054, 'eval_samples_per_second': 328.529, 'eval_steps_per_second': 41.256, 'epoch': 3.0}
{'loss': 0.0299, 'grad_norm': 0.40263989567756653, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04561324417591095, 'eval_precision': 0.6878980891719745, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6917534027221778, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6103, 'eval_samples_per_second': 328.177, 'eval_steps_per_second': 41.212, 'epoch': 4.0}
{'loss': 0.024, 'grad_norm': 0.2995549142360687, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04857625812292099, 'eval_precision': 0.6777777777777778, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6826538768984812, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6215, 'eval_samples_per_second': 327.381, 'eval_steps_per_second': 41.112, 'epoch': 5.0}
{'loss': 0.0183, 'grad_norm': 0.2305360734462738, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04985329881310463, 'eval_precision': 0.6931637519872814, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6976, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6285, 'eval_samples_per_second': 326.885, 'eval_steps_per_second': 41.05, 'epoch': 6.0}
{'loss': 0.0149, 'grad_norm': 0.6527522802352905, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05337146669626236, 'eval_precision': 0.694136291600634, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6996805111821087, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.5917, 'eval_samples_per_second': 329.507, 'eval_steps_per_second': 41.379, 'epoch': 7.0}
{'loss': 0.0125, 'grad_norm': 0.6099770069122314, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05613493546843529, 'eval_precision': 0.6919431279620853, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6985645933014355, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.6281, 'eval_samples_per_second': 326.919, 'eval_steps_per_second': 41.054, 'epoch': 8.0}
{'loss': 0.0103, 'grad_norm': 0.15032760798931122, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05755027383565903, 'eval_precision': 0.7018927444794952, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7091633466135459, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6404, 'eval_samples_per_second': 326.049, 'eval_steps_per_second': 40.945, 'epoch': 9.0}
{'loss': 0.0091, 'grad_norm': 0.19388584792613983, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05905016139149666, 'eval_precision': 0.6976377952755906, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7054140127388534, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.7005, 'eval_samples_per_second': 321.883, 'eval_steps_per_second': 40.421, 'epoch': 10.0}
{'loss': 0.0079, 'grad_norm': 0.34146812558174133, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05926194041967392, 'eval_precision': 0.6868217054263566, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6998420221169036, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.8723, 'eval_samples_per_second': 310.532, 'eval_steps_per_second': 38.996, 'epoch': 11.0}
{'loss': 0.0076, 'grad_norm': 0.34959056973457336, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0600101463496685, 'eval_precision': 0.69375, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7042030134813639, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.643, 'eval_samples_per_second': 325.87, 'eval_steps_per_second': 40.922, 'epoch': 12.0}
{'train_runtime': 981.6674, 'train_samples_per_second': 104.761, 'train_steps_per_second': 1.638, 'train_loss': 0.0317184263052632, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0317
  train_runtime            = 0:16:21.66
  train_samples            =       8570
  train_samples_per_second =    104.761
  train_steps_per_second   =      1.638
[{'loss': 0.1612, 'grad_norm': 0.634760320186615, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05708969384431839, 'eval_precision': 0.5311526479750779, 'eval_recall': 0.5491143317230274, 'eval_f1': 0.5399841646872525, 'eval_accuracy': 0.9831164263102357, 'eval_runtime': 4.6433, 'eval_samples_per_second': 325.845, 'eval_steps_per_second': 40.919, 'epoch': 1.0, 'step': 134}, {'loss': 0.0476, 'grad_norm': 0.5518796443939209, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04658982902765274, 'eval_precision': 0.6491499227202473, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.662460567823344, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6138, 'eval_samples_per_second': 327.932, 'eval_steps_per_second': 41.181, 'epoch': 2.0, 'step': 268}, {'loss': 0.0373, 'grad_norm': 0.44569095969200134, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04447637498378754, 'eval_precision': 0.6861198738170347, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6932270916334662, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.6054, 'eval_samples_per_second': 328.529, 'eval_steps_per_second': 41.256, 'epoch': 3.0, 'step': 402}, {'loss': 0.0299, 'grad_norm': 0.40263989567756653, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04561324417591095, 'eval_precision': 0.6878980891719745, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6917534027221778, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6103, 'eval_samples_per_second': 328.177, 'eval_steps_per_second': 41.212, 'epoch': 4.0, 'step': 536}, {'loss': 0.024, 'grad_norm': 0.2995549142360687, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04857625812292099, 'eval_precision': 0.6777777777777778, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6826538768984812, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6215, 'eval_samples_per_second': 327.381, 'eval_steps_per_second': 41.112, 'epoch': 5.0, 'step': 670}, {'loss': 0.0183, 'grad_norm': 0.2305360734462738, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04985329881310463, 'eval_precision': 0.6931637519872814, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6976, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6285, 'eval_samples_per_second': 326.885, 'eval_steps_per_second': 41.05, 'epoch': 6.0, 'step': 804}, {'loss': 0.0149, 'grad_norm': 0.6527522802352905, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05337146669626236, 'eval_precision': 0.694136291600634, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6996805111821087, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.5917, 'eval_samples_per_second': 329.507, 'eval_steps_per_second': 41.379, 'epoch': 7.0, 'step': 938}, {'loss': 0.0125, 'grad_norm': 0.6099770069122314, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05613493546843529, 'eval_precision': 0.6919431279620853, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6985645933014355, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.6281, 'eval_samples_per_second': 326.919, 'eval_steps_per_second': 41.054, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0103, 'grad_norm': 0.15032760798931122, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05755027383565903, 'eval_precision': 0.7018927444794952, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7091633466135459, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6404, 'eval_samples_per_second': 326.049, 'eval_steps_per_second': 40.945, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0091, 'grad_norm': 0.19388584792613983, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05905016139149666, 'eval_precision': 0.6976377952755906, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7054140127388534, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.7005, 'eval_samples_per_second': 321.883, 'eval_steps_per_second': 40.421, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0079, 'grad_norm': 0.34146812558174133, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05926194041967392, 'eval_precision': 0.6868217054263566, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6998420221169036, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.8723, 'eval_samples_per_second': 310.532, 'eval_steps_per_second': 38.996, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0076, 'grad_norm': 0.34959056973457336, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.0600101463496685, 'eval_precision': 0.69375, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7042030134813639, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.643, 'eval_samples_per_second': 325.87, 'eval_steps_per_second': 40.922, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 981.6674, 'train_samples_per_second': 104.761, 'train_steps_per_second': 1.638, 'total_flos': 1.290639183201522e+16, 'train_loss': 0.0317184263052632, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9885
  predict_f1                 =      0.685
  predict_loss               =      0.043
  predict_precision          =     0.6612
  predict_recall             =     0.7105
  predict_runtime            = 0:00:03.91
  predict_samples_per_second =    319.855
  predict_steps_per_second   =      40.11
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_303.json completed. F1: 0.6849894291754757
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_404.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5674.39 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5014.44 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5693.76 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5182.45 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5800.62 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6352.53 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6617.69 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6929.78 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5865.00 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7385.56 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4229.99 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7410.34 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4289.59 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1561, 'grad_norm': 0.8460744023323059, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.05186738073825836, 'eval_precision': 0.6047244094488189, 'eval_recall': 0.6183574879227053, 'eval_f1': 0.6114649681528661, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 4.6471, 'eval_samples_per_second': 325.582, 'eval_steps_per_second': 40.886, 'epoch': 1.0}
{'loss': 0.0459, 'grad_norm': 0.3815837800502777, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04487570375204086, 'eval_precision': 0.6722306525037937, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6921875, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.5838, 'eval_samples_per_second': 330.072, 'eval_steps_per_second': 41.45, 'epoch': 2.0}
{'loss': 0.034, 'grad_norm': 0.5559785962104797, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.045474033802747726, 'eval_precision': 0.658267716535433, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6656050955414012, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5799, 'eval_samples_per_second': 330.353, 'eval_steps_per_second': 41.485, 'epoch': 3.0}
{'loss': 0.0265, 'grad_norm': 0.4875628352165222, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04307582974433899, 'eval_precision': 0.6806853582554517, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6920031670625495, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.616, 'eval_samples_per_second': 327.771, 'eval_steps_per_second': 41.161, 'epoch': 4.0}
{'loss': 0.0203, 'grad_norm': 0.5442995429039001, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04869725927710533, 'eval_precision': 0.6786833855799373, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6878474980142971, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.6177, 'eval_samples_per_second': 327.652, 'eval_steps_per_second': 41.146, 'epoch': 5.0}
{'loss': 0.016, 'grad_norm': 0.482305109500885, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04872232675552368, 'eval_precision': 0.7001545595054096, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7145110410094638, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6122, 'eval_samples_per_second': 328.041, 'eval_steps_per_second': 41.195, 'epoch': 6.0}
{'loss': 0.0126, 'grad_norm': 0.1400502771139145, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05152606591582298, 'eval_precision': 0.694488188976378, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7022292993630574, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6056, 'eval_samples_per_second': 328.515, 'eval_steps_per_second': 41.254, 'epoch': 7.0}
{'loss': 0.0106, 'grad_norm': 0.30480456352233887, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05348148196935654, 'eval_precision': 0.7028301886792453, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.711217183770883, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.5935, 'eval_samples_per_second': 329.375, 'eval_steps_per_second': 41.362, 'epoch': 8.0}
{'loss': 0.0085, 'grad_norm': 0.1937626600265503, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05639683082699776, 'eval_precision': 0.7063492063492064, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7114308553157475, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5971, 'eval_samples_per_second': 329.117, 'eval_steps_per_second': 41.33, 'epoch': 9.0}
{'loss': 0.0071, 'grad_norm': 0.4648561179637909, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.057915691286325455, 'eval_precision': 0.7075471698113207, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7159904534606206, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6064, 'eval_samples_per_second': 328.454, 'eval_steps_per_second': 41.247, 'epoch': 10.0}
{'loss': 0.0066, 'grad_norm': 0.5174996256828308, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06024874001741409, 'eval_precision': 0.7113237639553429, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7147435897435898, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.599, 'eval_samples_per_second': 328.981, 'eval_steps_per_second': 41.313, 'epoch': 11.0}
{'loss': 0.006, 'grad_norm': 0.12830069661140442, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06026988849043846, 'eval_precision': 0.7099841521394612, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7156549520766774, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.6038, 'eval_samples_per_second': 328.64, 'eval_steps_per_second': 41.27, 'epoch': 12.0}
{'train_runtime': 975.2436, 'train_samples_per_second': 105.451, 'train_steps_per_second': 1.649, 'train_loss': 0.0291942516295471, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0292
  train_runtime            = 0:16:15.24
  train_samples            =       8570
  train_samples_per_second =    105.451
  train_steps_per_second   =      1.649
[{'loss': 0.1561, 'grad_norm': 0.8460744023323059, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05186738073825836, 'eval_precision': 0.6047244094488189, 'eval_recall': 0.6183574879227053, 'eval_f1': 0.6114649681528661, 'eval_accuracy': 0.9853441200609685, 'eval_runtime': 4.6471, 'eval_samples_per_second': 325.582, 'eval_steps_per_second': 40.886, 'epoch': 1.0, 'step': 134}, {'loss': 0.0459, 'grad_norm': 0.3815837800502777, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04487570375204086, 'eval_precision': 0.6722306525037937, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6921875, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.5838, 'eval_samples_per_second': 330.072, 'eval_steps_per_second': 41.45, 'epoch': 2.0, 'step': 268}, {'loss': 0.034, 'grad_norm': 0.5559785962104797, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.045474033802747726, 'eval_precision': 0.658267716535433, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6656050955414012, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.5799, 'eval_samples_per_second': 330.353, 'eval_steps_per_second': 41.485, 'epoch': 3.0, 'step': 402}, {'loss': 0.0265, 'grad_norm': 0.4875628352165222, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04307582974433899, 'eval_precision': 0.6806853582554517, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6920031670625495, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.616, 'eval_samples_per_second': 327.771, 'eval_steps_per_second': 41.161, 'epoch': 4.0, 'step': 536}, {'loss': 0.0203, 'grad_norm': 0.5442995429039001, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04869725927710533, 'eval_precision': 0.6786833855799373, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6878474980142971, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.6177, 'eval_samples_per_second': 327.652, 'eval_steps_per_second': 41.146, 'epoch': 5.0, 'step': 670}, {'loss': 0.016, 'grad_norm': 0.482305109500885, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04872232675552368, 'eval_precision': 0.7001545595054096, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7145110410094638, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6122, 'eval_samples_per_second': 328.041, 'eval_steps_per_second': 41.195, 'epoch': 6.0, 'step': 804}, {'loss': 0.0126, 'grad_norm': 0.1400502771139145, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05152606591582298, 'eval_precision': 0.694488188976378, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7022292993630574, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.6056, 'eval_samples_per_second': 328.515, 'eval_steps_per_second': 41.254, 'epoch': 7.0, 'step': 938}, {'loss': 0.0106, 'grad_norm': 0.30480456352233887, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05348148196935654, 'eval_precision': 0.7028301886792453, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.711217183770883, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 4.5935, 'eval_samples_per_second': 329.375, 'eval_steps_per_second': 41.362, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0085, 'grad_norm': 0.1937626600265503, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05639683082699776, 'eval_precision': 0.7063492063492064, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7114308553157475, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5971, 'eval_samples_per_second': 329.117, 'eval_steps_per_second': 41.33, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0071, 'grad_norm': 0.4648561179637909, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.057915691286325455, 'eval_precision': 0.7075471698113207, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7159904534606206, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6064, 'eval_samples_per_second': 328.454, 'eval_steps_per_second': 41.247, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0066, 'grad_norm': 0.5174996256828308, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06024874001741409, 'eval_precision': 0.7113237639553429, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7147435897435898, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.599, 'eval_samples_per_second': 328.981, 'eval_steps_per_second': 41.313, 'epoch': 11.0, 'step': 1474}, {'loss': 0.006, 'grad_norm': 0.12830069661140442, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06026988849043846, 'eval_precision': 0.7099841521394612, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7156549520766774, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 4.6038, 'eval_samples_per_second': 328.64, 'eval_steps_per_second': 41.27, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 975.2436, 'train_samples_per_second': 105.451, 'train_steps_per_second': 1.649, 'total_flos': 1.2875344500118716e+16, 'train_loss': 0.0291942516295471, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9899
  predict_f1                 =     0.7269
  predict_loss               =     0.0395
  predict_precision          =     0.6976
  predict_recall             =     0.7588
  predict_runtime            = 0:00:03.89
  predict_samples_per_second =    321.502
  predict_steps_per_second   =     40.316
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_07_404.json completed. F1: 0.7268907563025211
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_101.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7294.56 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 3962.54 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5009.00 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 4942.29 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5595.10 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6193.91 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6492.49 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6862.09 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5561.70 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7450.58 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4348.89 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7363.47 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4196.94 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0756, 'grad_norm': 0.6265527009963989, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05016938969492912, 'eval_precision': 0.642433234421365, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6687258687258688, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 4.7007, 'eval_samples_per_second': 321.866, 'eval_steps_per_second': 40.419, 'epoch': 1.0}
{'loss': 0.0332, 'grad_norm': 0.7210885882377625, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04792077839374542, 'eval_precision': 0.6691616766467066, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6935608999224204, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 4.6437, 'eval_samples_per_second': 325.815, 'eval_steps_per_second': 40.915, 'epoch': 2.0}
{'loss': 0.0197, 'grad_norm': 0.20525968074798584, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.048177771270275116, 'eval_precision': 0.7193798449612403, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7330173775671407, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6143, 'eval_samples_per_second': 327.894, 'eval_steps_per_second': 41.176, 'epoch': 3.0}
{'loss': 0.0112, 'grad_norm': 0.4238174259662628, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05690498277544975, 'eval_precision': 0.6539050535987749, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6703296703296703, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 4.63, 'eval_samples_per_second': 326.78, 'eval_steps_per_second': 41.036, 'epoch': 4.0}
{'loss': 0.0071, 'grad_norm': 0.3668273091316223, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05919355899095535, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6777513855898654, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 4.6379, 'eval_samples_per_second': 326.225, 'eval_steps_per_second': 40.967, 'epoch': 5.0}
{'loss': 0.0052, 'grad_norm': 1.0193333625793457, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06542591005563736, 'eval_precision': 0.6808846761453397, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6874003189792663, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.6343, 'eval_samples_per_second': 326.477, 'eval_steps_per_second': 40.998, 'epoch': 6.0}
{'loss': 0.0019, 'grad_norm': 0.09687819331884384, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06936971098184586, 'eval_precision': 0.6896551724137931, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.698967434471803, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.6237, 'eval_samples_per_second': 327.228, 'eval_steps_per_second': 41.093, 'epoch': 7.0}
{'loss': 0.0007, 'grad_norm': 0.004804011434316635, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07610303163528442, 'eval_precision': 0.6772655007949125, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6816000000000001, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6312, 'eval_samples_per_second': 326.697, 'eval_steps_per_second': 41.026, 'epoch': 8.0}
{'loss': 0.0002, 'grad_norm': 0.0038964743725955486, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07628018409013748, 'eval_precision': 0.7, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7050359712230215, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6256, 'eval_samples_per_second': 327.094, 'eval_steps_per_second': 41.076, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.017812278121709824, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07817796617746353, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6265, 'eval_samples_per_second': 327.03, 'eval_steps_per_second': 41.068, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.0033089127391576767, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08296234160661697, 'eval_precision': 0.6934189406099518, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6945337620578778, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.6167, 'eval_samples_per_second': 327.722, 'eval_steps_per_second': 41.155, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.010982204228639603, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08177190274000168, 'eval_precision': 0.6875, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6978588421887392, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.631, 'eval_samples_per_second': 326.709, 'eval_steps_per_second': 41.028, 'epoch': 12.0}
{'train_runtime': 960.63, 'train_samples_per_second': 107.055, 'train_steps_per_second': 3.348, 'train_loss': 0.012931605955048357, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0129
  train_runtime            = 0:16:00.62
  train_samples            =       8570
  train_samples_per_second =    107.055
  train_steps_per_second   =      3.348
[{'loss': 0.0756, 'grad_norm': 0.6265527009963989, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.05016938969492912, 'eval_precision': 0.642433234421365, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6687258687258688, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 4.7007, 'eval_samples_per_second': 321.866, 'eval_steps_per_second': 40.419, 'epoch': 1.0, 'step': 268}, {'loss': 0.0332, 'grad_norm': 0.7210885882377625, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04792077839374542, 'eval_precision': 0.6691616766467066, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6935608999224204, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 4.6437, 'eval_samples_per_second': 325.815, 'eval_steps_per_second': 40.915, 'epoch': 2.0, 'step': 536}, {'loss': 0.0197, 'grad_norm': 0.20525968074798584, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.048177771270275116, 'eval_precision': 0.7193798449612403, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7330173775671407, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6143, 'eval_samples_per_second': 327.894, 'eval_steps_per_second': 41.176, 'epoch': 3.0, 'step': 804}, {'loss': 0.0112, 'grad_norm': 0.4238174259662628, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05690498277544975, 'eval_precision': 0.6539050535987749, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6703296703296703, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 4.63, 'eval_samples_per_second': 326.78, 'eval_steps_per_second': 41.036, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0071, 'grad_norm': 0.3668273091316223, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05919355899095535, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6777513855898654, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 4.6379, 'eval_samples_per_second': 326.225, 'eval_steps_per_second': 40.967, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0052, 'grad_norm': 1.0193333625793457, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06542591005563736, 'eval_precision': 0.6808846761453397, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6874003189792663, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.6343, 'eval_samples_per_second': 326.477, 'eval_steps_per_second': 40.998, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0019, 'grad_norm': 0.09687819331884384, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.06936971098184586, 'eval_precision': 0.6896551724137931, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.698967434471803, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.6237, 'eval_samples_per_second': 327.228, 'eval_steps_per_second': 41.093, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0007, 'grad_norm': 0.004804011434316635, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07610303163528442, 'eval_precision': 0.6772655007949125, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6816000000000001, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 4.6312, 'eval_samples_per_second': 326.697, 'eval_steps_per_second': 41.026, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0002, 'grad_norm': 0.0038964743725955486, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07628018409013748, 'eval_precision': 0.7, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7050359712230215, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6256, 'eval_samples_per_second': 327.094, 'eval_steps_per_second': 41.076, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.017812278121709824, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07817796617746353, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6265, 'eval_samples_per_second': 327.03, 'eval_steps_per_second': 41.068, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0001, 'grad_norm': 0.0033089127391576767, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08296234160661697, 'eval_precision': 0.6934189406099518, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6945337620578778, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 4.6167, 'eval_samples_per_second': 327.722, 'eval_steps_per_second': 41.155, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0001, 'grad_norm': 0.010982204228639603, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08177190274000168, 'eval_precision': 0.6875, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6978588421887392, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.631, 'eval_samples_per_second': 326.709, 'eval_steps_per_second': 41.028, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 960.63, 'train_samples_per_second': 107.055, 'train_steps_per_second': 3.348, 'total_flos': 1.1352534181638552e+16, 'train_loss': 0.012931605955048357, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9889
  predict_f1                 =      0.713
  predict_loss               =     0.0435
  predict_precision          =     0.6758
  predict_recall             =     0.7544
  predict_runtime            = 0:00:03.92
  predict_samples_per_second =    319.017
  predict_steps_per_second   =     40.005
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_04_101.json completed. F1: 0.7129533678756477
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_303.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7145.85 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5075.93 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5213.21 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:01, 4293.37 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5022.27 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5680.49 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6064.92 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6483.21 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5433.10 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7305.25 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3467.99 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7204.58 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5154.98 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0908, 'grad_norm': 0.33208853006362915, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.044345881789922714, 'eval_precision': 0.6636904761904762, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6898685228151585, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 4.6799, 'eval_samples_per_second': 323.298, 'eval_steps_per_second': 40.599, 'epoch': 1.0}
{'loss': 0.0348, 'grad_norm': 0.415816068649292, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.043525580316782, 'eval_precision': 0.7007633587786259, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7194357366771159, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6502, 'eval_samples_per_second': 325.363, 'eval_steps_per_second': 40.859, 'epoch': 2.0}
{'loss': 0.021, 'grad_norm': 0.5249454975128174, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04521283134818077, 'eval_precision': 0.6814024390243902, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7000783085356304, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.6377, 'eval_samples_per_second': 326.242, 'eval_steps_per_second': 40.969, 'epoch': 3.0}
{'loss': 0.0134, 'grad_norm': 0.24265804886817932, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.049905918538570404, 'eval_precision': 0.6987179487179487, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.7004016064257028, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6329, 'eval_samples_per_second': 326.579, 'eval_steps_per_second': 41.011, 'epoch': 4.0}
{'loss': 0.0079, 'grad_norm': 0.2204538881778717, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05291791260242462, 'eval_precision': 0.6641337386018237, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6833463643471461, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 8.7765, 'eval_samples_per_second': 172.392, 'eval_steps_per_second': 21.649, 'epoch': 5.0}
{'loss': 0.0042, 'grad_norm': 0.022637834772467613, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.0559428371489048, 'eval_precision': 0.6884735202492211, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6999208234362629, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6473, 'eval_samples_per_second': 325.565, 'eval_steps_per_second': 40.884, 'epoch': 6.0}
{'loss': 0.0022, 'grad_norm': 0.18030628561973572, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06074608489871025, 'eval_precision': 0.6631736526946108, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6873545384018618, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.6261, 'eval_samples_per_second': 327.057, 'eval_steps_per_second': 41.071, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.07898766547441483, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06262840330600739, 'eval_precision': 0.696923076923077, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7128245476003148, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.643, 'eval_samples_per_second': 325.865, 'eval_steps_per_second': 40.922, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.011572035029530525, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06720598042011261, 'eval_precision': 0.6865203761755486, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6957903097696584, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.633, 'eval_samples_per_second': 326.568, 'eval_steps_per_second': 41.01, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.003808737499639392, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06828556209802628, 'eval_precision': 0.695447409733124, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7042925278219395, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6376, 'eval_samples_per_second': 326.248, 'eval_steps_per_second': 40.97, 'epoch': 10.0}
{'loss': 0.0003, 'grad_norm': 0.011440622620284557, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06935035437345505, 'eval_precision': 0.6923076923076923, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7081038552321007, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6289, 'eval_samples_per_second': 326.86, 'eval_steps_per_second': 41.046, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.004620126448571682, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06963684409856796, 'eval_precision': 0.6987577639751553, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7114624505928854, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6361, 'eval_samples_per_second': 326.349, 'eval_steps_per_second': 40.982, 'epoch': 12.0}
{'train_runtime': 985.3138, 'train_samples_per_second': 104.373, 'train_steps_per_second': 1.632, 'train_loss': 0.014718429824865577, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0147
  train_runtime            = 0:16:25.31
  train_samples            =       8570
  train_samples_per_second =    104.373
  train_steps_per_second   =      1.632
[{'loss': 0.0908, 'grad_norm': 0.33208853006362915, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.044345881789922714, 'eval_precision': 0.6636904761904762, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6898685228151585, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 4.6799, 'eval_samples_per_second': 323.298, 'eval_steps_per_second': 40.599, 'epoch': 1.0, 'step': 134}, {'loss': 0.0348, 'grad_norm': 0.415816068649292, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.043525580316782, 'eval_precision': 0.7007633587786259, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7194357366771159, 'eval_accuracy': 0.9884316254347911, 'eval_runtime': 4.6502, 'eval_samples_per_second': 325.363, 'eval_steps_per_second': 40.859, 'epoch': 2.0, 'step': 268}, {'loss': 0.021, 'grad_norm': 0.5249454975128174, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04521283134818077, 'eval_precision': 0.6814024390243902, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7000783085356304, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.6377, 'eval_samples_per_second': 326.242, 'eval_steps_per_second': 40.969, 'epoch': 3.0, 'step': 402}, {'loss': 0.0134, 'grad_norm': 0.24265804886817932, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.049905918538570404, 'eval_precision': 0.6987179487179487, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.7004016064257028, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.6329, 'eval_samples_per_second': 326.579, 'eval_steps_per_second': 41.011, 'epoch': 4.0, 'step': 536}, {'loss': 0.0079, 'grad_norm': 0.2204538881778717, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05291791260242462, 'eval_precision': 0.6641337386018237, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6833463643471461, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 8.7765, 'eval_samples_per_second': 172.392, 'eval_steps_per_second': 21.649, 'epoch': 5.0, 'step': 670}, {'loss': 0.0042, 'grad_norm': 0.022637834772467613, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.0559428371489048, 'eval_precision': 0.6884735202492211, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6999208234362629, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6473, 'eval_samples_per_second': 325.565, 'eval_steps_per_second': 40.884, 'epoch': 6.0, 'step': 804}, {'loss': 0.0022, 'grad_norm': 0.18030628561973572, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06074608489871025, 'eval_precision': 0.6631736526946108, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6873545384018618, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.6261, 'eval_samples_per_second': 327.057, 'eval_steps_per_second': 41.071, 'epoch': 7.0, 'step': 938}, {'loss': 0.001, 'grad_norm': 0.07898766547441483, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.06262840330600739, 'eval_precision': 0.696923076923077, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7128245476003148, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.643, 'eval_samples_per_second': 325.865, 'eval_steps_per_second': 40.922, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0005, 'grad_norm': 0.011572035029530525, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06720598042011261, 'eval_precision': 0.6865203761755486, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6957903097696584, 'eval_accuracy': 0.9882362137022707, 'eval_runtime': 4.633, 'eval_samples_per_second': 326.568, 'eval_steps_per_second': 41.01, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0003, 'grad_norm': 0.003808737499639392, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06828556209802628, 'eval_precision': 0.695447409733124, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7042925278219395, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6376, 'eval_samples_per_second': 326.248, 'eval_steps_per_second': 40.97, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0003, 'grad_norm': 0.011440622620284557, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06935035437345505, 'eval_precision': 0.6923076923076923, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7081038552321007, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 4.6289, 'eval_samples_per_second': 326.86, 'eval_steps_per_second': 41.046, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0002, 'grad_norm': 0.004620126448571682, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06963684409856796, 'eval_precision': 0.6987577639751553, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7114624505928854, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.6361, 'eval_samples_per_second': 326.349, 'eval_steps_per_second': 40.982, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 985.3138, 'train_samples_per_second': 104.373, 'train_steps_per_second': 1.632, 'total_flos': 1.290639183201522e+16, 'train_loss': 0.014718429824865577, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9893
  predict_f1                 =     0.7181
  predict_loss               =     0.0426
  predict_precision          =     0.6925
  predict_recall             =     0.7456
  predict_runtime            = 0:00:03.91
  predict_samples_per_second =    320.172
  predict_steps_per_second   =     40.149
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_303.json completed. F1: 0.7180570221752903
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_101.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7117.05 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5514.13 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5183.91 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5831.89 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6221.56 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6594.62 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6718.81 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6958.90 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6066.41 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7117.05 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3847.02 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7091.24 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3006.24 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1412, 'grad_norm': 0.44892603158950806, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.056869760155677795, 'eval_precision': 0.5670103092783505, 'eval_recall': 0.6199677938808373, 'eval_f1': 0.5923076923076924, 'eval_accuracy': 0.9832336733497479, 'eval_runtime': 2.373, 'eval_samples_per_second': 637.599, 'eval_steps_per_second': 80.069, 'epoch': 1.0}
{'loss': 0.0463, 'grad_norm': 0.32085269689559937, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.05186174437403679, 'eval_precision': 0.6026392961876833, 'eval_recall': 0.6618357487922706, 'eval_f1': 0.6308518802762855, 'eval_accuracy': 0.98421073201235, 'eval_runtime': 2.3542, 'eval_samples_per_second': 642.691, 'eval_steps_per_second': 80.708, 'epoch': 2.0}
{'loss': 0.0366, 'grad_norm': 0.18712680041790009, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04857206717133522, 'eval_precision': 0.665158371040724, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6869158878504674, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.3167, 'eval_samples_per_second': 653.076, 'eval_steps_per_second': 82.012, 'epoch': 3.0}
{'loss': 0.03, 'grad_norm': 0.7899013161659241, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04743708297610283, 'eval_precision': 0.654490106544901, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.672926447574335, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 2.32, 'eval_samples_per_second': 652.167, 'eval_steps_per_second': 81.898, 'epoch': 4.0}
{'loss': 0.0241, 'grad_norm': 0.4501199424266815, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.050453558564186096, 'eval_precision': 0.6290322580645161, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6584804297774367, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3162, 'eval_samples_per_second': 653.213, 'eval_steps_per_second': 82.029, 'epoch': 5.0}
{'loss': 0.0197, 'grad_norm': 1.3737651109695435, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05397411808371544, 'eval_precision': 0.679549114331723, 'eval_recall': 0.679549114331723, 'eval_f1': 0.679549114331723, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.5533, 'eval_samples_per_second': 592.562, 'eval_steps_per_second': 74.413, 'epoch': 6.0}
{'loss': 0.0162, 'grad_norm': 0.27878424525260925, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.055077727884054184, 'eval_precision': 0.6797583081570997, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7014809041309432, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 2.3242, 'eval_samples_per_second': 650.969, 'eval_steps_per_second': 81.748, 'epoch': 7.0}
{'loss': 0.0136, 'grad_norm': 0.39865514636039734, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05590071156620979, 'eval_precision': 0.6915887850467289, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7030878859857481, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 2.3199, 'eval_samples_per_second': 652.171, 'eval_steps_per_second': 81.898, 'epoch': 8.0}
{'loss': 0.0113, 'grad_norm': 0.6810364127159119, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06029243767261505, 'eval_precision': 0.6883720930232559, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7014218009478673, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.325, 'eval_samples_per_second': 650.766, 'eval_steps_per_second': 81.722, 'epoch': 9.0}
{'loss': 0.0099, 'grad_norm': 0.5141301155090332, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06100786477327347, 'eval_precision': 0.6875, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.706342991386061, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.3197, 'eval_samples_per_second': 652.231, 'eval_steps_per_second': 81.906, 'epoch': 10.0}
{'loss': 0.0084, 'grad_norm': 0.1136462464928627, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06114792823791504, 'eval_precision': 0.6918335901386748, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7070866141732283, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 2.3231, 'eval_samples_per_second': 651.291, 'eval_steps_per_second': 81.788, 'epoch': 11.0}
{'loss': 0.0078, 'grad_norm': 0.8791372179985046, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.062308236956596375, 'eval_precision': 0.6881720430107527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7044025157232705, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3307, 'eval_samples_per_second': 649.174, 'eval_steps_per_second': 81.522, 'epoch': 12.0}
{'train_runtime': 398.3609, 'train_samples_per_second': 258.158, 'train_steps_per_second': 8.073, 'train_loss': 0.030406097793460485, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0304
  train_runtime            = 0:06:38.36
  train_samples            =       8570
  train_samples_per_second =    258.158
  train_steps_per_second   =      8.073
[{'loss': 0.1412, 'grad_norm': 0.44892603158950806, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.056869760155677795, 'eval_precision': 0.5670103092783505, 'eval_recall': 0.6199677938808373, 'eval_f1': 0.5923076923076924, 'eval_accuracy': 0.9832336733497479, 'eval_runtime': 2.373, 'eval_samples_per_second': 637.599, 'eval_steps_per_second': 80.069, 'epoch': 1.0, 'step': 268}, {'loss': 0.0463, 'grad_norm': 0.32085269689559937, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.05186174437403679, 'eval_precision': 0.6026392961876833, 'eval_recall': 0.6618357487922706, 'eval_f1': 0.6308518802762855, 'eval_accuracy': 0.98421073201235, 'eval_runtime': 2.3542, 'eval_samples_per_second': 642.691, 'eval_steps_per_second': 80.708, 'epoch': 2.0, 'step': 536}, {'loss': 0.0366, 'grad_norm': 0.18712680041790009, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04857206717133522, 'eval_precision': 0.665158371040724, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6869158878504674, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.3167, 'eval_samples_per_second': 653.076, 'eval_steps_per_second': 82.012, 'epoch': 3.0, 'step': 804}, {'loss': 0.03, 'grad_norm': 0.7899013161659241, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04743708297610283, 'eval_precision': 0.654490106544901, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.672926447574335, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 2.32, 'eval_samples_per_second': 652.167, 'eval_steps_per_second': 81.898, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0241, 'grad_norm': 0.4501199424266815, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.050453558564186096, 'eval_precision': 0.6290322580645161, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6584804297774367, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3162, 'eval_samples_per_second': 653.213, 'eval_steps_per_second': 82.029, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0197, 'grad_norm': 1.3737651109695435, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05397411808371544, 'eval_precision': 0.679549114331723, 'eval_recall': 0.679549114331723, 'eval_f1': 0.679549114331723, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.5533, 'eval_samples_per_second': 592.562, 'eval_steps_per_second': 74.413, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0162, 'grad_norm': 0.27878424525260925, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.055077727884054184, 'eval_precision': 0.6797583081570997, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7014809041309432, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 2.3242, 'eval_samples_per_second': 650.969, 'eval_steps_per_second': 81.748, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0136, 'grad_norm': 0.39865514636039734, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05590071156620979, 'eval_precision': 0.6915887850467289, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7030878859857481, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 2.3199, 'eval_samples_per_second': 652.171, 'eval_steps_per_second': 81.898, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0113, 'grad_norm': 0.6810364127159119, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06029243767261505, 'eval_precision': 0.6883720930232559, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7014218009478673, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.325, 'eval_samples_per_second': 650.766, 'eval_steps_per_second': 81.722, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0099, 'grad_norm': 0.5141301155090332, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06100786477327347, 'eval_precision': 0.6875, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.706342991386061, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.3197, 'eval_samples_per_second': 652.231, 'eval_steps_per_second': 81.906, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0084, 'grad_norm': 0.1136462464928627, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06114792823791504, 'eval_precision': 0.6918335901386748, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7070866141732283, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 2.3231, 'eval_samples_per_second': 651.291, 'eval_steps_per_second': 81.788, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0078, 'grad_norm': 0.8791372179985046, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.062308236956596375, 'eval_precision': 0.6881720430107527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7044025157232705, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3307, 'eval_samples_per_second': 649.174, 'eval_steps_per_second': 81.522, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 398.3609, 'train_samples_per_second': 258.158, 'train_steps_per_second': 8.073, 'total_flos': 3923517530267868.0, 'train_loss': 0.030406097793460485, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9883
  predict_f1                 =     0.6794
  predict_loss               =     0.0455
  predict_precision          =     0.6564
  predict_recall             =     0.7039
  predict_runtime            = 0:00:01.96
  predict_samples_per_second =    637.267
  predict_steps_per_second   =     79.913
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_02_101.json completed. F1: 0.6793650793650794
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_404.json
03091023_elsa-intensity_nb-bert-large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7132.09 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5575.08 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6224.71 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5324.88 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5910.86 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6422.36 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6633.30 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6951.05 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6031.51 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6974.91 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4364.12 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 5973.46 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3215.42 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0947, 'grad_norm': 0.889831006526947, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.045329272747039795, 'eval_precision': 0.6619718309859155, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6714285714285715, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 4.6541, 'eval_samples_per_second': 325.093, 'eval_steps_per_second': 40.825, 'epoch': 1.0}
{'loss': 0.0357, 'grad_norm': 0.25131475925445557, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.045617807656526566, 'eval_precision': 0.6355140186915887, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6460807600950119, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 4.6278, 'eval_samples_per_second': 326.941, 'eval_steps_per_second': 41.057, 'epoch': 2.0}
{'loss': 0.0212, 'grad_norm': 0.18740695714950562, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.046927835792303085, 'eval_precision': 0.6967340590979783, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7088607594936708, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.6305, 'eval_samples_per_second': 326.746, 'eval_steps_per_second': 41.032, 'epoch': 3.0}
{'loss': 0.0129, 'grad_norm': 0.555512547492981, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.0601976178586483, 'eval_precision': 0.6573208722741433, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6682501979414093, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 4.6154, 'eval_samples_per_second': 327.814, 'eval_steps_per_second': 41.166, 'epoch': 4.0}
{'loss': 0.0084, 'grad_norm': 0.2628263235092163, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05817565694451332, 'eval_precision': 0.6893819334389857, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.694888178913738, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.772, 'eval_samples_per_second': 317.061, 'eval_steps_per_second': 39.816, 'epoch': 5.0}
{'loss': 0.0051, 'grad_norm': 0.34540247917175293, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06505274027585983, 'eval_precision': 0.6919431279620853, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6985645933014355, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6655, 'eval_samples_per_second': 324.294, 'eval_steps_per_second': 40.724, 'epoch': 6.0}
{'loss': 0.0026, 'grad_norm': 0.02262861095368862, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06810550391674042, 'eval_precision': 0.6682098765432098, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6824271079590228, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.628, 'eval_samples_per_second': 326.921, 'eval_steps_per_second': 41.054, 'epoch': 7.0}
{'loss': 0.0013, 'grad_norm': 0.019345058128237724, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07547775655984879, 'eval_precision': 0.6923076923076923, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7011128775834659, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.6219, 'eval_samples_per_second': 327.352, 'eval_steps_per_second': 41.108, 'epoch': 8.0}
{'loss': 0.0008, 'grad_norm': 0.06975819170475006, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07702901214361191, 'eval_precision': 0.6945736434108527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7077409162717219, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6323, 'eval_samples_per_second': 326.621, 'eval_steps_per_second': 41.016, 'epoch': 9.0}
{'loss': 0.0004, 'grad_norm': 0.017013931646943092, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07634874433279037, 'eval_precision': 0.697452229299363, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7013610888710969, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6327, 'eval_samples_per_second': 326.589, 'eval_steps_per_second': 41.013, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.08926080912351608, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07863493263721466, 'eval_precision': 0.7046875, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7153053132434576, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.7292, 'eval_samples_per_second': 319.925, 'eval_steps_per_second': 40.176, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.0049156127497553825, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07930760085582733, 'eval_precision': 0.705511811023622, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7133757961783439, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6361, 'eval_samples_per_second': 326.349, 'eval_steps_per_second': 40.982, 'epoch': 12.0}
{'train_runtime': 982.0481, 'train_samples_per_second': 104.72, 'train_steps_per_second': 1.637, 'train_loss': 0.015288828897628174, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0153
  train_runtime            = 0:16:22.04
  train_samples            =       8570
  train_samples_per_second =     104.72
  train_steps_per_second   =      1.637
[{'loss': 0.0947, 'grad_norm': 0.889831006526947, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.045329272747039795, 'eval_precision': 0.6619718309859155, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6714285714285715, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 4.6541, 'eval_samples_per_second': 325.093, 'eval_steps_per_second': 40.825, 'epoch': 1.0, 'step': 134}, {'loss': 0.0357, 'grad_norm': 0.25131475925445557, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.045617807656526566, 'eval_precision': 0.6355140186915887, 'eval_recall': 0.6570048309178744, 'eval_f1': 0.6460807600950119, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 4.6278, 'eval_samples_per_second': 326.941, 'eval_steps_per_second': 41.057, 'epoch': 2.0, 'step': 268}, {'loss': 0.0212, 'grad_norm': 0.18740695714950562, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.046927835792303085, 'eval_precision': 0.6967340590979783, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7088607594936708, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.6305, 'eval_samples_per_second': 326.746, 'eval_steps_per_second': 41.032, 'epoch': 3.0, 'step': 402}, {'loss': 0.0129, 'grad_norm': 0.555512547492981, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.0601976178586483, 'eval_precision': 0.6573208722741433, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6682501979414093, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 4.6154, 'eval_samples_per_second': 327.814, 'eval_steps_per_second': 41.166, 'epoch': 4.0, 'step': 536}, {'loss': 0.0084, 'grad_norm': 0.2628263235092163, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05817565694451332, 'eval_precision': 0.6893819334389857, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.694888178913738, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.772, 'eval_samples_per_second': 317.061, 'eval_steps_per_second': 39.816, 'epoch': 5.0, 'step': 670}, {'loss': 0.0051, 'grad_norm': 0.34540247917175293, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06505274027585983, 'eval_precision': 0.6919431279620853, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6985645933014355, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.6655, 'eval_samples_per_second': 324.294, 'eval_steps_per_second': 40.724, 'epoch': 6.0, 'step': 804}, {'loss': 0.0026, 'grad_norm': 0.02262861095368862, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06810550391674042, 'eval_precision': 0.6682098765432098, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6824271079590228, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 4.628, 'eval_samples_per_second': 326.921, 'eval_steps_per_second': 41.054, 'epoch': 7.0, 'step': 938}, {'loss': 0.0013, 'grad_norm': 0.019345058128237724, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07547775655984879, 'eval_precision': 0.6923076923076923, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7011128775834659, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.6219, 'eval_samples_per_second': 327.352, 'eval_steps_per_second': 41.108, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0008, 'grad_norm': 0.06975819170475006, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.07702901214361191, 'eval_precision': 0.6945736434108527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7077409162717219, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.6323, 'eval_samples_per_second': 326.621, 'eval_steps_per_second': 41.016, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0004, 'grad_norm': 0.017013931646943092, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07634874433279037, 'eval_precision': 0.697452229299363, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7013610888710969, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6327, 'eval_samples_per_second': 326.589, 'eval_steps_per_second': 41.013, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0002, 'grad_norm': 0.08926080912351608, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07863493263721466, 'eval_precision': 0.7046875, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7153053132434576, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.7292, 'eval_samples_per_second': 319.925, 'eval_steps_per_second': 40.176, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0002, 'grad_norm': 0.0049156127497553825, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07930760085582733, 'eval_precision': 0.705511811023622, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7133757961783439, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 4.6361, 'eval_samples_per_second': 326.349, 'eval_steps_per_second': 40.982, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 982.0481, 'train_samples_per_second': 104.72, 'train_steps_per_second': 1.637, 'total_flos': 1.2875344500118716e+16, 'train_loss': 0.015288828897628174, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9869
  predict_f1                 =     0.6551
  predict_loss               =     0.0451
  predict_precision          =     0.6501
  predict_recall             =     0.6601
  predict_runtime            = 0:00:03.92
  predict_samples_per_second =    318.824
  predict_steps_per_second   =      39.98
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert-large_10_404.json completed. F1: 0.6550598476605005
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_303.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6487.35 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 6033.36 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5131.08 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5809.88 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6216.77 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6617.73 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6733.56 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6956.39 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6206.71 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6568.08 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3203.84 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7143.52 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4476.06 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-1072 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1086, 'grad_norm': 0.23683685064315796, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05327640846371651, 'eval_precision': 0.5301369863013699, 'eval_recall': 0.6231884057971014, 'eval_f1': 0.5729089563286455, 'eval_accuracy': 0.9827256028451948, 'eval_runtime': 2.4327, 'eval_samples_per_second': 621.942, 'eval_steps_per_second': 78.102, 'epoch': 1.0}
{'loss': 0.0422, 'grad_norm': 0.24960795044898987, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.05115319788455963, 'eval_precision': 0.6358208955223881, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6599535243996902, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3224, 'eval_samples_per_second': 651.494, 'eval_steps_per_second': 81.814, 'epoch': 2.0}
{'loss': 0.0298, 'grad_norm': 0.2962501645088196, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.050681084394454956, 'eval_precision': 0.6631259484066768, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6828125000000002, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 2.3896, 'eval_samples_per_second': 633.147, 'eval_steps_per_second': 79.51, 'epoch': 3.0}
{'loss': 0.0201, 'grad_norm': 0.4382416903972626, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.061155129224061966, 'eval_precision': 0.6607669616519174, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6897613548883756, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3186, 'eval_samples_per_second': 652.548, 'eval_steps_per_second': 81.946, 'epoch': 4.0}
{'loss': 0.0137, 'grad_norm': 0.3388197720050812, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.0630999505519867, 'eval_precision': 0.6478658536585366, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6656225528582616, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3296, 'eval_samples_per_second': 649.454, 'eval_steps_per_second': 81.557, 'epoch': 5.0}
{'loss': 0.0094, 'grad_norm': 0.4096653163433075, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07340037822723389, 'eval_precision': 0.630527817403709, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6686838124054463, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.3405, 'eval_samples_per_second': 646.443, 'eval_steps_per_second': 81.179, 'epoch': 6.0}
{'loss': 0.0055, 'grad_norm': 0.17270998656749725, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.0768149197101593, 'eval_precision': 0.6226993865030674, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6378633150039277, 'eval_accuracy': 0.9847969672099113, 'eval_runtime': 2.3137, 'eval_samples_per_second': 653.929, 'eval_steps_per_second': 82.119, 'epoch': 7.0}
{'loss': 0.0036, 'grad_norm': 0.10435841977596283, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07927581667900085, 'eval_precision': 0.6253776435045317, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6453624318004677, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3083, 'eval_samples_per_second': 655.447, 'eval_steps_per_second': 82.31, 'epoch': 8.0}
{'loss': 0.0022, 'grad_norm': 0.32238200306892395, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08227469027042389, 'eval_precision': 0.65527950310559, 'eval_recall': 0.679549114331723, 'eval_f1': 0.667193675889328, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3279, 'eval_samples_per_second': 649.93, 'eval_steps_per_second': 81.617, 'epoch': 9.0}
{'loss': 0.0014, 'grad_norm': 0.020788302645087242, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08557554334402084, 'eval_precision': 0.6541471048513302, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6634920634920636, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3166, 'eval_samples_per_second': 653.123, 'eval_steps_per_second': 82.018, 'epoch': 10.0}
{'loss': 0.0012, 'grad_norm': 0.05419745296239853, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08747870475053787, 'eval_precision': 0.6451612903225806, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.660377358490566, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.3398, 'eval_samples_per_second': 646.643, 'eval_steps_per_second': 81.204, 'epoch': 11.0}
{'loss': 0.0009, 'grad_norm': 0.07728418707847595, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0881638303399086, 'eval_precision': 0.6453576864535768, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6635367762128326, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.4192, 'eval_samples_per_second': 625.422, 'eval_steps_per_second': 78.539, 'epoch': 12.0}
{'train_runtime': 397.9325, 'train_samples_per_second': 258.436, 'train_steps_per_second': 4.041, 'train_loss': 0.019881075831936365, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0199
  train_runtime            = 0:06:37.93
  train_samples            =       8570
  train_samples_per_second =    258.436
  train_steps_per_second   =      4.041
[{'loss': 0.1086, 'grad_norm': 0.23683685064315796, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05327640846371651, 'eval_precision': 0.5301369863013699, 'eval_recall': 0.6231884057971014, 'eval_f1': 0.5729089563286455, 'eval_accuracy': 0.9827256028451948, 'eval_runtime': 2.4327, 'eval_samples_per_second': 621.942, 'eval_steps_per_second': 78.102, 'epoch': 1.0, 'step': 134}, {'loss': 0.0422, 'grad_norm': 0.24960795044898987, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.05115319788455963, 'eval_precision': 0.6358208955223881, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6599535243996902, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3224, 'eval_samples_per_second': 651.494, 'eval_steps_per_second': 81.814, 'epoch': 2.0, 'step': 268}, {'loss': 0.0298, 'grad_norm': 0.2962501645088196, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.050681084394454956, 'eval_precision': 0.6631259484066768, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6828125000000002, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 2.3896, 'eval_samples_per_second': 633.147, 'eval_steps_per_second': 79.51, 'epoch': 3.0, 'step': 402}, {'loss': 0.0201, 'grad_norm': 0.4382416903972626, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.061155129224061966, 'eval_precision': 0.6607669616519174, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6897613548883756, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3186, 'eval_samples_per_second': 652.548, 'eval_steps_per_second': 81.946, 'epoch': 4.0, 'step': 536}, {'loss': 0.0137, 'grad_norm': 0.3388197720050812, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.0630999505519867, 'eval_precision': 0.6478658536585366, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6656225528582616, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3296, 'eval_samples_per_second': 649.454, 'eval_steps_per_second': 81.557, 'epoch': 5.0, 'step': 670}, {'loss': 0.0094, 'grad_norm': 0.4096653163433075, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.07340037822723389, 'eval_precision': 0.630527817403709, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6686838124054463, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.3405, 'eval_samples_per_second': 646.443, 'eval_steps_per_second': 81.179, 'epoch': 6.0, 'step': 804}, {'loss': 0.0055, 'grad_norm': 0.17270998656749725, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.0768149197101593, 'eval_precision': 0.6226993865030674, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6378633150039277, 'eval_accuracy': 0.9847969672099113, 'eval_runtime': 2.3137, 'eval_samples_per_second': 653.929, 'eval_steps_per_second': 82.119, 'epoch': 7.0, 'step': 938}, {'loss': 0.0036, 'grad_norm': 0.10435841977596283, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07927581667900085, 'eval_precision': 0.6253776435045317, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6453624318004677, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3083, 'eval_samples_per_second': 655.447, 'eval_steps_per_second': 82.31, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0022, 'grad_norm': 0.32238200306892395, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.08227469027042389, 'eval_precision': 0.65527950310559, 'eval_recall': 0.679549114331723, 'eval_f1': 0.667193675889328, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.3279, 'eval_samples_per_second': 649.93, 'eval_steps_per_second': 81.617, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0014, 'grad_norm': 0.020788302645087242, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.08557554334402084, 'eval_precision': 0.6541471048513302, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6634920634920636, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3166, 'eval_samples_per_second': 653.123, 'eval_steps_per_second': 82.018, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0012, 'grad_norm': 0.05419745296239853, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.08747870475053787, 'eval_precision': 0.6451612903225806, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.660377358490566, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.3398, 'eval_samples_per_second': 646.643, 'eval_steps_per_second': 81.204, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0009, 'grad_norm': 0.07728418707847595, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.0881638303399086, 'eval_precision': 0.6453576864535768, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6635367762128326, 'eval_accuracy': 0.9855395317934889, 'eval_runtime': 2.4192, 'eval_samples_per_second': 625.422, 'eval_steps_per_second': 78.539, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 397.9325, 'train_samples_per_second': 258.436, 'train_steps_per_second': 4.041, 'total_flos': 4434832167386640.0, 'train_loss': 0.019881075831936365, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9887
  predict_f1                 =     0.7039
  predict_loss               =     0.0455
  predict_precision          =     0.6775
  predict_recall             =     0.7325
  predict_runtime            = 0:00:02.01
  predict_samples_per_second =    620.381
  predict_steps_per_second   =     77.795
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_303.json completed. F1: 0.7038988408851421
/cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_404.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_404.json
03091023_elsa-intensity_nb-bert_base Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 7064.25 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 6345.97 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5918.38 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6365.90 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6610.33 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6870.64 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6911.53 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7086.11 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6422.51 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6513.73 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3442.69 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7077.43 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4113.96 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03091023_elsa-intensity_nb-bert_base/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03091023_elsa-intensity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1032, 'grad_norm': 0.7537677884101868, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.05178171023726463, 'eval_precision': 0.6170212765957447, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6348709929632527, 'eval_accuracy': 0.9850314612889358, 'eval_runtime': 2.3785, 'eval_samples_per_second': 636.105, 'eval_steps_per_second': 79.881, 'epoch': 1.0}
{'loss': 0.0398, 'grad_norm': 0.20180456340312958, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04950021952390671, 'eval_precision': 0.6624405705229794, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6677316293929711, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.4514, 'eval_samples_per_second': 617.202, 'eval_steps_per_second': 77.507, 'epoch': 2.0}
{'loss': 0.0284, 'grad_norm': 0.31446173787117004, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.052864763885736465, 'eval_precision': 0.652931854199683, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.65814696485623, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3614, 'eval_samples_per_second': 640.731, 'eval_steps_per_second': 80.462, 'epoch': 3.0}
{'loss': 0.0192, 'grad_norm': 0.41239291429519653, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.06002508103847504, 'eval_precision': 0.6589785831960461, 'eval_recall': 0.644122383252818, 'eval_f1': 0.6514657980456025, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3455, 'eval_samples_per_second': 645.069, 'eval_steps_per_second': 81.007, 'epoch': 4.0}
{'loss': 0.0133, 'grad_norm': 0.48192211985588074, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06123039871454239, 'eval_precision': 0.6409861325115562, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6551181102362206, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.3186, 'eval_samples_per_second': 652.539, 'eval_steps_per_second': 81.945, 'epoch': 5.0}
{'loss': 0.0085, 'grad_norm': 0.4827412962913513, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06995578855276108, 'eval_precision': 0.6244274809160305, 'eval_recall': 0.6586151368760065, 'eval_f1': 0.6410658307210031, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 2.3381, 'eval_samples_per_second': 647.11, 'eval_steps_per_second': 81.263, 'epoch': 6.0}
{'loss': 0.0062, 'grad_norm': 0.6618915796279907, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07265818864107132, 'eval_precision': 0.6305343511450382, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6473354231974922, 'eval_accuracy': 0.9850314612889358, 'eval_runtime': 2.3325, 'eval_samples_per_second': 648.662, 'eval_steps_per_second': 81.458, 'epoch': 7.0}
{'loss': 0.0037, 'grad_norm': 0.1816193014383316, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.078424833714962, 'eval_precision': 0.6321321321321322, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6542346542346542, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 2.3325, 'eval_samples_per_second': 648.671, 'eval_steps_per_second': 81.459, 'epoch': 8.0}
{'loss': 0.0024, 'grad_norm': 0.6961805820465088, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.08171355724334717, 'eval_precision': 0.6380090497737556, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6588785046728972, 'eval_accuracy': 0.9849532965959277, 'eval_runtime': 2.3272, 'eval_samples_per_second': 650.141, 'eval_steps_per_second': 81.644, 'epoch': 9.0}
{'loss': 0.0017, 'grad_norm': 0.008059787563979626, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08350390195846558, 'eval_precision': 0.6257575757575757, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6448087431693988, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3333, 'eval_samples_per_second': 648.447, 'eval_steps_per_second': 81.431, 'epoch': 10.0}
{'loss': 0.0012, 'grad_norm': 0.04274490103125572, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08586077392101288, 'eval_precision': 0.6389728096676737, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6593920498830865, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 2.3321, 'eval_samples_per_second': 648.784, 'eval_steps_per_second': 81.473, 'epoch': 11.0}
{'loss': 0.0009, 'grad_norm': 0.014060147106647491, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08786269277334213, 'eval_precision': 0.6287425149700598, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6516679596586501, 'eval_accuracy': 0.9849142142494235, 'eval_runtime': 2.4401, 'eval_samples_per_second': 620.061, 'eval_steps_per_second': 77.866, 'epoch': 12.0}
{'train_runtime': 396.5116, 'train_samples_per_second': 259.362, 'train_steps_per_second': 4.055, 'train_loss': 0.019052968337316418, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0191
  train_runtime            = 0:06:36.51
  train_samples            =       8570
  train_samples_per_second =    259.362
  train_steps_per_second   =      4.055
[{'loss': 0.1032, 'grad_norm': 0.7537677884101868, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.05178171023726463, 'eval_precision': 0.6170212765957447, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.6348709929632527, 'eval_accuracy': 0.9850314612889358, 'eval_runtime': 2.3785, 'eval_samples_per_second': 636.105, 'eval_steps_per_second': 79.881, 'epoch': 1.0, 'step': 134}, {'loss': 0.0398, 'grad_norm': 0.20180456340312958, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04950021952390671, 'eval_precision': 0.6624405705229794, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.6677316293929711, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.4514, 'eval_samples_per_second': 617.202, 'eval_steps_per_second': 77.507, 'epoch': 2.0, 'step': 268}, {'loss': 0.0284, 'grad_norm': 0.31446173787117004, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.052864763885736465, 'eval_precision': 0.652931854199683, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.65814696485623, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 2.3614, 'eval_samples_per_second': 640.731, 'eval_steps_per_second': 80.462, 'epoch': 3.0, 'step': 402}, {'loss': 0.0192, 'grad_norm': 0.41239291429519653, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.06002508103847504, 'eval_precision': 0.6589785831960461, 'eval_recall': 0.644122383252818, 'eval_f1': 0.6514657980456025, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3455, 'eval_samples_per_second': 645.069, 'eval_steps_per_second': 81.007, 'epoch': 4.0, 'step': 536}, {'loss': 0.0133, 'grad_norm': 0.48192211985588074, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.06123039871454239, 'eval_precision': 0.6409861325115562, 'eval_recall': 0.6698872785829307, 'eval_f1': 0.6551181102362206, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.3186, 'eval_samples_per_second': 652.539, 'eval_steps_per_second': 81.945, 'epoch': 5.0, 'step': 670}, {'loss': 0.0085, 'grad_norm': 0.4827412962913513, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06995578855276108, 'eval_precision': 0.6244274809160305, 'eval_recall': 0.6586151368760065, 'eval_f1': 0.6410658307210031, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 2.3381, 'eval_samples_per_second': 647.11, 'eval_steps_per_second': 81.263, 'epoch': 6.0, 'step': 804}, {'loss': 0.0062, 'grad_norm': 0.6618915796279907, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.07265818864107132, 'eval_precision': 0.6305343511450382, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6473354231974922, 'eval_accuracy': 0.9850314612889358, 'eval_runtime': 2.3325, 'eval_samples_per_second': 648.662, 'eval_steps_per_second': 81.458, 'epoch': 7.0, 'step': 938}, {'loss': 0.0037, 'grad_norm': 0.1816193014383316, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.078424833714962, 'eval_precision': 0.6321321321321322, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6542346542346542, 'eval_accuracy': 0.9853832024074726, 'eval_runtime': 2.3325, 'eval_samples_per_second': 648.671, 'eval_steps_per_second': 81.459, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0024, 'grad_norm': 0.6961805820465088, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.08171355724334717, 'eval_precision': 0.6380090497737556, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6588785046728972, 'eval_accuracy': 0.9849532965959277, 'eval_runtime': 2.3272, 'eval_samples_per_second': 650.141, 'eval_steps_per_second': 81.644, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0017, 'grad_norm': 0.008059787563979626, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.08350390195846558, 'eval_precision': 0.6257575757575757, 'eval_recall': 0.6650563607085346, 'eval_f1': 0.6448087431693988, 'eval_accuracy': 0.9852659553679602, 'eval_runtime': 2.3333, 'eval_samples_per_second': 648.447, 'eval_steps_per_second': 81.431, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0012, 'grad_norm': 0.04274490103125572, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.08586077392101288, 'eval_precision': 0.6389728096676737, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6593920498830865, 'eval_accuracy': 0.9853050377144644, 'eval_runtime': 2.3321, 'eval_samples_per_second': 648.784, 'eval_steps_per_second': 81.473, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0009, 'grad_norm': 0.014060147106647491, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.08786269277334213, 'eval_precision': 0.6287425149700598, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6516679596586501, 'eval_accuracy': 0.9849142142494235, 'eval_runtime': 2.4401, 'eval_samples_per_second': 620.061, 'eval_steps_per_second': 77.866, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 396.5116, 'train_samples_per_second': 259.362, 'train_steps_per_second': 4.055, 'total_flos': 4419828838366056.0, 'train_loss': 0.019052968337316418, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9882
  predict_f1                 =     0.6832
  predict_loss               =     0.0432
  predict_precision          =     0.6716
  predict_recall             =     0.6952
  predict_runtime            = 0:00:01.96
  predict_samples_per_second =    637.052
  predict_steps_per_second   =     79.886
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03091023_elsa-intensity_nb-bert_base_11_404.json completed. F1: 0.6831896551724138

Job 10910776 consumed 67.5 billing hours from project nn9851k.

Submitted 2024-03-09T10:40:24; waited 15.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 14.0 hours
Elapsed wallclock time:   8.4 hours

Task and CPU statistics:
ID              CPUs  Tasks  CPU util                Start  Elapsed  Exit status
10910776           1            0.0 %  2024-03-09T10:40:39    8.4 h  0
10910776.batch     1      1    88.1 %  2024-03-09T10:40:39    8.4 h  0

Used CPU time:   7.4 CPU hours
Unused CPU time: 1.0 CPU hours

Memory statistics, in GiB:
ID               Alloc   Usage
10910776          24.0        
10910776.batch    24.0     2.6

GPU usage stats:
Job 10910776 completed at Sat Mar 9 19:07:09 CET 2024
