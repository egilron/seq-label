Starting job 10917672 on gpu-12-8 on saga at Mon Mar 11 14:04:35 CET 2024

/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_06_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_06_101.json
03111215_ner2_norbert3-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:12, 2303.41 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:07, 3524.75 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 4610.17 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:04, 5205.53 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:01<00:04, 5976.30 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6697.84 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 7108.68 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:03, 6274.37 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:02, 7029.92 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 7531.67 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 8336.35 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:02<00:02, 8049.06 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:01, 8100.99 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7930.50 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 8021.29 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 8072.53 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7626.51 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7509.73 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7551.86 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 6332.48 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 6741.05 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7252.34 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 7509.88 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 7407.22 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 7307.01 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:04<00:00, 6455.53 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6756.55 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 7000.73 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 7245.64 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 5997.84 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 8671.17 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 8386.09 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 6422.27 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 6586.80 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 6833.55 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7919.22 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 4968.83 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 5033.82 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 5223.36 examples/s]
03111215_ner2_norbert3-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0426, 'grad_norm': 0.20905907452106476, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.009331299923360348, 'eval_precision': 0.9645969498910676, 'eval_recall': 0.9316149395055234, 'eval_f1': 0.9478191062349478, 'eval_accuracy': 0.9969830964669694, 'eval_runtime': 15.7924, 'eval_samples_per_second': 272.283, 'eval_steps_per_second': 34.067, 'epoch': 1.0}
{'loss': 0.0051, 'grad_norm': 0.765914797782898, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.008791268803179264, 'eval_precision': 0.9603803486529319, 'eval_recall': 0.9563387690689111, 'eval_f1': 0.9583552978386928, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 15.3224, 'eval_samples_per_second': 280.635, 'eval_steps_per_second': 35.112, 'epoch': 2.0}
{'loss': 0.0024, 'grad_norm': 0.33681803941726685, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.010502147488296032, 'eval_precision': 0.9600840336134454, 'eval_recall': 0.961599158337717, 'eval_f1': 0.9608409986859396, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 15.3693, 'eval_samples_per_second': 279.778, 'eval_steps_per_second': 35.005, 'epoch': 3.0}
{'loss': 0.0012, 'grad_norm': 0.08976755291223526, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.011518607847392559, 'eval_precision': 0.9590120861797162, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9595162986330179, 'eval_accuracy': 0.9977077448646091, 'eval_runtime': 15.3321, 'eval_samples_per_second': 280.457, 'eval_steps_per_second': 35.09, 'epoch': 4.0}
{'loss': 0.0008, 'grad_norm': 0.03498028591275215, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.012067076750099659, 'eval_precision': 0.9557982319292772, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9612970711297072, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.3104, 'eval_samples_per_second': 280.855, 'eval_steps_per_second': 35.14, 'epoch': 5.0}
{'loss': 0.0005, 'grad_norm': 0.003025566693395376, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.014745385386049747, 'eval_precision': 0.9600420609884333, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9602945043386799, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.1557, 'eval_samples_per_second': 283.722, 'eval_steps_per_second': 35.498, 'epoch': 6.0}
{'loss': 0.0004, 'grad_norm': 0.002984351012855768, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.014405391179025173, 'eval_precision': 0.9591623036649215, 'eval_recall': 0.9637033140452393, 'eval_f1': 0.96142744686434, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.2891, 'eval_samples_per_second': 281.245, 'eval_steps_per_second': 35.188, 'epoch': 7.0}
{'loss': 0.0002, 'grad_norm': 0.01102866418659687, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.016447067260742188, 'eval_precision': 0.9644184811471057, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9598308668076111, 'eval_accuracy': 0.9975894349221373, 'eval_runtime': 15.1965, 'eval_samples_per_second': 282.96, 'eval_steps_per_second': 35.403, 'epoch': 8.0}
{'loss': 0.0002, 'grad_norm': 0.0036958351265639067, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.01571115292608738, 'eval_precision': 0.954498448810755, 'eval_recall': 0.9710678590215676, 'eval_f1': 0.9627118644067797, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.3021, 'eval_samples_per_second': 281.007, 'eval_steps_per_second': 35.159, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.024808919057250023, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.0162094384431839, 'eval_precision': 0.9573361082206036, 'eval_recall': 0.9679116254602841, 'eval_f1': 0.9625948208213445, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.376, 'eval_samples_per_second': 279.657, 'eval_steps_per_second': 34.99, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.003824171843007207, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.01670410856604576, 'eval_precision': 0.9568382735309412, 'eval_recall': 0.9679116254602841, 'eval_f1': 0.9623430962343096, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.2918, 'eval_samples_per_second': 281.196, 'eval_steps_per_second': 35.182, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.0006333171622827649, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.01690272055566311, 'eval_precision': 0.9553710430721328, 'eval_recall': 0.9684376643871646, 'eval_f1': 0.9618599791013583, 'eval_accuracy': 0.9977077448646091, 'eval_runtime': 15.4639, 'eval_samples_per_second': 278.066, 'eval_steps_per_second': 34.791, 'epoch': 12.0}
{'train_runtime': 3743.7732, 'train_samples_per_second': 95.743, 'train_steps_per_second': 1.497, 'train_loss': 0.004462677560983323, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0045
  train_runtime            = 1:02:23.77
  train_samples            =      29870
  train_samples_per_second =     95.743
  train_steps_per_second   =      1.497
[{'loss': 0.0426, 'grad_norm': 0.20905907452106476, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 467}, {'eval_loss': 0.009331299923360348, 'eval_precision': 0.9645969498910676, 'eval_recall': 0.9316149395055234, 'eval_f1': 0.9478191062349478, 'eval_accuracy': 0.9969830964669694, 'eval_runtime': 15.7924, 'eval_samples_per_second': 272.283, 'eval_steps_per_second': 34.067, 'epoch': 1.0, 'step': 467}, {'loss': 0.0051, 'grad_norm': 0.765914797782898, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 934}, {'eval_loss': 0.008791268803179264, 'eval_precision': 0.9603803486529319, 'eval_recall': 0.9563387690689111, 'eval_f1': 0.9583552978386928, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 15.3224, 'eval_samples_per_second': 280.635, 'eval_steps_per_second': 35.112, 'epoch': 2.0, 'step': 934}, {'loss': 0.0024, 'grad_norm': 0.33681803941726685, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 1401}, {'eval_loss': 0.010502147488296032, 'eval_precision': 0.9600840336134454, 'eval_recall': 0.961599158337717, 'eval_f1': 0.9608409986859396, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 15.3693, 'eval_samples_per_second': 279.778, 'eval_steps_per_second': 35.005, 'epoch': 3.0, 'step': 1401}, {'loss': 0.0012, 'grad_norm': 0.08976755291223526, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1868}, {'eval_loss': 0.011518607847392559, 'eval_precision': 0.9590120861797162, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9595162986330179, 'eval_accuracy': 0.9977077448646091, 'eval_runtime': 15.3321, 'eval_samples_per_second': 280.457, 'eval_steps_per_second': 35.09, 'epoch': 4.0, 'step': 1868}, {'loss': 0.0008, 'grad_norm': 0.03498028591275215, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 2335}, {'eval_loss': 0.012067076750099659, 'eval_precision': 0.9557982319292772, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9612970711297072, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.3104, 'eval_samples_per_second': 280.855, 'eval_steps_per_second': 35.14, 'epoch': 5.0, 'step': 2335}, {'loss': 0.0005, 'grad_norm': 0.003025566693395376, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 2802}, {'eval_loss': 0.014745385386049747, 'eval_precision': 0.9600420609884333, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9602945043386799, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.1557, 'eval_samples_per_second': 283.722, 'eval_steps_per_second': 35.498, 'epoch': 6.0, 'step': 2802}, {'loss': 0.0004, 'grad_norm': 0.002984351012855768, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 3269}, {'eval_loss': 0.014405391179025173, 'eval_precision': 0.9591623036649215, 'eval_recall': 0.9637033140452393, 'eval_f1': 0.96142744686434, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.2891, 'eval_samples_per_second': 281.245, 'eval_steps_per_second': 35.188, 'epoch': 7.0, 'step': 3269}, {'loss': 0.0002, 'grad_norm': 0.01102866418659687, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 3736}, {'eval_loss': 0.016447067260742188, 'eval_precision': 0.9644184811471057, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9598308668076111, 'eval_accuracy': 0.9975894349221373, 'eval_runtime': 15.1965, 'eval_samples_per_second': 282.96, 'eval_steps_per_second': 35.403, 'epoch': 8.0, 'step': 3736}, {'loss': 0.0002, 'grad_norm': 0.0036958351265639067, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 4203}, {'eval_loss': 0.01571115292608738, 'eval_precision': 0.954498448810755, 'eval_recall': 0.9710678590215676, 'eval_f1': 0.9627118644067797, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.3021, 'eval_samples_per_second': 281.007, 'eval_steps_per_second': 35.159, 'epoch': 9.0, 'step': 4203}, {'loss': 0.0001, 'grad_norm': 0.024808919057250023, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 4670}, {'eval_loss': 0.0162094384431839, 'eval_precision': 0.9573361082206036, 'eval_recall': 0.9679116254602841, 'eval_f1': 0.9625948208213445, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.376, 'eval_samples_per_second': 279.657, 'eval_steps_per_second': 34.99, 'epoch': 10.0, 'step': 4670}, {'loss': 0.0001, 'grad_norm': 0.003824171843007207, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 5137}, {'eval_loss': 0.01670410856604576, 'eval_precision': 0.9568382735309412, 'eval_recall': 0.9679116254602841, 'eval_f1': 0.9623430962343096, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.2918, 'eval_samples_per_second': 281.196, 'eval_steps_per_second': 35.182, 'epoch': 11.0, 'step': 5137}, {'loss': 0.0001, 'grad_norm': 0.0006333171622827649, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 5604}, {'eval_loss': 0.01690272055566311, 'eval_precision': 0.9553710430721328, 'eval_recall': 0.9684376643871646, 'eval_f1': 0.9618599791013583, 'eval_accuracy': 0.9977077448646091, 'eval_runtime': 15.4639, 'eval_samples_per_second': 278.066, 'eval_steps_per_second': 34.791, 'epoch': 12.0, 'step': 5604}, {'train_runtime': 3743.7732, 'train_samples_per_second': 95.743, 'train_steps_per_second': 1.497, 'total_flos': 3.966332167288483e+16, 'train_loss': 0.004462677560983323, 'epoch': 12.0, 'step': 5604}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9963
  predict_f1                 =     0.9364
  predict_loss               =     0.0143
  predict_precision          =     0.9382
  predict_recall             =     0.9345
  predict_runtime            = 0:00:12.35
  predict_samples_per_second =    279.126
  predict_steps_per_second   =     34.951
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_06_101.json completed. F1: 0.9363636363636364
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_09_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_09_101.json
03111215_ner2_norbert3-large_09_101.json seems to be completed. Exiting
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_00_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_00_101.json
03111215_ner2_norbert3-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:14, 2034.97 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:09, 3020.98 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:06, 4045.23 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:01<00:05, 4703.50 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:01<00:04, 5547.33 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6321.26 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 6809.44 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:02, 7408.92 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:03, 6480.48 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 7072.65 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:02<00:02, 8014.98 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:02<00:02, 7819.00 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:01, 7950.44 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7776.80 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7943.99 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 8061.34 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7732.50 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7608.34 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:03<00:01, 7640.05 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 7671.74 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 7692.68 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7988.01 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 6561.69 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 6699.60 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 6799.91 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:04<00:00, 6144.78 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6502.10 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6815.01 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 7084.11 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:05<00:00, 5797.37 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 7714.47 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 7987.56 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 4748.30 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 5112.71 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4816.50 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7057.45 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 5763.14 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 6197.45 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 4314.39 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03111215_ner2_norbert3-large/checkpoint-934 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03111215_ner2_norbert3-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0291, 'grad_norm': 3.2658586502075195, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.008217421360313892, 'eval_precision': 0.964247598719317, 'eval_recall': 0.9505523408732246, 'eval_f1': 0.9573509933774835, 'eval_accuracy': 0.9974711249796655, 'eval_runtime': 15.3262, 'eval_samples_per_second': 280.565, 'eval_steps_per_second': 35.103, 'epoch': 1.0}
{'loss': 0.0042, 'grad_norm': 0.06078805401921272, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.009586527943611145, 'eval_precision': 0.9676735559088501, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9640971488912355, 'eval_accuracy': 0.997766899835845, 'eval_runtime': 15.1462, 'eval_samples_per_second': 283.9, 'eval_steps_per_second': 35.521, 'epoch': 2.0}
{'loss': 0.0017, 'grad_norm': 0.006479821167886257, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.010823146440088749, 'eval_precision': 0.9644562334217507, 'eval_recall': 0.9563387690689111, 'eval_f1': 0.9603803486529318, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.2269, 'eval_samples_per_second': 282.395, 'eval_steps_per_second': 35.332, 'epoch': 3.0}
{'loss': 0.001, 'grad_norm': 0.06669572740793228, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.012443904764950275, 'eval_precision': 0.9597911227154047, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9633123689727463, 'eval_accuracy': 0.9978556322926988, 'eval_runtime': 15.2494, 'eval_samples_per_second': 281.979, 'eval_steps_per_second': 35.28, 'epoch': 4.0}
{'loss': 0.0006, 'grad_norm': 0.004115555435419083, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.01387442834675312, 'eval_precision': 0.9634727368978295, 'eval_recall': 0.9573908469226723, 'eval_f1': 0.9604221635883905, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.2325, 'eval_samples_per_second': 282.291, 'eval_steps_per_second': 35.319, 'epoch': 5.0}
{'loss': 0.0003, 'grad_norm': 3.550933599472046, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.01588158868253231, 'eval_precision': 0.9600210415570752, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9600210415570752, 'eval_accuracy': 0.9976633786361822, 'eval_runtime': 15.18, 'eval_samples_per_second': 283.267, 'eval_steps_per_second': 35.441, 'epoch': 6.0}
Traceback (most recent call last):
  File "/cluster/work/users/egilron/seq-label_github/seq_label.py", line 289, in <module>
    train_result = trainer.train(resume_from_checkpoint=False)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 2911, in training_step
    self.accelerator.backward(loss)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/accelerate/accelerator.py", line 1966, in backward
    loss.backward(**kwargs)
  File "/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Job 10917672 consumed 13.9 billing hours from project nn9851k.

Submitted 2024-03-11T14:04:18; waited 16.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 14.0 hours
Elapsed wallclock time:   1.7 hours

Task and CPU statistics:
ID              CPUs  Tasks  CPU util                Start  Elapsed  Exit status
10917672           1            0.0 %  2024-03-11T14:04:34    1.7 h  1
10917672.batch     1      1    84.1 %  2024-03-11T14:04:34    1.7 h  1

Used CPU time:   1.5 CPU hours
Unused CPU time: 16.6 CPU minutes

Memory statistics, in GiB:
ID               Alloc   Usage
10917672          24.0        
10917672.batch    24.0     2.3

GPU usage stats:
Job 10917672 completed at Mon Mar 11 15:48:55 CET 2024
