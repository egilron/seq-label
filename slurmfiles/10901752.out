Starting job 10901752 on gpu-12-5 on saga at Fri Mar 8 11:04:50 CET 2024

/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_large_18.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_large_18.json
03081014_elsa-intensity_NB-BERT_large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3401.43 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4411.22 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5388.36 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5655.05 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6151.99 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6574.05 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6738.36 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7008.71 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6115.87 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7300.43 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5985.25 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7273.23 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5108.51 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03081014_elsa-intensity_NB-BERT_large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0723, 'grad_norm': 0.6361818909645081, 'learning_rate': 4.5e-05, 'epoch': 1.0}
{'eval_loss': 0.048027291893959045, 'eval_precision': 0.7059773828756059, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.7048387096774195, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6294, 'eval_samples_per_second': 326.822, 'eval_steps_per_second': 41.042, 'epoch': 1.0}
{'loss': 0.0321, 'grad_norm': 0.5500916242599487, 'learning_rate': 4e-05, 'epoch': 2.0}
{'eval_loss': 0.044993218034505844, 'eval_precision': 0.6842105263157895, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6977111286503552, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.7036, 'eval_samples_per_second': 321.668, 'eval_steps_per_second': 40.395, 'epoch': 2.0}
{'loss': 0.0183, 'grad_norm': 0.43859848380088806, 'learning_rate': 3.5e-05, 'epoch': 3.0}
{'eval_loss': 0.04890437424182892, 'eval_precision': 0.7127329192546584, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7256916996047431, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.6107, 'eval_samples_per_second': 328.147, 'eval_steps_per_second': 41.208, 'epoch': 3.0}
{'loss': 0.0102, 'grad_norm': 0.43421486020088196, 'learning_rate': 3e-05, 'epoch': 4.0}
{'eval_loss': 0.05644597113132477, 'eval_precision': 0.6640986132511556, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6787401574803149, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.6155, 'eval_samples_per_second': 327.807, 'eval_steps_per_second': 41.165, 'epoch': 4.0}
{'loss': 0.0065, 'grad_norm': 0.2754650413990021, 'learning_rate': 2.5e-05, 'epoch': 5.0}
{'eval_loss': 0.0678371712565422, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6746221161495625, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6046, 'eval_samples_per_second': 328.584, 'eval_steps_per_second': 41.263, 'epoch': 5.0}
{'loss': 0.0028, 'grad_norm': 0.37110719084739685, 'learning_rate': 2e-05, 'epoch': 6.0}
{'eval_loss': 0.06883254647254944, 'eval_precision': 0.6912225705329154, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7005559968228754, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.5938, 'eval_samples_per_second': 329.354, 'eval_steps_per_second': 41.36, 'epoch': 6.0}
{'loss': 0.0015, 'grad_norm': 0.029887385666370392, 'learning_rate': 1.5e-05, 'epoch': 7.0}
{'eval_loss': 0.06848381459712982, 'eval_precision': 0.6986089644513137, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7129337539432177, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.5941, 'eval_samples_per_second': 329.337, 'eval_steps_per_second': 41.358, 'epoch': 7.0}
{'loss': 0.0005, 'grad_norm': 0.007073550019413233, 'learning_rate': 1e-05, 'epoch': 8.0}
{'eval_loss': 0.07572206854820251, 'eval_precision': 0.7158908507223114, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7170418006430868, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6294, 'eval_samples_per_second': 326.824, 'eval_steps_per_second': 41.042, 'epoch': 8.0}
{'loss': 0.0004, 'grad_norm': 0.006134325172752142, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.07708268612623215, 'eval_precision': 0.7022292993630573, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7061649319455564, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6788, 'eval_samples_per_second': 323.372, 'eval_steps_per_second': 40.609, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.03229478374123573, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.07769123464822769, 'eval_precision': 0.7049441786283892, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7083333333333335, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6267, 'eval_samples_per_second': 327.012, 'eval_steps_per_second': 41.066, 'epoch': 10.0}
{'train_runtime': 806.0526, 'train_samples_per_second': 106.321, 'train_steps_per_second': 3.325, 'train_loss': 0.01447055918484259, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0145
  train_runtime            = 0:13:26.05
  train_samples            =       8570
  train_samples_per_second =    106.321
  train_steps_per_second   =      3.325
[{'loss': 0.0723, 'grad_norm': 0.6361818909645081, 'learning_rate': 4.5e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.048027291893959045, 'eval_precision': 0.7059773828756059, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.7048387096774195, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6294, 'eval_samples_per_second': 326.822, 'eval_steps_per_second': 41.042, 'epoch': 1.0, 'step': 268}, {'loss': 0.0321, 'grad_norm': 0.5500916242599487, 'learning_rate': 4e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.044993218034505844, 'eval_precision': 0.6842105263157895, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6977111286503552, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.7036, 'eval_samples_per_second': 321.668, 'eval_steps_per_second': 40.395, 'epoch': 2.0, 'step': 536}, {'loss': 0.0183, 'grad_norm': 0.43859848380088806, 'learning_rate': 3.5e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04890437424182892, 'eval_precision': 0.7127329192546584, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7256916996047431, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.6107, 'eval_samples_per_second': 328.147, 'eval_steps_per_second': 41.208, 'epoch': 3.0, 'step': 804}, {'loss': 0.0102, 'grad_norm': 0.43421486020088196, 'learning_rate': 3e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05644597113132477, 'eval_precision': 0.6640986132511556, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6787401574803149, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 4.6155, 'eval_samples_per_second': 327.807, 'eval_steps_per_second': 41.165, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0065, 'grad_norm': 0.2754650413990021, 'learning_rate': 2.5e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.0678371712565422, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6746221161495625, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6046, 'eval_samples_per_second': 328.584, 'eval_steps_per_second': 41.263, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0028, 'grad_norm': 0.37110719084739685, 'learning_rate': 2e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06883254647254944, 'eval_precision': 0.6912225705329154, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7005559968228754, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.5938, 'eval_samples_per_second': 329.354, 'eval_steps_per_second': 41.36, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0015, 'grad_norm': 0.029887385666370392, 'learning_rate': 1.5e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.06848381459712982, 'eval_precision': 0.6986089644513137, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7129337539432177, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.5941, 'eval_samples_per_second': 329.337, 'eval_steps_per_second': 41.358, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0005, 'grad_norm': 0.007073550019413233, 'learning_rate': 1e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07572206854820251, 'eval_precision': 0.7158908507223114, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7170418006430868, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 4.6294, 'eval_samples_per_second': 326.824, 'eval_steps_per_second': 41.042, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0004, 'grad_norm': 0.006134325172752142, 'learning_rate': 5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07708268612623215, 'eval_precision': 0.7022292993630573, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7061649319455564, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6788, 'eval_samples_per_second': 323.372, 'eval_steps_per_second': 40.609, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.03229478374123573, 'learning_rate': 0.0, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07769123464822769, 'eval_precision': 0.7049441786283892, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7083333333333335, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6267, 'eval_samples_per_second': 327.012, 'eval_steps_per_second': 41.066, 'epoch': 10.0, 'step': 2680}, {'train_runtime': 806.0526, 'train_samples_per_second': 106.321, 'train_steps_per_second': 3.325, 'total_flos': 9463580842399488.0, 'train_loss': 0.01447055918484259, 'epoch': 10.0, 'step': 2680}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9884
  predict_f1                 =       0.68
  predict_loss               =     0.0417
  predict_precision          =     0.6538
  predict_recall             =     0.7083
  predict_runtime            = 0:00:03.95
  predict_samples_per_second =    316.521
  predict_steps_per_second   =     39.692
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_large_18.json completed. F1: 0.68
/cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_large_08.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03081014_elsa-intensity_NB-BERT_large_08.json
03081014_elsa-intensity_NB-BERT_large Our label2id: {'O': 0, 'B-Negative_Slight': 1, 'I-Negative_Slight': 2, 'B-Negative_Standard': 3, 'I-Negative_Standard': 4, 'B-Neutral': 5, 'I-Neutral': 6, 'B-Positive_Slight': 7, 'I-Positive_Slight': 8, 'B-Positive_Standard': 9, 'I-Positive_Standard': 10}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4684.26 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4516.87 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5501.13 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5451.68 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5978.01 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6441.40 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6635.23 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6927.95 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5927.76 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6829.60 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4158.02 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7283.38 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2539.86 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Could not locate the best model at /cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_large/checkpoint-536/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
03081014_elsa-intensity_NB-BERT_large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0692, 'grad_norm': 0.3452032506465912, 'learning_rate': 4.5e-05, 'epoch': 1.0}
{'eval_loss': 0.04846442490816116, 'eval_precision': 0.6558823529411765, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6856264411990777, 'eval_accuracy': 0.9863211787235706, 'eval_runtime': 4.786, 'eval_samples_per_second': 316.132, 'eval_steps_per_second': 39.699, 'epoch': 1.0}
{'loss': 0.0359, 'grad_norm': 0.0751739889383316, 'learning_rate': 4e-05, 'epoch': 2.0}
{'eval_loss': 0.04971543326973915, 'eval_precision': 0.6308623298033282, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6505460218408736, 'eval_accuracy': 0.9862430140305624, 'eval_runtime': 4.7238, 'eval_samples_per_second': 320.294, 'eval_steps_per_second': 40.222, 'epoch': 2.0}
{'loss': 0.0201, 'grad_norm': 0.010069703683257103, 'learning_rate': 3.5e-05, 'epoch': 3.0}
{'eval_loss': 0.059903334826231, 'eval_precision': 0.6635071090047393, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6698564593301435, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 5.1055, 'eval_samples_per_second': 296.347, 'eval_steps_per_second': 37.215, 'epoch': 3.0}
{'loss': 0.0122, 'grad_norm': 0.8909783363342285, 'learning_rate': 3e-05, 'epoch': 4.0}
{'eval_loss': 0.06114296615123749, 'eval_precision': 0.6724683544303798, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6783719074221867, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 4.926, 'eval_samples_per_second': 307.146, 'eval_steps_per_second': 38.571, 'epoch': 4.0}
{'loss': 0.0072, 'grad_norm': 1.5402097702026367, 'learning_rate': 2.5e-05, 'epoch': 5.0}
{'eval_loss': 0.06766275316476822, 'eval_precision': 0.6740506329113924, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6799680766161215, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 4.761, 'eval_samples_per_second': 317.787, 'eval_steps_per_second': 39.907, 'epoch': 5.0}
{'loss': 0.0036, 'grad_norm': 1.1953495740890503, 'learning_rate': 2e-05, 'epoch': 6.0}
{'eval_loss': 0.08134544640779495, 'eval_precision': 0.6486068111455109, 'eval_recall': 0.6747181964573269, 'eval_f1': 0.6614048934490924, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 4.5982, 'eval_samples_per_second': 329.044, 'eval_steps_per_second': 41.321, 'epoch': 6.0}
{'loss': 0.0017, 'grad_norm': 0.017577502876520157, 'learning_rate': 1.5e-05, 'epoch': 7.0}
{'eval_loss': 0.07894936949014664, 'eval_precision': 0.677570093457944, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6888361045130642, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 4.6041, 'eval_samples_per_second': 328.623, 'eval_steps_per_second': 41.268, 'epoch': 7.0}
{'loss': 0.0004, 'grad_norm': 0.004362443927675486, 'learning_rate': 1e-05, 'epoch': 8.0}
{'eval_loss': 0.08407512307167053, 'eval_precision': 0.6773162939297125, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6800320769847634, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.9573, 'eval_samples_per_second': 305.207, 'eval_steps_per_second': 38.327, 'epoch': 8.0}
{'loss': 0.0001, 'grad_norm': 0.0007082292577251792, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.08678694069385529, 'eval_precision': 0.6816, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6837881219903692, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 4.6006, 'eval_samples_per_second': 328.872, 'eval_steps_per_second': 41.299, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.23249506950378418, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 0.08641546219587326, 'eval_precision': 0.6921850079744817, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6955128205128205, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 4.6056, 'eval_samples_per_second': 328.515, 'eval_steps_per_second': 41.254, 'epoch': 10.0}
{'train_runtime': 1034.4561, 'train_samples_per_second': 82.845, 'train_steps_per_second': 5.181, 'train_loss': 0.015067631638706174, 'epoch': 10.0}
Traceback (most recent call last):
  File "/cluster/work/users/egilron/seq-label_github/seq_label.py", line 276, in <module>
    train_result = trainer.train(resume_from_checkpoint=False)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 2106, in _inner_training_loop
    if not os.path.samefile(checkpoint, self.state.best_model_checkpoint):
  File "/cluster/amd/zen2/software/Python/3.10.8-GCCcore-12.2.0/lib/python3.10/genericpath.py", line 101, in samefile
    s2 = os.stat(f2)
FileNotFoundError: [Errno 2] No such file or directory: '/cluster/work/users/egilron/finetunes/03081014_elsa-intensity_NB-BERT_large/checkpoint-536'

Job 10901752 consumed 4.3 billing hours from project nn9851k.

Submitted 2024-03-08T11:04:45; waited 5.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 4.0 hours
Elapsed wallclock time:   32.5 minutes

Task and CPU statistics:
ID              CPUs  Tasks  CPU util                Start   Elapsed  Exit status
10901752           1            0.0 %  2024-03-08T11:04:50  1949.0 s  1
10901752.batch     1      1    83.6 %  2024-03-08T11:04:50  1949.0 s  1

Used CPU time:   27.2 CPU minutes
Unused CPU time: 5.3 CPU minutes

Memory statistics, in GiB:
ID               Alloc   Usage
10901752          24.0        
10901752.batch    24.0     5.1

GPU usage stats:
Job 10901752 completed at Fri Mar 8 11:37:19 CET 2024
