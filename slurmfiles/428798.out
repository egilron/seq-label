Starting job 428798 on gpu-2 at Mon Feb 12 15:53:04 CET 2024

/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_37_c.json
Python: 3.10.4 (main, Aug 30 2023, 20:05:23) [GCC 11.3.0]
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_37_c.json
01191518_tsa-bin_NorBERT_3_large Our label2id: {'O': 0, 'B-targ-Negative': 1, 'I-targ-Negative': 2, 'B-targ-Positive': 3, 'I-targ-Positive': 4}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:02, 2969.59 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 4581.35 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:01, 5335.62 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 5855.30 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6217.96 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:01<00:00, 6538.22 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 6813.96 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 5982.29 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 5991.46 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 5709.92 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 6473.78 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6297.78 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 5546.17 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 5550.97 examples/s]
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Tokenization completed

Model: ltg/norbert3-large Trust Remote code? True
01191518_tsa-bin_NorBERT_3_large 
Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'] 

Traceback (most recent call last):
  File "/fp/homes01/u01/ec-egilron/sqlabel-github/seq_label_2024.py", line 434, in <module>
    train_result = trainer.train(resume_from_checkpoint=False)
  File "/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
  File "/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2735, in training_step
    loss = self.compute_loss(model, inputs)
  File "/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2758, in compute_loss
    outputs = model(**inputs)
  File "/cluster/software/EL9/easybuild/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/work/projects/ec30/egilron/.cache/modules/transformers_modules/ltg/norbert3-large/4e875b349d7a129e0503a39798112a08004ab84d/modeling_norbert.py", line 521, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
  File "/cluster/work/projects/ec30/egilron/.cache/modules/transformers_modules/ltg/norbert3-large/4e875b349d7a129e0503a39798112a08004ab84d/modeling_norbert.py", line 317, in get_contextualized_embeddings
    contextualized_embeddings = [contextualized_embeddings[0]] + [
  File "/cluster/work/projects/ec30/egilron/.cache/modules/transformers_modules/ltg/norbert3-large/4e875b349d7a129e0503a39798112a08004ab84d/modeling_norbert.py", line 318, in <listcomp>
    contextualized_embeddings[i] - contextualized_embeddings[i - 1]
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 39.39 GiB total capacity; 33.33 GiB already allocated; 3.94 MiB free; 38.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Task and CPU usage stats:
JobID           JobName  AllocCPUS   NTasks     MinCPU MinCPUTask     AveCPU    Elapsed ExitCode 
------------ ---------- ---------- -------- ---------- ---------- ---------- ---------- -------- 
428798       tsa_norbe+          1                                             00:01:32      1:0 
428798.batch      batch          1        1   00:00:45          0   00:00:45   00:01:32      1:0 
428798.exte+     extern          1        1   00:00:00          0   00:00:00   00:01:32      0:0 

Memory usage stats:
JobID            MaxRSS MaxRSSTask     AveRSS MaxPages   MaxPagesTask   AvePages 
------------ ---------- ---------- ---------- -------- -------------- ---------- 
428798                                                                           
428798.batch   2349588K          0   2349588K        0              0          0 
428798.exte+          0          0          0        0              0          0 

Disk usage stats:
JobID         MaxDiskRead MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteTask   AveDiskWrite 
------------ ------------ --------------- -------------- ------------ ---------------- -------------- 
428798                                                                                                
428798.batch     1911.51M               0       1911.51M        9.76M                0          9.76M 
428798.exte+        0.01M               0          0.01M        0.00M                0          0.00M 

GPU usage stats:
Successfully retrieved statistics for job: 428798. 
+------------------------------------------------------------------------------+
| GPU ID: 0                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Mon Feb 12 15:53:04 2024                |
| End Time                           | Mon Feb 12 15:54:35 2024                |
| Total Execution Time (sec)         | 91.44                                   |
| No. of Processes                   | 1                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 4085                                    |
| Power Usage (Watts)                | Avg: 104.365, Max: 244.275, Min: 34.378 |
| Max GPU Memory Used (bytes)        | 15523119104                             |
| SM Clock (MHz)                     | Avg: 1185, Max: 1410, Min: 765          |
| Memory Clock (MHz)                 | Avg: 1215, Max: 1215, Min: 1215         |
| SM Utilization (%)                 | Avg: 27, Max: 74, Min: 0                |
| Memory Utilization (%)             | Avg: 0, Max: 0, Min: 0                  |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 3118910                                 |
|     Avg SM Utilization (%)         | 17                                      |
|     Avg Memory Utilization (%)     | 9                                       |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+


Job 428798 completed at Mon Feb 12 15:54:35 CET 2024
