Starting job 10918080 on gpu-12-8 on saga at Mon Mar 11 18:19:40 CET 2024

/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_01_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_01_101.json
03111215_ner2_nb-bert-large_01_101.json seems to be completed. Exiting
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_06_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_06_101.json
03111215_ner2_norbert3-large_06_101.json seems to be completed. Exiting
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_09_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_09_101.json
03111215_ner2_norbert3-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:07, 3730.47 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:06, 4462.87 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 4522.83 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:05, 5123.79 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:00<00:04, 5852.70 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6496.99 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 6873.00 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:02, 7458.15 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:03, 6404.42 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 6925.95 examples/s]Running tokenizer on train dataset:  37%|███▋      | 11000/29870 [00:01<00:02, 7588.82 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 7814.27 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:02<00:02, 7562.93 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:02, 7695.37 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7513.50 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7643.16 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7724.84 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7649.27 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7423.55 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7448.83 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 7466.49 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 7592.34 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7843.87 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 6397.67 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 6534.53 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 6606.08 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:04<00:00, 6160.65 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6462.70 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6697.92 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6929.34 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6610.92 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 5747.65 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 6890.72 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 5122.48 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 5472.34 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4760.76 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7850.77 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 6032.81 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 6702.37 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:01<00:00, 3416.91 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03111215_ner2_norbert3-large/checkpoint-5604 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03111215_ner2_norbert3-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0213, 'grad_norm': 2.041804313659668, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.01054418832063675, 'eval_precision': 0.9445617740232313, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9428194993412383, 'eval_accuracy': 0.9967908428104527, 'eval_runtime': 16.114, 'eval_samples_per_second': 266.848, 'eval_steps_per_second': 33.387, 'epoch': 1.0}
{'loss': 0.0037, 'grad_norm': 0.49012768268585205, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.010951328091323376, 'eval_precision': 0.9579787234042553, 'eval_recall': 0.9473961073119411, 'eval_f1': 0.9526580269769903, 'eval_accuracy': 0.9973084488087668, 'eval_runtime': 15.8239, 'eval_samples_per_second': 271.74, 'eval_steps_per_second': 33.999, 'epoch': 2.0}
{'loss': 0.0016, 'grad_norm': 0.06573326885700226, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.012541270814836025, 'eval_precision': 0.9483667017913593, 'eval_recall': 0.9468700683850605, 'eval_f1': 0.9476177941563569, 'eval_accuracy': 0.997219716351913, 'eval_runtime': 15.836, 'eval_samples_per_second': 271.534, 'eval_steps_per_second': 33.973, 'epoch': 3.0}
{'loss': 0.0009, 'grad_norm': 0.06374617666006088, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.015888303518295288, 'eval_precision': 0.9470619375330863, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9440633245382586, 'eval_accuracy': 0.9970866176666322, 'eval_runtime': 15.8424, 'eval_samples_per_second': 271.424, 'eval_steps_per_second': 33.96, 'epoch': 4.0}
{'loss': 0.0003, 'grad_norm': 0.038865551352500916, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.015268529765307903, 'eval_precision': 0.9662921348314607, 'eval_recall': 0.9500263019463441, 'eval_f1': 0.9580901856763927, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.841, 'eval_samples_per_second': 271.448, 'eval_steps_per_second': 33.963, 'epoch': 5.0}
{'loss': 0.0003, 'grad_norm': 0.11681029945611954, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.014793023467063904, 'eval_precision': 0.9618239660657476, 'eval_recall': 0.9542346133613887, 'eval_f1': 0.9580142593081595, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.8191, 'eval_samples_per_second': 271.824, 'eval_steps_per_second': 34.01, 'epoch': 6.0}
{'loss': 0.0003, 'grad_norm': 0.016519177705049515, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.015519258566200733, 'eval_precision': 0.960867265996827, 'eval_recall': 0.9558127301420305, 'eval_f1': 0.9583333333333334, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.9226, 'eval_samples_per_second': 270.056, 'eval_steps_per_second': 33.788, 'epoch': 7.0}
{'loss': 0.0003, 'grad_norm': 0.0010256976820528507, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.0152632687240839, 'eval_precision': 0.95, 'eval_recall': 0.9594950026301946, 'eval_f1': 0.9547238942685161, 'eval_accuracy': 0.9974267587512385, 'eval_runtime': 15.704, 'eval_samples_per_second': 273.815, 'eval_steps_per_second': 34.259, 'epoch': 8.0}
{'loss': 0.0001, 'grad_norm': 0.0005881937104277313, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.01602907106280327, 'eval_precision': 0.9555206698063841, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9580272822665267, 'eval_accuracy': 0.9976338011505642, 'eval_runtime': 15.7873, 'eval_samples_per_second': 272.37, 'eval_steps_per_second': 34.078, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.00028473458951339126, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.016610315069556236, 'eval_precision': 0.9519832985386222, 'eval_recall': 0.9594950026301946, 'eval_f1': 0.9557243908828923, 'eval_accuracy': 0.9975007024652834, 'eval_runtime': 15.6458, 'eval_samples_per_second': 274.835, 'eval_steps_per_second': 34.386, 'epoch': 10.0}
{'loss': 0.0, 'grad_norm': 9.766186849446967e-05, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.017097705975174904, 'eval_precision': 0.9510161542470037, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9554973821989529, 'eval_accuracy': 0.9975154912080924, 'eval_runtime': 15.6706, 'eval_samples_per_second': 274.399, 'eval_steps_per_second': 34.332, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 0.0003940385358873755, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.017278505489230156, 'eval_precision': 0.95205836373111, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9565445026178011, 'eval_accuracy': 0.9975302799509014, 'eval_runtime': 15.8019, 'eval_samples_per_second': 272.119, 'eval_steps_per_second': 34.047, 'epoch': 12.0}
{'train_runtime': 3483.0016, 'train_samples_per_second': 102.911, 'train_steps_per_second': 1.609, 'train_loss': 0.0024111125354643785, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0024
  train_runtime            = 0:58:03.00
  train_samples            =      29870
  train_samples_per_second =    102.911
  train_steps_per_second   =      1.609
[{'loss': 0.0213, 'grad_norm': 2.041804313659668, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 467}, {'eval_loss': 0.01054418832063675, 'eval_precision': 0.9445617740232313, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9428194993412383, 'eval_accuracy': 0.9967908428104527, 'eval_runtime': 16.114, 'eval_samples_per_second': 266.848, 'eval_steps_per_second': 33.387, 'epoch': 1.0, 'step': 467}, {'loss': 0.0037, 'grad_norm': 0.49012768268585205, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 934}, {'eval_loss': 0.010951328091323376, 'eval_precision': 0.9579787234042553, 'eval_recall': 0.9473961073119411, 'eval_f1': 0.9526580269769903, 'eval_accuracy': 0.9973084488087668, 'eval_runtime': 15.8239, 'eval_samples_per_second': 271.74, 'eval_steps_per_second': 33.999, 'epoch': 2.0, 'step': 934}, {'loss': 0.0016, 'grad_norm': 0.06573326885700226, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 1401}, {'eval_loss': 0.012541270814836025, 'eval_precision': 0.9483667017913593, 'eval_recall': 0.9468700683850605, 'eval_f1': 0.9476177941563569, 'eval_accuracy': 0.997219716351913, 'eval_runtime': 15.836, 'eval_samples_per_second': 271.534, 'eval_steps_per_second': 33.973, 'epoch': 3.0, 'step': 1401}, {'loss': 0.0009, 'grad_norm': 0.06374617666006088, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1868}, {'eval_loss': 0.015888303518295288, 'eval_precision': 0.9470619375330863, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9440633245382586, 'eval_accuracy': 0.9970866176666322, 'eval_runtime': 15.8424, 'eval_samples_per_second': 271.424, 'eval_steps_per_second': 33.96, 'epoch': 4.0, 'step': 1868}, {'loss': 0.0003, 'grad_norm': 0.038865551352500916, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 2335}, {'eval_loss': 0.015268529765307903, 'eval_precision': 0.9662921348314607, 'eval_recall': 0.9500263019463441, 'eval_f1': 0.9580901856763927, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.841, 'eval_samples_per_second': 271.448, 'eval_steps_per_second': 33.963, 'epoch': 5.0, 'step': 2335}, {'loss': 0.0003, 'grad_norm': 0.11681029945611954, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 2802}, {'eval_loss': 0.014793023467063904, 'eval_precision': 0.9618239660657476, 'eval_recall': 0.9542346133613887, 'eval_f1': 0.9580142593081595, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.8191, 'eval_samples_per_second': 271.824, 'eval_steps_per_second': 34.01, 'epoch': 6.0, 'step': 2802}, {'loss': 0.0003, 'grad_norm': 0.016519177705049515, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 3269}, {'eval_loss': 0.015519258566200733, 'eval_precision': 0.960867265996827, 'eval_recall': 0.9558127301420305, 'eval_f1': 0.9583333333333334, 'eval_accuracy': 0.9976781673789911, 'eval_runtime': 15.9226, 'eval_samples_per_second': 270.056, 'eval_steps_per_second': 33.788, 'epoch': 7.0, 'step': 3269}, {'loss': 0.0003, 'grad_norm': 0.0010256976820528507, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 3736}, {'eval_loss': 0.0152632687240839, 'eval_precision': 0.95, 'eval_recall': 0.9594950026301946, 'eval_f1': 0.9547238942685161, 'eval_accuracy': 0.9974267587512385, 'eval_runtime': 15.704, 'eval_samples_per_second': 273.815, 'eval_steps_per_second': 34.259, 'epoch': 8.0, 'step': 3736}, {'loss': 0.0001, 'grad_norm': 0.0005881937104277313, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 4203}, {'eval_loss': 0.01602907106280327, 'eval_precision': 0.9555206698063841, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9580272822665267, 'eval_accuracy': 0.9976338011505642, 'eval_runtime': 15.7873, 'eval_samples_per_second': 272.37, 'eval_steps_per_second': 34.078, 'epoch': 9.0, 'step': 4203}, {'loss': 0.0001, 'grad_norm': 0.00028473458951339126, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 4670}, {'eval_loss': 0.016610315069556236, 'eval_precision': 0.9519832985386222, 'eval_recall': 0.9594950026301946, 'eval_f1': 0.9557243908828923, 'eval_accuracy': 0.9975007024652834, 'eval_runtime': 15.6458, 'eval_samples_per_second': 274.835, 'eval_steps_per_second': 34.386, 'epoch': 10.0, 'step': 4670}, {'loss': 0.0, 'grad_norm': 9.766186849446967e-05, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 5137}, {'eval_loss': 0.017097705975174904, 'eval_precision': 0.9510161542470037, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9554973821989529, 'eval_accuracy': 0.9975154912080924, 'eval_runtime': 15.6706, 'eval_samples_per_second': 274.399, 'eval_steps_per_second': 34.332, 'epoch': 11.0, 'step': 5137}, {'loss': 0.0, 'grad_norm': 0.0003940385358873755, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 5604}, {'eval_loss': 0.017278505489230156, 'eval_precision': 0.95205836373111, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9565445026178011, 'eval_accuracy': 0.9975302799509014, 'eval_runtime': 15.8019, 'eval_samples_per_second': 272.119, 'eval_steps_per_second': 34.047, 'epoch': 12.0, 'step': 5604}, {'train_runtime': 3483.0016, 'train_samples_per_second': 102.911, 'train_steps_per_second': 1.609, 'total_flos': 3.966332167288483e+16, 'train_loss': 0.0024111125354643785, 'epoch': 12.0, 'step': 5604}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9963
  predict_f1                 =     0.9391
  predict_loss               =     0.0125
  predict_precision          =     0.9385
  predict_recall             =     0.9397
  predict_runtime            = 0:00:12.68
  predict_samples_per_second =    271.923
  predict_steps_per_second   =     34.049
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_09_101.json completed. F1: 0.939119170984456
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert_base_11_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert_base_11_101.json
03111215_ner2_nb-bert_base Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:09, 2970.59 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:06, 4064.25 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 4701.03 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:04, 5490.12 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:00<00:04, 6102.04 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6713.66 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 7000.44 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:02, 7545.71 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:02, 8028.68 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:03, 6582.77 examples/s]Running tokenizer on train dataset:  37%|███▋      | 11000/29870 [00:01<00:02, 7306.40 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 7596.11 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:02<00:02, 7207.14 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:02, 7352.62 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:02, 7178.12 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7400.70 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7546.26 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7504.57 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7263.37 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7300.09 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 7298.95 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 7387.16 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7577.26 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 6218.70 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 6340.31 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 6397.39 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:04<00:00, 5900.32 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6172.63 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6408.41 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6646.87 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6625.15 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 3608.69 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 3844.76 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 4795.61 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 4687.87 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4567.91 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 4952.99 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 5557.62 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 5393.42 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 4326.84 examples/s]
03111215_ner2_nb-bert_base Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0241, 'grad_norm': 0.36932244896888733, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.008774472400546074, 'eval_precision': 0.9579115610015982, 'eval_recall': 0.9458179905312993, 'eval_f1': 0.9518263631551085, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 6.7844, 'eval_samples_per_second': 633.804, 'eval_steps_per_second': 79.299, 'epoch': 1.0}
{'loss': 0.0055, 'grad_norm': 0.26061728596687317, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.011365535669028759, 'eval_precision': 0.9658351409978309, 'eval_recall': 0.9368753287743293, 'eval_f1': 0.9511348464619493, 'eval_accuracy': 0.9970866176666322, 'eval_runtime': 6.5794, 'eval_samples_per_second': 653.552, 'eval_steps_per_second': 81.77, 'epoch': 2.0}
{'loss': 0.0032, 'grad_norm': 0.24892355501651764, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.011132162995636463, 'eval_precision': 0.9449922158796056, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9514106583072099, 'eval_accuracy': 0.9972788713231489, 'eval_runtime': 6.4723, 'eval_samples_per_second': 664.371, 'eval_steps_per_second': 83.124, 'epoch': 3.0}
{'loss': 0.0018, 'grad_norm': 1.2134284973144531, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.013186556287109852, 'eval_precision': 0.9527806925498427, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9540320462306278, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 6.6908, 'eval_samples_per_second': 642.677, 'eval_steps_per_second': 80.409, 'epoch': 4.0}
{'loss': 0.0011, 'grad_norm': 0.0012037074193358421, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.015170066617429256, 'eval_precision': 0.943698347107438, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9523064894448788, 'eval_accuracy': 0.997130983895059, 'eval_runtime': 6.5259, 'eval_samples_per_second': 658.915, 'eval_steps_per_second': 82.441, 'epoch': 5.0}
{'loss': 0.0009, 'grad_norm': 0.013905497267842293, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.015998894348740578, 'eval_precision': 0.9566137566137566, 'eval_recall': 0.9510783798001052, 'eval_f1': 0.9538380374571352, 'eval_accuracy': 0.9972345050947219, 'eval_runtime': 6.5509, 'eval_samples_per_second': 656.402, 'eval_steps_per_second': 82.127, 'epoch': 6.0}
{'loss': 0.0007, 'grad_norm': 0.07888499647378922, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.015167326666414738, 'eval_precision': 0.9485446985446986, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9542483660130718, 'eval_accuracy': 0.9971457726378681, 'eval_runtime': 6.6341, 'eval_samples_per_second': 648.166, 'eval_steps_per_second': 81.096, 'epoch': 7.0}
{'loss': 0.0005, 'grad_norm': 0.00774005614221096, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.016408996656537056, 'eval_precision': 0.950733752620545, 'eval_recall': 0.9542346133613887, 'eval_f1': 0.9524809661328432, 'eval_accuracy': 0.997175350123486, 'eval_runtime': 6.4976, 'eval_samples_per_second': 661.788, 'eval_steps_per_second': 82.8, 'epoch': 8.0}
{'loss': 0.0004, 'grad_norm': 0.0007292840746231377, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.016344167292118073, 'eval_precision': 0.9591944886062533, 'eval_recall': 0.9521304576538664, 'eval_f1': 0.9556494192185851, 'eval_accuracy': 0.9973528150371936, 'eval_runtime': 6.4796, 'eval_samples_per_second': 663.622, 'eval_steps_per_second': 83.03, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.0006909280782565475, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.01748760975897312, 'eval_precision': 0.9564075630252101, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9571616294349541, 'eval_accuracy': 0.9974119700084296, 'eval_runtime': 6.5076, 'eval_samples_per_second': 660.769, 'eval_steps_per_second': 82.673, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.00036948491469956934, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.018261296674609184, 'eval_precision': 0.9549973835688121, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9575026232948585, 'eval_accuracy': 0.9974267587512385, 'eval_runtime': 6.5921, 'eval_samples_per_second': 652.292, 'eval_steps_per_second': 81.612, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 0.0002603334141895175, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.01851368322968483, 'eval_precision': 0.951092611862643, 'eval_recall': 0.961599158337717, 'eval_f1': 0.9563170285116401, 'eval_accuracy': 0.9973676037800027, 'eval_runtime': 6.4719, 'eval_samples_per_second': 664.408, 'eval_steps_per_second': 83.128, 'epoch': 12.0}
{'train_runtime': 1207.6841, 'train_samples_per_second': 296.799, 'train_steps_per_second': 4.64, 'train_loss': 0.0032001839778509037, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0032
  train_runtime            = 0:20:07.68
  train_samples            =      29870
  train_samples_per_second =    296.799
  train_steps_per_second   =       4.64
[{'loss': 0.0241, 'grad_norm': 0.36932244896888733, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 467}, {'eval_loss': 0.008774472400546074, 'eval_precision': 0.9579115610015982, 'eval_recall': 0.9458179905312993, 'eval_f1': 0.9518263631551085, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 6.7844, 'eval_samples_per_second': 633.804, 'eval_steps_per_second': 79.299, 'epoch': 1.0, 'step': 467}, {'loss': 0.0055, 'grad_norm': 0.26061728596687317, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 934}, {'eval_loss': 0.011365535669028759, 'eval_precision': 0.9658351409978309, 'eval_recall': 0.9368753287743293, 'eval_f1': 0.9511348464619493, 'eval_accuracy': 0.9970866176666322, 'eval_runtime': 6.5794, 'eval_samples_per_second': 653.552, 'eval_steps_per_second': 81.77, 'epoch': 2.0, 'step': 934}, {'loss': 0.0032, 'grad_norm': 0.24892355501651764, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 1401}, {'eval_loss': 0.011132162995636463, 'eval_precision': 0.9449922158796056, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9514106583072099, 'eval_accuracy': 0.9972788713231489, 'eval_runtime': 6.4723, 'eval_samples_per_second': 664.371, 'eval_steps_per_second': 83.124, 'epoch': 3.0, 'step': 1401}, {'loss': 0.0018, 'grad_norm': 1.2134284973144531, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1868}, {'eval_loss': 0.013186556287109852, 'eval_precision': 0.9527806925498427, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9540320462306278, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 6.6908, 'eval_samples_per_second': 642.677, 'eval_steps_per_second': 80.409, 'epoch': 4.0, 'step': 1868}, {'loss': 0.0011, 'grad_norm': 0.0012037074193358421, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 2335}, {'eval_loss': 0.015170066617429256, 'eval_precision': 0.943698347107438, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9523064894448788, 'eval_accuracy': 0.997130983895059, 'eval_runtime': 6.5259, 'eval_samples_per_second': 658.915, 'eval_steps_per_second': 82.441, 'epoch': 5.0, 'step': 2335}, {'loss': 0.0009, 'grad_norm': 0.013905497267842293, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 2802}, {'eval_loss': 0.015998894348740578, 'eval_precision': 0.9566137566137566, 'eval_recall': 0.9510783798001052, 'eval_f1': 0.9538380374571352, 'eval_accuracy': 0.9972345050947219, 'eval_runtime': 6.5509, 'eval_samples_per_second': 656.402, 'eval_steps_per_second': 82.127, 'epoch': 6.0, 'step': 2802}, {'loss': 0.0007, 'grad_norm': 0.07888499647378922, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 3269}, {'eval_loss': 0.015167326666414738, 'eval_precision': 0.9485446985446986, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9542483660130718, 'eval_accuracy': 0.9971457726378681, 'eval_runtime': 6.6341, 'eval_samples_per_second': 648.166, 'eval_steps_per_second': 81.096, 'epoch': 7.0, 'step': 3269}, {'loss': 0.0005, 'grad_norm': 0.00774005614221096, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 3736}, {'eval_loss': 0.016408996656537056, 'eval_precision': 0.950733752620545, 'eval_recall': 0.9542346133613887, 'eval_f1': 0.9524809661328432, 'eval_accuracy': 0.997175350123486, 'eval_runtime': 6.4976, 'eval_samples_per_second': 661.788, 'eval_steps_per_second': 82.8, 'epoch': 8.0, 'step': 3736}, {'loss': 0.0004, 'grad_norm': 0.0007292840746231377, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 4203}, {'eval_loss': 0.016344167292118073, 'eval_precision': 0.9591944886062533, 'eval_recall': 0.9521304576538664, 'eval_f1': 0.9556494192185851, 'eval_accuracy': 0.9973528150371936, 'eval_runtime': 6.4796, 'eval_samples_per_second': 663.622, 'eval_steps_per_second': 83.03, 'epoch': 9.0, 'step': 4203}, {'loss': 0.0001, 'grad_norm': 0.0006909280782565475, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 4670}, {'eval_loss': 0.01748760975897312, 'eval_precision': 0.9564075630252101, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9571616294349541, 'eval_accuracy': 0.9974119700084296, 'eval_runtime': 6.5076, 'eval_samples_per_second': 660.769, 'eval_steps_per_second': 82.673, 'epoch': 10.0, 'step': 4670}, {'loss': 0.0001, 'grad_norm': 0.00036948491469956934, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 5137}, {'eval_loss': 0.018261296674609184, 'eval_precision': 0.9549973835688121, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.9575026232948585, 'eval_accuracy': 0.9974267587512385, 'eval_runtime': 6.5921, 'eval_samples_per_second': 652.292, 'eval_steps_per_second': 81.612, 'epoch': 11.0, 'step': 5137}, {'loss': 0.0, 'grad_norm': 0.0002603334141895175, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 5604}, {'eval_loss': 0.01851368322968483, 'eval_precision': 0.951092611862643, 'eval_recall': 0.961599158337717, 'eval_f1': 0.9563170285116401, 'eval_accuracy': 0.9973676037800027, 'eval_runtime': 6.4719, 'eval_samples_per_second': 664.408, 'eval_steps_per_second': 83.128, 'epoch': 12.0, 'step': 5604}, {'train_runtime': 1207.6841, 'train_samples_per_second': 296.799, 'train_steps_per_second': 4.64, 'total_flos': 1.437432751179282e+16, 'train_loss': 0.0032001839778509037, 'epoch': 12.0, 'step': 5604}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9958
  predict_f1                 =     0.9312
  predict_loss               =     0.0131
  predict_precision          =     0.9235
  predict_recall             =     0.9391
  predict_runtime            = 0:00:05.33
  predict_samples_per_second =    646.792
  predict_steps_per_second   =      80.99
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert_base_11_101.json completed. F1: 0.9312339331619538
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_10_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_10_101.json
03111215_ner2_nb-bert-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:04, 5951.94 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:06, 4172.96 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 5053.57 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:04, 5642.42 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:00<00:03, 6262.92 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:00<00:03, 6865.80 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 7131.40 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:03, 6248.20 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:02, 6981.22 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 7430.77 examples/s]Running tokenizer on train dataset:  37%|███▋      | 11000/29870 [00:01<00:02, 8040.25 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 8076.46 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:01<00:02, 7625.52 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:02, 7732.10 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7495.85 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7684.81 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7779.07 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7681.41 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7438.40 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7437.82 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 6120.15 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 6530.49 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7010.49 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 7191.09 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 7051.65 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 6926.26 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:03<00:00, 6251.30 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6505.45 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6699.69 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6899.27 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6753.55 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 7543.89 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 7724.13 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 6993.58 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 6830.76 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 5905.25 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 6164.50 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 3379.28 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 4549.07 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 3737.24 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03111215_ner2_nb-bert-large/checkpoint-467 already exists and is non-empty. Saving will proceed but saved results may be invalid.
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03111215_ner2_nb-bert-large/checkpoint-934 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03111215_ner2_nb-bert-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0237, 'grad_norm': 0.16446520388126373, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.010711414739489555, 'eval_precision': 0.9440928270042194, 'eval_recall': 0.9416096791162546, 'eval_f1': 0.942849618119568, 'eval_accuracy': 0.9965690116683181, 'eval_runtime': 12.3129, 'eval_samples_per_second': 349.226, 'eval_steps_per_second': 43.694, 'epoch': 1.0}
{'loss': 0.007, 'grad_norm': 0.4320718050003052, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.013857620768249035, 'eval_precision': 0.9371398638030383, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9391076115485565, 'eval_accuracy': 0.9963767580118014, 'eval_runtime': 12.2796, 'eval_samples_per_second': 350.175, 'eval_steps_per_second': 43.813, 'epoch': 2.0}
{'loss': 0.0033, 'grad_norm': 0.17401820421218872, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.015533668920397758, 'eval_precision': 0.9418297197250132, 'eval_recall': 0.9368753287743293, 'eval_f1': 0.9393459915611815, 'eval_accuracy': 0.9965985891539361, 'eval_runtime': 12.2432, 'eval_samples_per_second': 351.214, 'eval_steps_per_second': 43.943, 'epoch': 3.0}
{'loss': 0.0017, 'grad_norm': 0.36530032753944397, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.01876720041036606, 'eval_precision': 0.9366492146596859, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9388611912883758, 'eval_accuracy': 0.9964654904686553, 'eval_runtime': 12.2533, 'eval_samples_per_second': 350.925, 'eval_steps_per_second': 43.906, 'epoch': 4.0}
{'loss': 0.0013, 'grad_norm': 1.2335697412490845, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.019124574959278107, 'eval_precision': 0.9497326203208556, 'eval_recall': 0.9342451341399264, 'eval_f1': 0.9419252187748608, 'eval_accuracy': 0.9966873216107899, 'eval_runtime': 12.3628, 'eval_samples_per_second': 347.817, 'eval_steps_per_second': 43.518, 'epoch': 5.0}
{'loss': 0.001, 'grad_norm': 0.0013674660585820675, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.018611066043376923, 'eval_precision': 0.9561965811965812, 'eval_recall': 0.9416096791162546, 'eval_f1': 0.948847071296051, 'eval_accuracy': 0.9968795752673065, 'eval_runtime': 12.211, 'eval_samples_per_second': 352.142, 'eval_steps_per_second': 44.059, 'epoch': 6.0}
{'loss': 0.0007, 'grad_norm': 0.011949797160923481, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.01652977243065834, 'eval_precision': 0.9493136219640972, 'eval_recall': 0.9458179905312993, 'eval_f1': 0.9475625823451911, 'eval_accuracy': 0.9967760540676437, 'eval_runtime': 12.2684, 'eval_samples_per_second': 350.494, 'eval_steps_per_second': 43.852, 'epoch': 7.0}
{'loss': 0.0004, 'grad_norm': 0.0014810295542702079, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.017226293683052063, 'eval_precision': 0.9534391534391534, 'eval_recall': 0.9479221462388217, 'eval_f1': 0.9506726457399104, 'eval_accuracy': 0.9969535189813514, 'eval_runtime': 12.2554, 'eval_samples_per_second': 350.865, 'eval_steps_per_second': 43.899, 'epoch': 8.0}
{'loss': 0.0002, 'grad_norm': 0.0023329835385084152, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.021465808153152466, 'eval_precision': 0.9367283950617284, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9472041612483745, 'eval_accuracy': 0.9967760540676437, 'eval_runtime': 12.2754, 'eval_samples_per_second': 350.294, 'eval_steps_per_second': 43.827, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.00012580555630847812, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.022386584430933, 'eval_precision': 0.954230973922299, 'eval_recall': 0.9431877958968964, 'eval_f1': 0.9486772486772486, 'eval_accuracy': 0.9968795752673065, 'eval_runtime': 12.2592, 'eval_samples_per_second': 350.758, 'eval_steps_per_second': 43.885, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.0005063595017418265, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.022102568298578262, 'eval_precision': 0.9459317585301837, 'eval_recall': 0.9479221462388217, 'eval_f1': 0.9469259064634787, 'eval_accuracy': 0.9967908428104527, 'eval_runtime': 12.2462, 'eval_samples_per_second': 351.13, 'eval_steps_per_second': 43.932, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 3.748609378817491e-05, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.022555619478225708, 'eval_precision': 0.9456351280710925, 'eval_recall': 0.9516044187269858, 'eval_f1': 0.9486103828002097, 'eval_accuracy': 0.9968647865244975, 'eval_runtime': 12.3162, 'eval_samples_per_second': 349.133, 'eval_steps_per_second': 43.682, 'epoch': 12.0}
{'train_runtime': 2860.4716, 'train_samples_per_second': 125.308, 'train_steps_per_second': 1.959, 'train_loss': 0.0032941225213500578, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0033
  train_runtime            = 0:47:40.47
  train_samples            =      29870
  train_samples_per_second =    125.308
  train_steps_per_second   =      1.959
[{'loss': 0.0237, 'grad_norm': 0.16446520388126373, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 467}, {'eval_loss': 0.010711414739489555, 'eval_precision': 0.9440928270042194, 'eval_recall': 0.9416096791162546, 'eval_f1': 0.942849618119568, 'eval_accuracy': 0.9965690116683181, 'eval_runtime': 12.3129, 'eval_samples_per_second': 349.226, 'eval_steps_per_second': 43.694, 'epoch': 1.0, 'step': 467}, {'loss': 0.007, 'grad_norm': 0.4320718050003052, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 934}, {'eval_loss': 0.013857620768249035, 'eval_precision': 0.9371398638030383, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9391076115485565, 'eval_accuracy': 0.9963767580118014, 'eval_runtime': 12.2796, 'eval_samples_per_second': 350.175, 'eval_steps_per_second': 43.813, 'epoch': 2.0, 'step': 934}, {'loss': 0.0033, 'grad_norm': 0.17401820421218872, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 1401}, {'eval_loss': 0.015533668920397758, 'eval_precision': 0.9418297197250132, 'eval_recall': 0.9368753287743293, 'eval_f1': 0.9393459915611815, 'eval_accuracy': 0.9965985891539361, 'eval_runtime': 12.2432, 'eval_samples_per_second': 351.214, 'eval_steps_per_second': 43.943, 'epoch': 3.0, 'step': 1401}, {'loss': 0.0017, 'grad_norm': 0.36530032753944397, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1868}, {'eval_loss': 0.01876720041036606, 'eval_precision': 0.9366492146596859, 'eval_recall': 0.941083640189374, 'eval_f1': 0.9388611912883758, 'eval_accuracy': 0.9964654904686553, 'eval_runtime': 12.2533, 'eval_samples_per_second': 350.925, 'eval_steps_per_second': 43.906, 'epoch': 4.0, 'step': 1868}, {'loss': 0.0013, 'grad_norm': 1.2335697412490845, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 2335}, {'eval_loss': 0.019124574959278107, 'eval_precision': 0.9497326203208556, 'eval_recall': 0.9342451341399264, 'eval_f1': 0.9419252187748608, 'eval_accuracy': 0.9966873216107899, 'eval_runtime': 12.3628, 'eval_samples_per_second': 347.817, 'eval_steps_per_second': 43.518, 'epoch': 5.0, 'step': 2335}, {'loss': 0.001, 'grad_norm': 0.0013674660585820675, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 2802}, {'eval_loss': 0.018611066043376923, 'eval_precision': 0.9561965811965812, 'eval_recall': 0.9416096791162546, 'eval_f1': 0.948847071296051, 'eval_accuracy': 0.9968795752673065, 'eval_runtime': 12.211, 'eval_samples_per_second': 352.142, 'eval_steps_per_second': 44.059, 'epoch': 6.0, 'step': 2802}, {'loss': 0.0007, 'grad_norm': 0.011949797160923481, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 3269}, {'eval_loss': 0.01652977243065834, 'eval_precision': 0.9493136219640972, 'eval_recall': 0.9458179905312993, 'eval_f1': 0.9475625823451911, 'eval_accuracy': 0.9967760540676437, 'eval_runtime': 12.2684, 'eval_samples_per_second': 350.494, 'eval_steps_per_second': 43.852, 'epoch': 7.0, 'step': 3269}, {'loss': 0.0004, 'grad_norm': 0.0014810295542702079, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 3736}, {'eval_loss': 0.017226293683052063, 'eval_precision': 0.9534391534391534, 'eval_recall': 0.9479221462388217, 'eval_f1': 0.9506726457399104, 'eval_accuracy': 0.9969535189813514, 'eval_runtime': 12.2554, 'eval_samples_per_second': 350.865, 'eval_steps_per_second': 43.899, 'epoch': 8.0, 'step': 3736}, {'loss': 0.0002, 'grad_norm': 0.0023329835385084152, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 4203}, {'eval_loss': 0.021465808153152466, 'eval_precision': 0.9367283950617284, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9472041612483745, 'eval_accuracy': 0.9967760540676437, 'eval_runtime': 12.2754, 'eval_samples_per_second': 350.294, 'eval_steps_per_second': 43.827, 'epoch': 9.0, 'step': 4203}, {'loss': 0.0001, 'grad_norm': 0.00012580555630847812, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 4670}, {'eval_loss': 0.022386584430933, 'eval_precision': 0.954230973922299, 'eval_recall': 0.9431877958968964, 'eval_f1': 0.9486772486772486, 'eval_accuracy': 0.9968795752673065, 'eval_runtime': 12.2592, 'eval_samples_per_second': 350.758, 'eval_steps_per_second': 43.885, 'epoch': 10.0, 'step': 4670}, {'loss': 0.0001, 'grad_norm': 0.0005063595017418265, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 5137}, {'eval_loss': 0.022102568298578262, 'eval_precision': 0.9459317585301837, 'eval_recall': 0.9479221462388217, 'eval_f1': 0.9469259064634787, 'eval_accuracy': 0.9967908428104527, 'eval_runtime': 12.2462, 'eval_samples_per_second': 351.13, 'eval_steps_per_second': 43.932, 'epoch': 11.0, 'step': 5137}, {'loss': 0.0, 'grad_norm': 3.748609378817491e-05, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 5604}, {'eval_loss': 0.022555619478225708, 'eval_precision': 0.9456351280710925, 'eval_recall': 0.9516044187269858, 'eval_f1': 0.9486103828002097, 'eval_accuracy': 0.9968647865244975, 'eval_runtime': 12.3162, 'eval_samples_per_second': 349.133, 'eval_steps_per_second': 43.682, 'epoch': 12.0, 'step': 5604}, {'train_runtime': 2860.4716, 'train_samples_per_second': 125.308, 'train_steps_per_second': 1.959, 'total_flos': 3.890390621534781e+16, 'train_loss': 0.0032941225213500578, 'epoch': 12.0, 'step': 5604}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9956
  predict_f1                 =     0.9353
  predict_loss               =     0.0142
  predict_precision          =     0.9335
  predict_recall             =     0.9371
  predict_runtime            = 0:00:09.87
  predict_samples_per_second =    349.473
  predict_steps_per_second   =      43.76
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_10_101.json completed. F1: 0.9353169469598965
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_00_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_00_101.json
03111215_ner2_norbert3-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:10, 2800.04 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:06, 4083.16 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 5066.99 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:04, 5525.43 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:00<00:04, 6160.06 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6778.10 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 7060.50 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:02, 7640.99 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:03, 6502.63 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 7035.33 examples/s]Running tokenizer on train dataset:  37%|███▋      | 11000/29870 [00:01<00:02, 7682.45 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 7887.36 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:01<00:02, 7572.44 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:02, 7684.15 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7537.57 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7671.33 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7753.56 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7654.79 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7434.39 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7435.41 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 7443.19 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 7574.06 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7832.91 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 6356.50 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 6422.61 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 6521.56 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:04<00:00, 6088.75 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6376.72 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6633.61 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6878.82 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6603.27 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 7622.10 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 7765.66 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 4419.87 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 4398.13 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4402.21 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7799.95 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 6558.64 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 5997.51 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 4347.65 examples/s]
03111215_ner2_norbert3-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0263, 'grad_norm': 4.578107833862305, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.008136721327900887, 'eval_precision': 0.957345971563981, 'eval_recall': 0.9563387690689111, 'eval_f1': 0.9568421052631578, 'eval_accuracy': 0.9975598574365193, 'eval_runtime': 15.6878, 'eval_samples_per_second': 274.098, 'eval_steps_per_second': 34.294, 'epoch': 1.0}
{'loss': 0.0041, 'grad_norm': 0.15152466297149658, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.009994658641517162, 'eval_precision': 0.9623741388447271, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9588173178458291, 'eval_accuracy': 0.9975302799509014, 'eval_runtime': 15.6706, 'eval_samples_per_second': 274.4, 'eval_steps_per_second': 34.332, 'epoch': 2.0}
{'loss': 0.0018, 'grad_norm': 0.003331070765852928, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.009985742159187794, 'eval_precision': 0.9625922023182297, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9618320610687022, 'eval_accuracy': 0.9977816885786539, 'eval_runtime': 15.8312, 'eval_samples_per_second': 271.615, 'eval_steps_per_second': 33.983, 'epoch': 3.0}
{'loss': 0.0008, 'grad_norm': 0.04355119168758392, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.014684975147247314, 'eval_precision': 0.9571577847439916, 'eval_recall': 0.9637033140452393, 'eval_f1': 0.9604193971166448, 'eval_accuracy': 0.9977077448646091, 'eval_runtime': 15.3848, 'eval_samples_per_second': 279.496, 'eval_steps_per_second': 34.969, 'epoch': 4.0}
{'loss': 0.0005, 'grad_norm': 0.004249158315360546, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.015272160060703754, 'eval_precision': 0.9572916666666667, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9620518188955771, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.3635, 'eval_samples_per_second': 279.883, 'eval_steps_per_second': 35.018, 'epoch': 5.0}
{'loss': 0.0003, 'grad_norm': 0.0017071983311325312, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.01626354455947876, 'eval_precision': 0.9620453347390617, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.961032122169563, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.455, 'eval_samples_per_second': 278.227, 'eval_steps_per_second': 34.811, 'epoch': 6.0}
{'loss': 0.0002, 'grad_norm': 0.0009683655225671828, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.017826933413743973, 'eval_precision': 0.9580272822665268, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9592855266614132, 'eval_accuracy': 0.9975894349221373, 'eval_runtime': 15.0357, 'eval_samples_per_second': 285.986, 'eval_steps_per_second': 35.782, 'epoch': 7.0}
{'loss': 0.0001, 'grad_norm': 0.02695998176932335, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.018776781857013702, 'eval_precision': 0.9560439560439561, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9585519412381953, 'eval_accuracy': 0.9975894349221373, 'eval_runtime': 15.0775, 'eval_samples_per_second': 285.193, 'eval_steps_per_second': 35.682, 'epoch': 8.0}
{'loss': 0.0001, 'grad_norm': 0.0006836227257736027, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.019410070031881332, 'eval_precision': 0.9624933967247755, 'eval_recall': 0.9584429247764334, 'eval_f1': 0.9604638903531892, 'eval_accuracy': 0.9976338011505642, 'eval_runtime': 15.0419, 'eval_samples_per_second': 285.868, 'eval_steps_per_second': 35.767, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.0007204046123661101, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.019519459456205368, 'eval_precision': 0.9567483064095883, 'eval_recall': 0.9658074697527617, 'eval_f1': 0.9612565445026178, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.1316, 'eval_samples_per_second': 284.174, 'eval_steps_per_second': 35.555, 'epoch': 10.0}
{'loss': 0.0, 'grad_norm': 0.00014988773909863085, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.020969128236174583, 'eval_precision': 0.9552549427679501, 'eval_recall': 0.9658074697527617, 'eval_f1': 0.9605022233847763, 'eval_accuracy': 0.9975746461793283, 'eval_runtime': 15.0546, 'eval_samples_per_second': 285.627, 'eval_steps_per_second': 35.737, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 3.832936272374354e-05, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.02115016058087349, 'eval_precision': 0.956725755995829, 'eval_recall': 0.9652814308258811, 'eval_f1': 0.9609845509295627, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 15.0718, 'eval_samples_per_second': 285.301, 'eval_steps_per_second': 35.696, 'epoch': 12.0}
{'train_runtime': 3374.1154, 'train_samples_per_second': 106.232, 'train_steps_per_second': 3.322, 'train_loss': 0.002875326964957673, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0029
  train_runtime            = 0:56:14.11
  train_samples            =      29870
  train_samples_per_second =    106.232
  train_steps_per_second   =      3.322
[{'loss': 0.0263, 'grad_norm': 4.578107833862305, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 934}, {'eval_loss': 0.008136721327900887, 'eval_precision': 0.957345971563981, 'eval_recall': 0.9563387690689111, 'eval_f1': 0.9568421052631578, 'eval_accuracy': 0.9975598574365193, 'eval_runtime': 15.6878, 'eval_samples_per_second': 274.098, 'eval_steps_per_second': 34.294, 'epoch': 1.0, 'step': 934}, {'loss': 0.0041, 'grad_norm': 0.15152466297149658, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 1868}, {'eval_loss': 0.009994658641517162, 'eval_precision': 0.9623741388447271, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9588173178458291, 'eval_accuracy': 0.9975302799509014, 'eval_runtime': 15.6706, 'eval_samples_per_second': 274.4, 'eval_steps_per_second': 34.332, 'epoch': 2.0, 'step': 1868}, {'loss': 0.0018, 'grad_norm': 0.003331070765852928, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 2802}, {'eval_loss': 0.009985742159187794, 'eval_precision': 0.9625922023182297, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9618320610687022, 'eval_accuracy': 0.9977816885786539, 'eval_runtime': 15.8312, 'eval_samples_per_second': 271.615, 'eval_steps_per_second': 33.983, 'epoch': 3.0, 'step': 2802}, {'loss': 0.0008, 'grad_norm': 0.04355119168758392, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 3736}, {'eval_loss': 0.014684975147247314, 'eval_precision': 0.9571577847439916, 'eval_recall': 0.9637033140452393, 'eval_f1': 0.9604193971166448, 'eval_accuracy': 0.9977077448646091, 'eval_runtime': 15.3848, 'eval_samples_per_second': 279.496, 'eval_steps_per_second': 34.969, 'epoch': 4.0, 'step': 3736}, {'loss': 0.0005, 'grad_norm': 0.004249158315360546, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 4670}, {'eval_loss': 0.015272160060703754, 'eval_precision': 0.9572916666666667, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9620518188955771, 'eval_accuracy': 0.997737322350227, 'eval_runtime': 15.3635, 'eval_samples_per_second': 279.883, 'eval_steps_per_second': 35.018, 'epoch': 5.0, 'step': 4670}, {'loss': 0.0003, 'grad_norm': 0.0017071983311325312, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 5604}, {'eval_loss': 0.01626354455947876, 'eval_precision': 0.9620453347390617, 'eval_recall': 0.9600210415570752, 'eval_f1': 0.961032122169563, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.455, 'eval_samples_per_second': 278.227, 'eval_steps_per_second': 34.811, 'epoch': 6.0, 'step': 5604}, {'loss': 0.0002, 'grad_norm': 0.0009683655225671828, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 6538}, {'eval_loss': 0.017826933413743973, 'eval_precision': 0.9580272822665268, 'eval_recall': 0.9605470804839558, 'eval_f1': 0.9592855266614132, 'eval_accuracy': 0.9975894349221373, 'eval_runtime': 15.0357, 'eval_samples_per_second': 285.986, 'eval_steps_per_second': 35.782, 'epoch': 7.0, 'step': 6538}, {'loss': 0.0001, 'grad_norm': 0.02695998176932335, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 7472}, {'eval_loss': 0.018776781857013702, 'eval_precision': 0.9560439560439561, 'eval_recall': 0.9610731194108364, 'eval_f1': 0.9585519412381953, 'eval_accuracy': 0.9975894349221373, 'eval_runtime': 15.0775, 'eval_samples_per_second': 285.193, 'eval_steps_per_second': 35.682, 'epoch': 8.0, 'step': 7472}, {'loss': 0.0001, 'grad_norm': 0.0006836227257736027, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 8406}, {'eval_loss': 0.019410070031881332, 'eval_precision': 0.9624933967247755, 'eval_recall': 0.9584429247764334, 'eval_f1': 0.9604638903531892, 'eval_accuracy': 0.9976338011505642, 'eval_runtime': 15.0419, 'eval_samples_per_second': 285.868, 'eval_steps_per_second': 35.767, 'epoch': 9.0, 'step': 8406}, {'loss': 0.0001, 'grad_norm': 0.0007204046123661101, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 9340}, {'eval_loss': 0.019519459456205368, 'eval_precision': 0.9567483064095883, 'eval_recall': 0.9658074697527617, 'eval_f1': 0.9612565445026178, 'eval_accuracy': 0.9976485898933731, 'eval_runtime': 15.1316, 'eval_samples_per_second': 284.174, 'eval_steps_per_second': 35.555, 'epoch': 10.0, 'step': 9340}, {'loss': 0.0, 'grad_norm': 0.00014988773909863085, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 10274}, {'eval_loss': 0.020969128236174583, 'eval_precision': 0.9552549427679501, 'eval_recall': 0.9658074697527617, 'eval_f1': 0.9605022233847763, 'eval_accuracy': 0.9975746461793283, 'eval_runtime': 15.0546, 'eval_samples_per_second': 285.627, 'eval_steps_per_second': 35.737, 'epoch': 11.0, 'step': 10274}, {'loss': 0.0, 'grad_norm': 3.832936272374354e-05, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 11208}, {'eval_loss': 0.02115016058087349, 'eval_precision': 0.956725755995829, 'eval_recall': 0.9652814308258811, 'eval_f1': 0.9609845509295627, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 15.0718, 'eval_samples_per_second': 285.301, 'eval_steps_per_second': 35.696, 'epoch': 12.0, 'step': 11208}, {'train_runtime': 3374.1154, 'train_samples_per_second': 106.232, 'train_steps_per_second': 3.322, 'total_flos': 3.5205542006220372e+16, 'train_loss': 0.002875326964957673, 'epoch': 12.0, 'step': 11208}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9967
  predict_f1                 =     0.9448
  predict_loss               =     0.0113
  predict_precision          =     0.9461
  predict_recall             =     0.9436
  predict_runtime            = 0:00:12.18
  predict_samples_per_second =      283.1
  predict_steps_per_second   =     35.449
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_norbert3-large_00_101.json completed. F1: 0.944841012329656
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert_base_08_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert_base_08_101.json
03111215_ner2_nb-bert_base Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:06, 4127.22 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:06, 4342.21 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 4570.84 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:04, 5384.06 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:00<00:04, 6044.03 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6694.36 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 7017.87 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:02, 7603.97 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:02, 8099.06 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 6677.73 examples/s]Running tokenizer on train dataset:  37%|███▋      | 11000/29870 [00:01<00:02, 7383.81 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 7689.51 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:01<00:02, 7317.81 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:02, 7473.72 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:02, 7299.09 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7523.88 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7687.42 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7656.02 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7429.91 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7457.49 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 7462.90 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 7562.49 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7787.88 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 6392.10 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 6517.86 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 6589.54 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:03<00:00, 6067.21 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6342.13 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6571.22 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6803.62 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6617.61 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 7102.93 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 5145.14 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 5823.64 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 5573.80 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4786.23 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7715.38 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 5663.57 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 6479.23 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 5438.84 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03111215_ner2_nb-bert_base/checkpoint-467 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03111215_ner2_nb-bert_base Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0488, 'grad_norm': 0.9341228008270264, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.009543804451823235, 'eval_precision': 0.9556025369978859, 'eval_recall': 0.9510783798001052, 'eval_f1': 0.9533350909570263, 'eval_accuracy': 0.9972788713231489, 'eval_runtime': 6.3643, 'eval_samples_per_second': 675.644, 'eval_steps_per_second': 84.534, 'epoch': 1.0}
{'loss': 0.008, 'grad_norm': 0.39360541105270386, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.008475257083773613, 'eval_precision': 0.9563182527301092, 'eval_recall': 0.9673855865334035, 'eval_f1': 0.9618200836820083, 'eval_accuracy': 0.9978112660642718, 'eval_runtime': 6.3305, 'eval_samples_per_second': 679.254, 'eval_steps_per_second': 84.986, 'epoch': 2.0}
{'loss': 0.0047, 'grad_norm': 0.560268759727478, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.009325859136879444, 'eval_precision': 0.9514963880288958, 'eval_recall': 0.9700157811678064, 'eval_f1': 0.9606668403230008, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 6.2961, 'eval_samples_per_second': 682.963, 'eval_steps_per_second': 85.45, 'epoch': 3.0}
{'loss': 0.0029, 'grad_norm': 0.14286816120147705, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.010632237419486046, 'eval_precision': 0.9477191184008201, 'eval_recall': 0.9726459758022094, 'eval_f1': 0.9600207684319834, 'eval_accuracy': 0.9975746461793283, 'eval_runtime': 6.3561, 'eval_samples_per_second': 676.519, 'eval_steps_per_second': 84.644, 'epoch': 4.0}
{'loss': 0.0021, 'grad_norm': 0.26421815156936646, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.009914752095937729, 'eval_precision': 0.9519876097057305, 'eval_recall': 0.9700157811678064, 'eval_f1': 0.9609171443460135, 'eval_accuracy': 0.997722533607418, 'eval_runtime': 6.2797, 'eval_samples_per_second': 684.747, 'eval_steps_per_second': 85.673, 'epoch': 5.0}
{'loss': 0.0015, 'grad_norm': 0.17611195147037506, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.010518153198063374, 'eval_precision': 0.9592689295039164, 'eval_recall': 0.9663335086796423, 'eval_f1': 0.9627882599580712, 'eval_accuracy': 0.9978408435498898, 'eval_runtime': 6.2812, 'eval_samples_per_second': 684.586, 'eval_steps_per_second': 85.653, 'epoch': 6.0}
{'loss': 0.0011, 'grad_norm': 0.5055036544799805, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.011445342563092709, 'eval_precision': 0.9554634904194718, 'eval_recall': 0.970541820094687, 'eval_f1': 0.9629436325678496, 'eval_accuracy': 0.9978260548070809, 'eval_runtime': 6.3306, 'eval_samples_per_second': 679.244, 'eval_steps_per_second': 84.984, 'epoch': 7.0}
{'loss': 0.0008, 'grad_norm': 0.00321596534922719, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.012187100015580654, 'eval_precision': 0.9559585492227979, 'eval_recall': 0.970541820094687, 'eval_f1': 0.9631949882537196, 'eval_accuracy': 0.9977816885786539, 'eval_runtime': 6.3397, 'eval_samples_per_second': 678.267, 'eval_steps_per_second': 84.862, 'epoch': 8.0}
{'loss': 0.0006, 'grad_norm': 0.031037531793117523, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.012228945270180702, 'eval_precision': 0.9642669469259064, 'eval_recall': 0.9652814308258811, 'eval_f1': 0.9647739221871713, 'eval_accuracy': 0.9979147872639347, 'eval_runtime': 6.3096, 'eval_samples_per_second': 681.497, 'eval_steps_per_second': 85.266, 'epoch': 9.0}
{'loss': 0.0005, 'grad_norm': 0.058877766132354736, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.012570619583129883, 'eval_precision': 0.9643231899265478, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9655897031783556, 'eval_accuracy': 0.9979739422351706, 'eval_runtime': 6.4642, 'eval_samples_per_second': 665.204, 'eval_steps_per_second': 83.228, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.0023687032517045736, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.012973724864423275, 'eval_precision': 0.960313315926893, 'eval_recall': 0.9673855865334035, 'eval_f1': 0.9638364779874214, 'eval_accuracy': 0.9978704210355078, 'eval_runtime': 6.4565, 'eval_samples_per_second': 665.991, 'eval_steps_per_second': 83.326, 'epoch': 11.0}
{'loss': 0.0004, 'grad_norm': 0.003110479097813368, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.012955970130860806, 'eval_precision': 0.9608150470219435, 'eval_recall': 0.9673855865334035, 'eval_f1': 0.964089121887287, 'eval_accuracy': 0.9978852097783167, 'eval_runtime': 6.3469, 'eval_samples_per_second': 677.491, 'eval_steps_per_second': 84.765, 'epoch': 12.0}
{'train_runtime': 1228.2089, 'train_samples_per_second': 291.84, 'train_steps_per_second': 4.563, 'train_loss': 0.005988991024939185, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =      0.006
  train_runtime            = 0:20:28.20
  train_samples            =      29870
  train_samples_per_second =     291.84
  train_steps_per_second   =      4.563
[{'loss': 0.0488, 'grad_norm': 0.9341228008270264, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 467}, {'eval_loss': 0.009543804451823235, 'eval_precision': 0.9556025369978859, 'eval_recall': 0.9510783798001052, 'eval_f1': 0.9533350909570263, 'eval_accuracy': 0.9972788713231489, 'eval_runtime': 6.3643, 'eval_samples_per_second': 675.644, 'eval_steps_per_second': 84.534, 'epoch': 1.0, 'step': 467}, {'loss': 0.008, 'grad_norm': 0.39360541105270386, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 934}, {'eval_loss': 0.008475257083773613, 'eval_precision': 0.9563182527301092, 'eval_recall': 0.9673855865334035, 'eval_f1': 0.9618200836820083, 'eval_accuracy': 0.9978112660642718, 'eval_runtime': 6.3305, 'eval_samples_per_second': 679.254, 'eval_steps_per_second': 84.986, 'epoch': 2.0, 'step': 934}, {'loss': 0.0047, 'grad_norm': 0.560268759727478, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 1401}, {'eval_loss': 0.009325859136879444, 'eval_precision': 0.9514963880288958, 'eval_recall': 0.9700157811678064, 'eval_f1': 0.9606668403230008, 'eval_accuracy': 0.9976190124077552, 'eval_runtime': 6.2961, 'eval_samples_per_second': 682.963, 'eval_steps_per_second': 85.45, 'epoch': 3.0, 'step': 1401}, {'loss': 0.0029, 'grad_norm': 0.14286816120147705, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1868}, {'eval_loss': 0.010632237419486046, 'eval_precision': 0.9477191184008201, 'eval_recall': 0.9726459758022094, 'eval_f1': 0.9600207684319834, 'eval_accuracy': 0.9975746461793283, 'eval_runtime': 6.3561, 'eval_samples_per_second': 676.519, 'eval_steps_per_second': 84.644, 'epoch': 4.0, 'step': 1868}, {'loss': 0.0021, 'grad_norm': 0.26421815156936646, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 2335}, {'eval_loss': 0.009914752095937729, 'eval_precision': 0.9519876097057305, 'eval_recall': 0.9700157811678064, 'eval_f1': 0.9609171443460135, 'eval_accuracy': 0.997722533607418, 'eval_runtime': 6.2797, 'eval_samples_per_second': 684.747, 'eval_steps_per_second': 85.673, 'epoch': 5.0, 'step': 2335}, {'loss': 0.0015, 'grad_norm': 0.17611195147037506, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 2802}, {'eval_loss': 0.010518153198063374, 'eval_precision': 0.9592689295039164, 'eval_recall': 0.9663335086796423, 'eval_f1': 0.9627882599580712, 'eval_accuracy': 0.9978408435498898, 'eval_runtime': 6.2812, 'eval_samples_per_second': 684.586, 'eval_steps_per_second': 85.653, 'epoch': 6.0, 'step': 2802}, {'loss': 0.0011, 'grad_norm': 0.5055036544799805, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 3269}, {'eval_loss': 0.011445342563092709, 'eval_precision': 0.9554634904194718, 'eval_recall': 0.970541820094687, 'eval_f1': 0.9629436325678496, 'eval_accuracy': 0.9978260548070809, 'eval_runtime': 6.3306, 'eval_samples_per_second': 679.244, 'eval_steps_per_second': 84.984, 'epoch': 7.0, 'step': 3269}, {'loss': 0.0008, 'grad_norm': 0.00321596534922719, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 3736}, {'eval_loss': 0.012187100015580654, 'eval_precision': 0.9559585492227979, 'eval_recall': 0.970541820094687, 'eval_f1': 0.9631949882537196, 'eval_accuracy': 0.9977816885786539, 'eval_runtime': 6.3397, 'eval_samples_per_second': 678.267, 'eval_steps_per_second': 84.862, 'epoch': 8.0, 'step': 3736}, {'loss': 0.0006, 'grad_norm': 0.031037531793117523, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 4203}, {'eval_loss': 0.012228945270180702, 'eval_precision': 0.9642669469259064, 'eval_recall': 0.9652814308258811, 'eval_f1': 0.9647739221871713, 'eval_accuracy': 0.9979147872639347, 'eval_runtime': 6.3096, 'eval_samples_per_second': 681.497, 'eval_steps_per_second': 85.266, 'epoch': 9.0, 'step': 4203}, {'loss': 0.0005, 'grad_norm': 0.058877766132354736, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 4670}, {'eval_loss': 0.012570619583129883, 'eval_precision': 0.9643231899265478, 'eval_recall': 0.9668595476065229, 'eval_f1': 0.9655897031783556, 'eval_accuracy': 0.9979739422351706, 'eval_runtime': 6.4642, 'eval_samples_per_second': 665.204, 'eval_steps_per_second': 83.228, 'epoch': 10.0, 'step': 4670}, {'loss': 0.0004, 'grad_norm': 0.0023687032517045736, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 5137}, {'eval_loss': 0.012973724864423275, 'eval_precision': 0.960313315926893, 'eval_recall': 0.9673855865334035, 'eval_f1': 0.9638364779874214, 'eval_accuracy': 0.9978704210355078, 'eval_runtime': 6.4565, 'eval_samples_per_second': 665.991, 'eval_steps_per_second': 83.326, 'epoch': 11.0, 'step': 5137}, {'loss': 0.0004, 'grad_norm': 0.003110479097813368, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 5604}, {'eval_loss': 0.012955970130860806, 'eval_precision': 0.9608150470219435, 'eval_recall': 0.9673855865334035, 'eval_f1': 0.964089121887287, 'eval_accuracy': 0.9978852097783167, 'eval_runtime': 6.3469, 'eval_samples_per_second': 677.491, 'eval_steps_per_second': 84.765, 'epoch': 12.0, 'step': 5604}, {'train_runtime': 1228.2089, 'train_samples_per_second': 291.84, 'train_steps_per_second': 4.563, 'total_flos': 1.437432751179282e+16, 'train_loss': 0.005988991024939185, 'epoch': 12.0, 'step': 5604}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9961
  predict_f1                 =     0.9358
  predict_loss               =     0.0146
  predict_precision          =     0.9269
  predict_recall             =     0.9449
  predict_runtime            = 0:00:05.23
  predict_samples_per_second =    659.047
  predict_steps_per_second   =     82.524
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert_base_08_101.json completed. F1: 0.9358151476251605
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_07_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_07_101.json
03111215_ner2_nb-bert-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:06, 4705.43 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:07, 3536.03 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:05, 4702.52 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:05, 4666.79 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:01<00:04, 5455.85 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:01<00:03, 6227.19 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 6656.82 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:03, 6015.75 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:03, 6791.83 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 7290.67 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 8077.34 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:02<00:02, 7719.59 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:02, 7815.85 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7601.90 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7760.49 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7850.82 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7745.95 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7515.39 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7521.03 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 6205.91 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 6624.60 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7110.14 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 7292.81 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 7160.24 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 7037.20 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:04<00:00, 6353.07 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6615.29 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6800.89 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 7004.62 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6553.56 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 8658.99 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 8255.48 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 5446.57 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 4652.75 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4739.20 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7832.98 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 5001.98 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 5998.22 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 4514.68 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03111215_ner2_nb-bert-large/checkpoint-467 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03111215_ner2_nb-bert-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.04, 'grad_norm': 0.3574131727218628, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.012391271069645882, 'eval_precision': 0.9329815303430079, 'eval_recall': 0.9300368227248816, 'eval_f1': 0.9315068493150686, 'eval_accuracy': 0.9959183069847233, 'eval_runtime': 42.4897, 'eval_samples_per_second': 101.201, 'eval_steps_per_second': 12.662, 'epoch': 1.0}
{'loss': 0.0078, 'grad_norm': 0.4030602276325226, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.010745231993496418, 'eval_precision': 0.9497620306716024, 'eval_recall': 0.9447659126775382, 'eval_f1': 0.9472573839662447, 'eval_accuracy': 0.9968943640101156, 'eval_runtime': 12.0384, 'eval_samples_per_second': 357.189, 'eval_steps_per_second': 44.69, 'epoch': 2.0}
{'loss': 0.0041, 'grad_norm': 0.5219482779502869, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.012143490836024284, 'eval_precision': 0.9365652398143373, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9458333333333334, 'eval_accuracy': 0.9968204202960707, 'eval_runtime': 12.138, 'eval_samples_per_second': 354.261, 'eval_steps_per_second': 44.324, 'epoch': 3.0}
{'loss': 0.0026, 'grad_norm': 0.17407436668872833, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.012732066214084625, 'eval_precision': 0.9422776911076443, 'eval_recall': 0.9531825355076275, 'eval_f1': 0.9476987447698745, 'eval_accuracy': 0.9969830964669694, 'eval_runtime': 12.0844, 'eval_samples_per_second': 355.83, 'eval_steps_per_second': 44.52, 'epoch': 4.0}
{'loss': 0.0015, 'grad_norm': 0.3384004533290863, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.013516699895262718, 'eval_precision': 0.946953781512605, 'eval_recall': 0.9484481851657023, 'eval_f1': 0.947700394218134, 'eval_accuracy': 0.9969683077241603, 'eval_runtime': 12.1847, 'eval_samples_per_second': 352.901, 'eval_steps_per_second': 44.154, 'epoch': 5.0}
{'loss': 0.001, 'grad_norm': 0.004491870757192373, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.015865376219153404, 'eval_precision': 0.9411157024793388, 'eval_recall': 0.9584429247764334, 'eval_f1': 0.9497002866823039, 'eval_accuracy': 0.9969683077241603, 'eval_runtime': 12.0924, 'eval_samples_per_second': 355.595, 'eval_steps_per_second': 44.491, 'epoch': 6.0}
{'loss': 0.0006, 'grad_norm': 0.04676653817296028, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.015292656607925892, 'eval_precision': 0.9415113871635611, 'eval_recall': 0.9568648079957917, 'eval_f1': 0.9491260109574746, 'eval_accuracy': 0.9970274626953962, 'eval_runtime': 12.0634, 'eval_samples_per_second': 356.452, 'eval_steps_per_second': 44.598, 'epoch': 7.0}
{'loss': 0.0005, 'grad_norm': 0.04011711850762367, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.01576710119843483, 'eval_precision': 0.9477260846837429, 'eval_recall': 0.9537085744345082, 'eval_f1': 0.9507079181961196, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 12.0757, 'eval_samples_per_second': 356.087, 'eval_steps_per_second': 44.552, 'epoch': 8.0}
{'loss': 0.0003, 'grad_norm': 0.00466212909668684, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.01749318093061447, 'eval_precision': 0.9384615384615385, 'eval_recall': 0.9626512361914782, 'eval_f1': 0.9504024928589977, 'eval_accuracy': 0.9969683077241603, 'eval_runtime': 12.0885, 'eval_samples_per_second': 355.709, 'eval_steps_per_second': 44.505, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.0015086089260876179, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.017053421586751938, 'eval_precision': 0.945974025974026, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9519079979090433, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 12.0661, 'eval_samples_per_second': 356.37, 'eval_steps_per_second': 44.588, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.001009574276395142, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.018222052603960037, 'eval_precision': 0.9416623644811565, 'eval_recall': 0.9594950026301946, 'eval_f1': 0.9504950495049506, 'eval_accuracy': 0.9970274626953962, 'eval_runtime': 12.0773, 'eval_samples_per_second': 356.039, 'eval_steps_per_second': 44.546, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.0008190391818061471, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.018680397421121597, 'eval_precision': 0.9430936368339369, 'eval_recall': 0.9589689637033141, 'eval_f1': 0.9509650495565989, 'eval_accuracy': 0.9970570401810143, 'eval_runtime': 12.1561, 'eval_samples_per_second': 353.733, 'eval_steps_per_second': 44.258, 'epoch': 12.0}
{'train_runtime': 2914.2627, 'train_samples_per_second': 122.995, 'train_steps_per_second': 1.923, 'train_loss': 0.004912757764781142, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0049
  train_runtime            = 0:48:34.26
  train_samples            =      29870
  train_samples_per_second =    122.995
  train_steps_per_second   =      1.923
[{'loss': 0.04, 'grad_norm': 0.3574131727218628, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 467}, {'eval_loss': 0.012391271069645882, 'eval_precision': 0.9329815303430079, 'eval_recall': 0.9300368227248816, 'eval_f1': 0.9315068493150686, 'eval_accuracy': 0.9959183069847233, 'eval_runtime': 42.4897, 'eval_samples_per_second': 101.201, 'eval_steps_per_second': 12.662, 'epoch': 1.0, 'step': 467}, {'loss': 0.0078, 'grad_norm': 0.4030602276325226, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 934}, {'eval_loss': 0.010745231993496418, 'eval_precision': 0.9497620306716024, 'eval_recall': 0.9447659126775382, 'eval_f1': 0.9472573839662447, 'eval_accuracy': 0.9968943640101156, 'eval_runtime': 12.0384, 'eval_samples_per_second': 357.189, 'eval_steps_per_second': 44.69, 'epoch': 2.0, 'step': 934}, {'loss': 0.0041, 'grad_norm': 0.5219482779502869, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 1401}, {'eval_loss': 0.012143490836024284, 'eval_precision': 0.9365652398143373, 'eval_recall': 0.95528669121515, 'eval_f1': 0.9458333333333334, 'eval_accuracy': 0.9968204202960707, 'eval_runtime': 12.138, 'eval_samples_per_second': 354.261, 'eval_steps_per_second': 44.324, 'epoch': 3.0, 'step': 1401}, {'loss': 0.0026, 'grad_norm': 0.17407436668872833, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1868}, {'eval_loss': 0.012732066214084625, 'eval_precision': 0.9422776911076443, 'eval_recall': 0.9531825355076275, 'eval_f1': 0.9476987447698745, 'eval_accuracy': 0.9969830964669694, 'eval_runtime': 12.0844, 'eval_samples_per_second': 355.83, 'eval_steps_per_second': 44.52, 'epoch': 4.0, 'step': 1868}, {'loss': 0.0015, 'grad_norm': 0.3384004533290863, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 2335}, {'eval_loss': 0.013516699895262718, 'eval_precision': 0.946953781512605, 'eval_recall': 0.9484481851657023, 'eval_f1': 0.947700394218134, 'eval_accuracy': 0.9969683077241603, 'eval_runtime': 12.1847, 'eval_samples_per_second': 352.901, 'eval_steps_per_second': 44.154, 'epoch': 5.0, 'step': 2335}, {'loss': 0.001, 'grad_norm': 0.004491870757192373, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 2802}, {'eval_loss': 0.015865376219153404, 'eval_precision': 0.9411157024793388, 'eval_recall': 0.9584429247764334, 'eval_f1': 0.9497002866823039, 'eval_accuracy': 0.9969683077241603, 'eval_runtime': 12.0924, 'eval_samples_per_second': 355.595, 'eval_steps_per_second': 44.491, 'epoch': 6.0, 'step': 2802}, {'loss': 0.0006, 'grad_norm': 0.04676653817296028, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 3269}, {'eval_loss': 0.015292656607925892, 'eval_precision': 0.9415113871635611, 'eval_recall': 0.9568648079957917, 'eval_f1': 0.9491260109574746, 'eval_accuracy': 0.9970274626953962, 'eval_runtime': 12.0634, 'eval_samples_per_second': 356.452, 'eval_steps_per_second': 44.598, 'epoch': 7.0, 'step': 3269}, {'loss': 0.0005, 'grad_norm': 0.04011711850762367, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 3736}, {'eval_loss': 0.01576710119843483, 'eval_precision': 0.9477260846837429, 'eval_recall': 0.9537085744345082, 'eval_f1': 0.9507079181961196, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 12.0757, 'eval_samples_per_second': 356.087, 'eval_steps_per_second': 44.552, 'epoch': 8.0, 'step': 3736}, {'loss': 0.0003, 'grad_norm': 0.00466212909668684, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 4203}, {'eval_loss': 0.01749318093061447, 'eval_precision': 0.9384615384615385, 'eval_recall': 0.9626512361914782, 'eval_f1': 0.9504024928589977, 'eval_accuracy': 0.9969683077241603, 'eval_runtime': 12.0885, 'eval_samples_per_second': 355.709, 'eval_steps_per_second': 44.505, 'epoch': 9.0, 'step': 4203}, {'loss': 0.0003, 'grad_norm': 0.0015086089260876179, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 4670}, {'eval_loss': 0.017053421586751938, 'eval_precision': 0.945974025974026, 'eval_recall': 0.9579168858495528, 'eval_f1': 0.9519079979090433, 'eval_accuracy': 0.9971161951522501, 'eval_runtime': 12.0661, 'eval_samples_per_second': 356.37, 'eval_steps_per_second': 44.588, 'epoch': 10.0, 'step': 4670}, {'loss': 0.0002, 'grad_norm': 0.001009574276395142, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 5137}, {'eval_loss': 0.018222052603960037, 'eval_precision': 0.9416623644811565, 'eval_recall': 0.9594950026301946, 'eval_f1': 0.9504950495049506, 'eval_accuracy': 0.9970274626953962, 'eval_runtime': 12.0773, 'eval_samples_per_second': 356.039, 'eval_steps_per_second': 44.546, 'epoch': 11.0, 'step': 5137}, {'loss': 0.0001, 'grad_norm': 0.0008190391818061471, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 5604}, {'eval_loss': 0.018680397421121597, 'eval_precision': 0.9430936368339369, 'eval_recall': 0.9589689637033141, 'eval_f1': 0.9509650495565989, 'eval_accuracy': 0.9970570401810143, 'eval_runtime': 12.1561, 'eval_samples_per_second': 353.733, 'eval_steps_per_second': 44.258, 'epoch': 12.0, 'step': 5604}, {'train_runtime': 2914.2627, 'train_samples_per_second': 122.995, 'train_steps_per_second': 1.923, 'total_flos': 3.890390621534781e+16, 'train_loss': 0.004912757764781142, 'epoch': 12.0, 'step': 5604}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9955
  predict_f1                 =     0.9289
  predict_loss               =     0.0172
  predict_precision          =      0.935
  predict_recall             =     0.9229
  predict_runtime            = 0:00:09.88
  predict_samples_per_second =    349.166
  predict_steps_per_second   =     43.722
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_07_101.json completed. F1: 0.928897586431833
/cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_04_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03111215_ner2_nb-bert-large_04_101.json
03111215_ner2_nb-bert-large Our label2id: {'O': 0, 'B-ORG': 1, 'I-ORG': 2, 'B-PER': 3, 'I-PER': 4}
Running tokenizer on train dataset:   0%|          | 0/29870 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:   3%|▎         | 1000/29870 [00:00<00:06, 4347.13 examples/s]Running tokenizer on train dataset:   7%|▋         | 2000/29870 [00:00<00:06, 4589.74 examples/s]Running tokenizer on train dataset:  10%|█         | 3000/29870 [00:00<00:04, 5583.38 examples/s]Running tokenizer on train dataset:  13%|█▎        | 4000/29870 [00:00<00:04, 5759.41 examples/s]Running tokenizer on train dataset:  17%|█▋        | 5000/29870 [00:00<00:03, 6366.12 examples/s]Running tokenizer on train dataset:  20%|██        | 6000/29870 [00:00<00:03, 7002.38 examples/s]Running tokenizer on train dataset:  23%|██▎       | 7000/29870 [00:01<00:03, 7260.05 examples/s]Running tokenizer on train dataset:  27%|██▋       | 8000/29870 [00:01<00:03, 6358.24 examples/s]Running tokenizer on train dataset:  30%|███       | 9000/29870 [00:01<00:02, 7095.35 examples/s]Running tokenizer on train dataset:  33%|███▎      | 10000/29870 [00:01<00:02, 7547.58 examples/s]Running tokenizer on train dataset:  40%|████      | 12000/29870 [00:01<00:02, 8279.03 examples/s]Running tokenizer on train dataset:  44%|████▎     | 13000/29870 [00:01<00:02, 7880.98 examples/s]Running tokenizer on train dataset:  47%|████▋     | 14000/29870 [00:02<00:01, 7953.50 examples/s]Running tokenizer on train dataset:  50%|█████     | 15000/29870 [00:02<00:01, 7724.75 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 16000/29870 [00:02<00:01, 7865.35 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 17000/29870 [00:02<00:01, 7955.31 examples/s]Running tokenizer on train dataset:  60%|██████    | 18000/29870 [00:02<00:01, 7870.50 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 19000/29870 [00:02<00:01, 7631.67 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 20000/29870 [00:02<00:01, 7624.54 examples/s]Running tokenizer on train dataset:  70%|███████   | 21000/29870 [00:03<00:01, 6277.41 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 22000/29870 [00:03<00:01, 6719.29 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 23000/29870 [00:03<00:00, 7206.81 examples/s]Running tokenizer on train dataset:  80%|████████  | 24000/29870 [00:03<00:00, 7304.73 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 25000/29870 [00:03<00:00, 7200.08 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 26000/29870 [00:03<00:00, 7085.03 examples/s]Running tokenizer on train dataset:  90%|█████████ | 27000/29870 [00:03<00:00, 6393.63 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 28000/29870 [00:04<00:00, 6654.67 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 29000/29870 [00:04<00:00, 6870.48 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 7085.07 examples/s]Running tokenizer on train dataset: 100%|██████████| 29870/29870 [00:04<00:00, 6779.97 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/4300 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  23%|██▎       | 1000/4300 [00:00<00:00, 7310.04 examples/s]Running tokenizer on validation dataset:  47%|████▋     | 2000/4300 [00:00<00:00, 7766.99 examples/s]Running tokenizer on validation dataset:  70%|██████▉   | 3000/4300 [00:00<00:00, 5263.51 examples/s]Running tokenizer on validation dataset:  93%|█████████▎| 4000/4300 [00:00<00:00, 4106.48 examples/s]Running tokenizer on validation dataset: 100%|██████████| 4300/4300 [00:00<00:00, 4304.18 examples/s]
Running tokenizer on test dataset:   0%|          | 0/3450 [00:00<?, ? examples/s]Running tokenizer on test dataset:  29%|██▉       | 1000/3450 [00:00<00:00, 7906.07 examples/s]Running tokenizer on test dataset:  58%|█████▊    | 2000/3450 [00:00<00:00, 4864.15 examples/s]Running tokenizer on test dataset:  87%|████████▋ | 3000/3450 [00:00<00:00, 5784.03 examples/s]Running tokenizer on test dataset: 100%|██████████| 3450/3450 [00:00<00:00, 4008.73 examples/s]
03111215_ner2_nb-bert-large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'ner_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
Traceback (most recent call last):
  File "/cluster/work/users/egilron/seq-label_github/seq_label.py", line 289, in <module>
    train_result = trainer.train(resume_from_checkpoint=False)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/accelerate/data_loader.py", line 461, in __iter__
    current_batch = send_to_device(current_batch, self.device)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/accelerate/utils/operations.py", line 167, in send_to_device
    {
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/accelerate/utils/operations.py", line 168, in <dictcomp>
    k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)
  File "/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/accelerate/utils/operations.py", line 186, in send_to_device
    return tensor.to(device, non_blocking=non_blocking)
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Job 10918080 consumed 36.2 billing hours from project nn9851k.

Submitted 2024-03-11T18:19:28; waited 12.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 20.0 hours
Elapsed wallclock time:   4.5 hours

Task and CPU statistics:
ID              CPUs  Tasks  CPU util                Start  Elapsed  Exit status
10918080           1            0.0 %  2024-03-11T18:19:40    4.5 h  1
10918080.batch     1      1    92.0 %  2024-03-11T18:19:40    4.5 h  1

Used CPU time:   4.2 CPU hours
Unused CPU time: 21.8 CPU minutes

Memory statistics, in GiB:
ID               Alloc   Usage
10918080          24.0        
10918080.batch    24.0     2.2

GPU usage stats:
Job 10918080 completed at Mon Mar 11 22:50:58 CET 2024
