Starting job 410060 on gpu-7 at Thu Jan 18 11:02:36 CET 2024

/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_09.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_09.json
01170944_tsa-intensity_NorBERT_1_09.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_39.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_39.json
01170944_tsa-intensity_NB-BERT_base_39.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_23.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_23.json
01170944_tsa-intensity_NorBERT_1_23.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_small_28.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_small_28.json
01170944_tsa-intensity_NorBERT_3_small_28.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_small_14.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_small_14.json
01170944_tsa-intensity_NorBERT_3_small_14.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_41.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_41.json
01170944_tsa-intensity_NorBERT_3_large_41.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_18.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_18.json
01170944_tsa-intensity_NB-BERT_base_18.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_04.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_04.json
01170944_tsa-intensity_NB-BERT_base_04.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_16.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_16.json
01170944_tsa-intensity_NorBERT_1_16.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_31.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_31.json
01170944_tsa-intensity_NorBERT_2_31.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_38.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_38.json
01170944_tsa-intensity_NorBERT_2_38.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_30.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_30.json
01170944_tsa-intensity_NorBERT_1_30.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_03.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_03.json
01170944_tsa-intensity_NorBERT_2_03.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_05.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_05.json
01170944_tsa-intensity_NB-BERT_large_05.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_base_22.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_base_22.json
01170944_tsa-intensity_NorBERT_3_base_22.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_34.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_34.json
01170944_tsa-intensity_NorBERT_3_large_34.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_25.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_25.json
01170944_tsa-intensity_NB-BERT_base_25.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_26.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_26.json
01170944_tsa-intensity_NB-BERT_large_26.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_small_07.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_small_07.json
01170944_tsa-intensity_NorBERT_3_small_07.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_base_15.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_base_15.json
01170944_tsa-intensity_NorBERT_3_base_15.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_37.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_1_37.json
01170944_tsa-intensity_NorBERT_1_37.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_17.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_17.json
01170944_tsa-intensity_NorBERT_2_17.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_40.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_40.json
01170944_tsa-intensity_NB-BERT_large_40.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_10.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_2_10.json
01170944_tsa-intensity_NorBERT_2_10.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_base_08.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_base_08.json
01170944_tsa-intensity_NorBERT_3_base_08.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_32.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_32.json
01170944_tsa-intensity_NB-BERT_base_32.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_12.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_large_12.json
01170944_tsa-intensity_NB-BERT_large_12.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_27.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_27.json
01170944_tsa-intensity_NorBERT_3_large_27.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_11.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_11.json
01170944_tsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-targ-Negative-0': 1, 'I-targ-Negative-0': 2, 'B-targ-Negative-1': 3, 'I-targ-Negative-1': 4, 'B-targ-Negative-2': 5, 'I-targ-Negative-2': 6, 'B-targ-Negative-3': 7, 'I-targ-Negative-3': 8, 'B-targ-Positive-0': 9, 'I-targ-Positive-0': 10, 'B-targ-Positive-1': 11, 'I-targ-Positive-1': 12, 'B-targ-Positive-2': 13, 'I-targ-Positive-2': 14, 'B-targ-Positive-3': 15, 'I-targ-Positive-3': 16}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 4779.72 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 6050.49 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6301.18 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6564.75 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6731.92 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 6928.06 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 6186.76 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 6519.75 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 6327.54 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 6094.93 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6181.18 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 6948.54 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 6777.71 examples/s]
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/projects/ec30/egilron/tsa-hf/01170944_tsa-intensity_NB-BERT_base/checkpoint-216 already exists and is non-empty.Saving will proceed but saved results may be invalid.
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
01170944_tsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2972, 'learning_rate': 7.666666666666668e-05, 'epoch': 1.0}
{'eval_loss': 0.21619418263435364, 'eval_precision': 0.2369109947643979, 'eval_recall': 0.2063854047890536, 'eval_f1': 0.22059719683120052, 'eval_accuracy': 0.9369258086929669, 'eval_runtime': 2.3845, 'eval_samples_per_second': 642.051, 'eval_steps_per_second': 80.518, 'epoch': 1.0}
{'loss': 0.1958, 'learning_rate': 7.333333333333333e-05, 'epoch': 2.0}
{'eval_loss': 0.22149111330509186, 'eval_precision': 0.3011093502377179, 'eval_recall': 0.21664766248574688, 'eval_f1': 0.2519893899204244, 'eval_accuracy': 0.9390102678916081, 'eval_runtime': 2.3571, 'eval_samples_per_second': 649.516, 'eval_steps_per_second': 81.455, 'epoch': 2.0}
{'loss': 0.136, 'learning_rate': 7.000000000000001e-05, 'epoch': 3.0}
{'eval_loss': 0.24153214693069458, 'eval_precision': 0.23768115942028986, 'eval_recall': 0.2805017103762828, 'eval_f1': 0.25732217573221755, 'eval_accuracy': 0.9308654365783988, 'eval_runtime': 2.363, 'eval_samples_per_second': 647.901, 'eval_steps_per_second': 81.252, 'epoch': 3.0}
{'loss': 0.0985, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.0}
{'eval_loss': 0.2801849842071533, 'eval_precision': 0.21728786677240286, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.25631431244153413, 'eval_accuracy': 0.9258086929668803, 'eval_runtime': 2.3578, 'eval_samples_per_second': 649.326, 'eval_steps_per_second': 81.431, 'epoch': 4.0}
{'loss': 0.0662, 'learning_rate': 6.333333333333333e-05, 'epoch': 5.0}
{'eval_loss': 0.2968991696834564, 'eval_precision': 0.2476489028213166, 'eval_recall': 0.2702394526795895, 'eval_f1': 0.2584514721919302, 'eval_accuracy': 0.932641087006871, 'eval_runtime': 2.3578, 'eval_samples_per_second': 649.329, 'eval_steps_per_second': 81.431, 'epoch': 5.0}
{'loss': 0.0489, 'learning_rate': 6.000000000000001e-05, 'epoch': 6.0}
{'eval_loss': 0.3292624056339264, 'eval_precision': 0.24096385542168675, 'eval_recall': 0.2508551881413911, 'eval_f1': 0.2458100558659218, 'eval_accuracy': 0.9334131089322937, 'eval_runtime': 2.3716, 'eval_samples_per_second': 645.566, 'eval_steps_per_second': 80.959, 'epoch': 6.0}
{'loss': 0.0318, 'learning_rate': 5.666666666666668e-05, 'epoch': 7.0}
{'eval_loss': 0.3666563332080841, 'eval_precision': 0.23769100169779286, 'eval_recall': 0.31927023945267957, 'eval_f1': 0.2725060827250608, 'eval_accuracy': 0.9240716436346792, 'eval_runtime': 2.3894, 'eval_samples_per_second': 640.742, 'eval_steps_per_second': 80.354, 'epoch': 7.0}
{'loss': 0.0194, 'learning_rate': 5.333333333333333e-05, 'epoch': 8.0}
{'eval_loss': 0.3740311861038208, 'eval_precision': 0.2578046324269889, 'eval_recall': 0.2919042189281642, 'eval_f1': 0.2737967914438503, 'eval_accuracy': 0.9299004091716204, 'eval_runtime': 2.4076, 'eval_samples_per_second': 635.902, 'eval_steps_per_second': 79.747, 'epoch': 8.0}
{'loss': 0.0164, 'learning_rate': 5e-05, 'epoch': 9.0}
{'eval_loss': 0.3940310776233673, 'eval_precision': 0.2492753623188406, 'eval_recall': 0.29418472063854045, 'eval_f1': 0.2698744769874477, 'eval_accuracy': 0.9307882343858566, 'eval_runtime': 2.3833, 'eval_samples_per_second': 642.387, 'eval_steps_per_second': 80.561, 'epoch': 9.0}
{'loss': 0.0108, 'learning_rate': 4.666666666666667e-05, 'epoch': 10.0}
{'eval_loss': 0.43407946825027466, 'eval_precision': 0.2807377049180328, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.29573664328116567, 'eval_accuracy': 0.9328726935844978, 'eval_runtime': 2.3656, 'eval_samples_per_second': 647.181, 'eval_steps_per_second': 81.162, 'epoch': 10.0}
{'loss': 0.0065, 'learning_rate': 4.3333333333333334e-05, 'epoch': 11.0}
{'eval_loss': 0.43044427037239075, 'eval_precision': 0.2748414376321353, 'eval_recall': 0.29646522234891676, 'eval_f1': 0.2852441031267142, 'eval_accuracy': 0.9335675133173782, 'eval_runtime': 2.4476, 'eval_samples_per_second': 625.523, 'eval_steps_per_second': 78.446, 'epoch': 11.0}
{'loss': 0.004, 'learning_rate': 4e-05, 'epoch': 12.0}
{'eval_loss': 0.44958776235580444, 'eval_precision': 0.29180327868852457, 'eval_recall': 0.30444697833523376, 'eval_f1': 0.2979910714285714, 'eval_accuracy': 0.9352659615533081, 'eval_runtime': 2.5562, 'eval_samples_per_second': 598.945, 'eval_steps_per_second': 75.113, 'epoch': 12.0}
{'loss': 0.0042, 'learning_rate': 3.6666666666666666e-05, 'epoch': 13.0}
{'eval_loss': 0.446728378534317, 'eval_precision': 0.2914847161572052, 'eval_recall': 0.30444697833523376, 'eval_f1': 0.2978248745119911, 'eval_accuracy': 0.9351501582644947, 'eval_runtime': 2.3622, 'eval_samples_per_second': 648.119, 'eval_steps_per_second': 81.279, 'epoch': 13.0}
{'loss': 0.0026, 'learning_rate': 3.3333333333333335e-05, 'epoch': 14.0}
{'eval_loss': 0.4705902934074402, 'eval_precision': 0.28771551724137934, 'eval_recall': 0.30444697833523376, 'eval_f1': 0.2958448753462604, 'eval_accuracy': 0.9351115571682236, 'eval_runtime': 2.3641, 'eval_samples_per_second': 647.599, 'eval_steps_per_second': 81.214, 'epoch': 14.0}
{'loss': 0.0026, 'learning_rate': 3.0000000000000004e-05, 'epoch': 15.0}
{'eval_loss': 0.47353458404541016, 'eval_precision': 0.27040816326530615, 'eval_recall': 0.30216647662485746, 'eval_f1': 0.28540656973613354, 'eval_accuracy': 0.9314444530224658, 'eval_runtime': 2.3631, 'eval_samples_per_second': 647.884, 'eval_steps_per_second': 81.25, 'epoch': 15.0}
{'loss': 0.0019, 'learning_rate': 2.6666666666666667e-05, 'epoch': 16.0}
{'eval_loss': 0.4831494987010956, 'eval_precision': 0.30672748004561, 'eval_recall': 0.30672748004561, 'eval_f1': 0.30672748004561, 'eval_accuracy': 0.9351115571682236, 'eval_runtime': 2.367, 'eval_samples_per_second': 646.81, 'eval_steps_per_second': 81.115, 'epoch': 16.0}
{'loss': 0.0016, 'learning_rate': 2.3333333333333336e-05, 'epoch': 17.0}
{'eval_loss': 0.5034204721450806, 'eval_precision': 0.29836829836829837, 'eval_recall': 0.2919042189281642, 'eval_f1': 0.29510086455331414, 'eval_accuracy': 0.936385393345171, 'eval_runtime': 2.3581, 'eval_samples_per_second': 649.241, 'eval_steps_per_second': 81.42, 'epoch': 17.0}
{'loss': 0.0012, 'learning_rate': 2e-05, 'epoch': 18.0}
{'eval_loss': 0.508039116859436, 'eval_precision': 0.26452905811623245, 'eval_recall': 0.3010262257696693, 'eval_f1': 0.2816, 'eval_accuracy': 0.9302478190380606, 'eval_runtime': 2.3661, 'eval_samples_per_second': 647.062, 'eval_steps_per_second': 81.147, 'epoch': 18.0}
{'loss': 0.001, 'learning_rate': 1.6666666666666667e-05, 'epoch': 19.0}
{'eval_loss': 0.5014714598655701, 'eval_precision': 0.3039443155452436, 'eval_recall': 0.29874572405929306, 'eval_f1': 0.3013225991949396, 'eval_accuracy': 0.9363081911526288, 'eval_runtime': 2.3623, 'eval_samples_per_second': 648.104, 'eval_steps_per_second': 81.278, 'epoch': 19.0}
{'loss': 0.0006, 'learning_rate': 1.3333333333333333e-05, 'epoch': 20.0}
{'eval_loss': 0.5116361379623413, 'eval_precision': 0.30205949656750575, 'eval_recall': 0.3010262257696693, 'eval_f1': 0.30154197601370647, 'eval_accuracy': 0.9346869451092411, 'eval_runtime': 2.4416, 'eval_samples_per_second': 627.048, 'eval_steps_per_second': 78.637, 'epoch': 20.0}
{'loss': 0.0006, 'learning_rate': 1e-05, 'epoch': 21.0}
{'eval_loss': 0.5164017677307129, 'eval_precision': 0.3039332538736591, 'eval_recall': 0.29076396807297605, 'eval_f1': 0.29720279720279724, 'eval_accuracy': 0.935806376901104, 'eval_runtime': 2.3617, 'eval_samples_per_second': 648.25, 'eval_steps_per_second': 81.296, 'epoch': 21.0}
{'loss': 0.0004, 'learning_rate': 6.666666666666667e-06, 'epoch': 22.0}
{'eval_loss': 0.5190087556838989, 'eval_precision': 0.294250281848929, 'eval_recall': 0.2976054732041049, 'eval_f1': 0.29591836734693877, 'eval_accuracy': 0.9340693275689029, 'eval_runtime': 2.3656, 'eval_samples_per_second': 647.193, 'eval_steps_per_second': 81.163, 'epoch': 22.0}
{'loss': 0.0005, 'learning_rate': 3.3333333333333333e-06, 'epoch': 23.0}
{'eval_loss': 0.5239507555961609, 'eval_precision': 0.29607609988109396, 'eval_recall': 0.2839224629418472, 'eval_f1': 0.289871944121071, 'eval_accuracy': 0.9353817648421215, 'eval_runtime': 2.3638, 'eval_samples_per_second': 647.691, 'eval_steps_per_second': 81.226, 'epoch': 23.0}
{'loss': 0.0004, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': 0.5243465304374695, 'eval_precision': 0.29308323563892147, 'eval_recall': 0.28506271379703535, 'eval_f1': 0.2890173410404624, 'eval_accuracy': 0.9344553385316143, 'eval_runtime': 2.3636, 'eval_samples_per_second': 647.729, 'eval_steps_per_second': 81.231, 'epoch': 24.0}
{'train_runtime': 861.0283, 'train_samples_per_second': 240.661, 'train_steps_per_second': 6.021, 'train_loss': 0.039540994754291056, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =     0.0395
  train_runtime            = 0:14:21.02
  train_samples            =       8634
  train_samples_per_second =    240.661
  train_steps_per_second   =      6.021
[{'loss': 0.2972, 'learning_rate': 7.666666666666668e-05, 'epoch': 1.0, 'step': 216}, {'eval_loss': 0.21619418263435364, 'eval_precision': 0.2369109947643979, 'eval_recall': 0.2063854047890536, 'eval_f1': 0.22059719683120052, 'eval_accuracy': 0.9369258086929669, 'eval_runtime': 2.3845, 'eval_samples_per_second': 642.051, 'eval_steps_per_second': 80.518, 'epoch': 1.0, 'step': 216}, {'loss': 0.1958, 'learning_rate': 7.333333333333333e-05, 'epoch': 2.0, 'step': 432}, {'eval_loss': 0.22149111330509186, 'eval_precision': 0.3011093502377179, 'eval_recall': 0.21664766248574688, 'eval_f1': 0.2519893899204244, 'eval_accuracy': 0.9390102678916081, 'eval_runtime': 2.3571, 'eval_samples_per_second': 649.516, 'eval_steps_per_second': 81.455, 'epoch': 2.0, 'step': 432}, {'loss': 0.136, 'learning_rate': 7.000000000000001e-05, 'epoch': 3.0, 'step': 648}, {'eval_loss': 0.24153214693069458, 'eval_precision': 0.23768115942028986, 'eval_recall': 0.2805017103762828, 'eval_f1': 0.25732217573221755, 'eval_accuracy': 0.9308654365783988, 'eval_runtime': 2.363, 'eval_samples_per_second': 647.901, 'eval_steps_per_second': 81.252, 'epoch': 3.0, 'step': 648}, {'loss': 0.0985, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.0, 'step': 864}, {'eval_loss': 0.2801849842071533, 'eval_precision': 0.21728786677240286, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.25631431244153413, 'eval_accuracy': 0.9258086929668803, 'eval_runtime': 2.3578, 'eval_samples_per_second': 649.326, 'eval_steps_per_second': 81.431, 'epoch': 4.0, 'step': 864}, {'loss': 0.0662, 'learning_rate': 6.333333333333333e-05, 'epoch': 5.0, 'step': 1080}, {'eval_loss': 0.2968991696834564, 'eval_precision': 0.2476489028213166, 'eval_recall': 0.2702394526795895, 'eval_f1': 0.2584514721919302, 'eval_accuracy': 0.932641087006871, 'eval_runtime': 2.3578, 'eval_samples_per_second': 649.329, 'eval_steps_per_second': 81.431, 'epoch': 5.0, 'step': 1080}, {'loss': 0.0489, 'learning_rate': 6.000000000000001e-05, 'epoch': 6.0, 'step': 1296}, {'eval_loss': 0.3292624056339264, 'eval_precision': 0.24096385542168675, 'eval_recall': 0.2508551881413911, 'eval_f1': 0.2458100558659218, 'eval_accuracy': 0.9334131089322937, 'eval_runtime': 2.3716, 'eval_samples_per_second': 645.566, 'eval_steps_per_second': 80.959, 'epoch': 6.0, 'step': 1296}, {'loss': 0.0318, 'learning_rate': 5.666666666666668e-05, 'epoch': 7.0, 'step': 1512}, {'eval_loss': 0.3666563332080841, 'eval_precision': 0.23769100169779286, 'eval_recall': 0.31927023945267957, 'eval_f1': 0.2725060827250608, 'eval_accuracy': 0.9240716436346792, 'eval_runtime': 2.3894, 'eval_samples_per_second': 640.742, 'eval_steps_per_second': 80.354, 'epoch': 7.0, 'step': 1512}, {'loss': 0.0194, 'learning_rate': 5.333333333333333e-05, 'epoch': 8.0, 'step': 1728}, {'eval_loss': 0.3740311861038208, 'eval_precision': 0.2578046324269889, 'eval_recall': 0.2919042189281642, 'eval_f1': 0.2737967914438503, 'eval_accuracy': 0.9299004091716204, 'eval_runtime': 2.4076, 'eval_samples_per_second': 635.902, 'eval_steps_per_second': 79.747, 'epoch': 8.0, 'step': 1728}, {'loss': 0.0164, 'learning_rate': 5e-05, 'epoch': 9.0, 'step': 1944}, {'eval_loss': 0.3940310776233673, 'eval_precision': 0.2492753623188406, 'eval_recall': 0.29418472063854045, 'eval_f1': 0.2698744769874477, 'eval_accuracy': 0.9307882343858566, 'eval_runtime': 2.3833, 'eval_samples_per_second': 642.387, 'eval_steps_per_second': 80.561, 'epoch': 9.0, 'step': 1944}, {'loss': 0.0108, 'learning_rate': 4.666666666666667e-05, 'epoch': 10.0, 'step': 2160}, {'eval_loss': 0.43407946825027466, 'eval_precision': 0.2807377049180328, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.29573664328116567, 'eval_accuracy': 0.9328726935844978, 'eval_runtime': 2.3656, 'eval_samples_per_second': 647.181, 'eval_steps_per_second': 81.162, 'epoch': 10.0, 'step': 2160}, {'loss': 0.0065, 'learning_rate': 4.3333333333333334e-05, 'epoch': 11.0, 'step': 2376}, {'eval_loss': 0.43044427037239075, 'eval_precision': 0.2748414376321353, 'eval_recall': 0.29646522234891676, 'eval_f1': 0.2852441031267142, 'eval_accuracy': 0.9335675133173782, 'eval_runtime': 2.4476, 'eval_samples_per_second': 625.523, 'eval_steps_per_second': 78.446, 'epoch': 11.0, 'step': 2376}, {'loss': 0.004, 'learning_rate': 4e-05, 'epoch': 12.0, 'step': 2592}, {'eval_loss': 0.44958776235580444, 'eval_precision': 0.29180327868852457, 'eval_recall': 0.30444697833523376, 'eval_f1': 0.2979910714285714, 'eval_accuracy': 0.9352659615533081, 'eval_runtime': 2.5562, 'eval_samples_per_second': 598.945, 'eval_steps_per_second': 75.113, 'epoch': 12.0, 'step': 2592}, {'loss': 0.0042, 'learning_rate': 3.6666666666666666e-05, 'epoch': 13.0, 'step': 2808}, {'eval_loss': 0.446728378534317, 'eval_precision': 0.2914847161572052, 'eval_recall': 0.30444697833523376, 'eval_f1': 0.2978248745119911, 'eval_accuracy': 0.9351501582644947, 'eval_runtime': 2.3622, 'eval_samples_per_second': 648.119, 'eval_steps_per_second': 81.279, 'epoch': 13.0, 'step': 2808}, {'loss': 0.0026, 'learning_rate': 3.3333333333333335e-05, 'epoch': 14.0, 'step': 3024}, {'eval_loss': 0.4705902934074402, 'eval_precision': 0.28771551724137934, 'eval_recall': 0.30444697833523376, 'eval_f1': 0.2958448753462604, 'eval_accuracy': 0.9351115571682236, 'eval_runtime': 2.3641, 'eval_samples_per_second': 647.599, 'eval_steps_per_second': 81.214, 'epoch': 14.0, 'step': 3024}, {'loss': 0.0026, 'learning_rate': 3.0000000000000004e-05, 'epoch': 15.0, 'step': 3240}, {'eval_loss': 0.47353458404541016, 'eval_precision': 0.27040816326530615, 'eval_recall': 0.30216647662485746, 'eval_f1': 0.28540656973613354, 'eval_accuracy': 0.9314444530224658, 'eval_runtime': 2.3631, 'eval_samples_per_second': 647.884, 'eval_steps_per_second': 81.25, 'epoch': 15.0, 'step': 3240}, {'loss': 0.0019, 'learning_rate': 2.6666666666666667e-05, 'epoch': 16.0, 'step': 3456}, {'eval_loss': 0.4831494987010956, 'eval_precision': 0.30672748004561, 'eval_recall': 0.30672748004561, 'eval_f1': 0.30672748004561, 'eval_accuracy': 0.9351115571682236, 'eval_runtime': 2.367, 'eval_samples_per_second': 646.81, 'eval_steps_per_second': 81.115, 'epoch': 16.0, 'step': 3456}, {'loss': 0.0016, 'learning_rate': 2.3333333333333336e-05, 'epoch': 17.0, 'step': 3672}, {'eval_loss': 0.5034204721450806, 'eval_precision': 0.29836829836829837, 'eval_recall': 0.2919042189281642, 'eval_f1': 0.29510086455331414, 'eval_accuracy': 0.936385393345171, 'eval_runtime': 2.3581, 'eval_samples_per_second': 649.241, 'eval_steps_per_second': 81.42, 'epoch': 17.0, 'step': 3672}, {'loss': 0.0012, 'learning_rate': 2e-05, 'epoch': 18.0, 'step': 3888}, {'eval_loss': 0.508039116859436, 'eval_precision': 0.26452905811623245, 'eval_recall': 0.3010262257696693, 'eval_f1': 0.2816, 'eval_accuracy': 0.9302478190380606, 'eval_runtime': 2.3661, 'eval_samples_per_second': 647.062, 'eval_steps_per_second': 81.147, 'epoch': 18.0, 'step': 3888}, {'loss': 0.001, 'learning_rate': 1.6666666666666667e-05, 'epoch': 19.0, 'step': 4104}, {'eval_loss': 0.5014714598655701, 'eval_precision': 0.3039443155452436, 'eval_recall': 0.29874572405929306, 'eval_f1': 0.3013225991949396, 'eval_accuracy': 0.9363081911526288, 'eval_runtime': 2.3623, 'eval_samples_per_second': 648.104, 'eval_steps_per_second': 81.278, 'epoch': 19.0, 'step': 4104}, {'loss': 0.0006, 'learning_rate': 1.3333333333333333e-05, 'epoch': 20.0, 'step': 4320}, {'eval_loss': 0.5116361379623413, 'eval_precision': 0.30205949656750575, 'eval_recall': 0.3010262257696693, 'eval_f1': 0.30154197601370647, 'eval_accuracy': 0.9346869451092411, 'eval_runtime': 2.4416, 'eval_samples_per_second': 627.048, 'eval_steps_per_second': 78.637, 'epoch': 20.0, 'step': 4320}, {'loss': 0.0006, 'learning_rate': 1e-05, 'epoch': 21.0, 'step': 4536}, {'eval_loss': 0.5164017677307129, 'eval_precision': 0.3039332538736591, 'eval_recall': 0.29076396807297605, 'eval_f1': 0.29720279720279724, 'eval_accuracy': 0.935806376901104, 'eval_runtime': 2.3617, 'eval_samples_per_second': 648.25, 'eval_steps_per_second': 81.296, 'epoch': 21.0, 'step': 4536}, {'loss': 0.0004, 'learning_rate': 6.666666666666667e-06, 'epoch': 22.0, 'step': 4752}, {'eval_loss': 0.5190087556838989, 'eval_precision': 0.294250281848929, 'eval_recall': 0.2976054732041049, 'eval_f1': 0.29591836734693877, 'eval_accuracy': 0.9340693275689029, 'eval_runtime': 2.3656, 'eval_samples_per_second': 647.193, 'eval_steps_per_second': 81.163, 'epoch': 22.0, 'step': 4752}, {'loss': 0.0005, 'learning_rate': 3.3333333333333333e-06, 'epoch': 23.0, 'step': 4968}, {'eval_loss': 0.5239507555961609, 'eval_precision': 0.29607609988109396, 'eval_recall': 0.2839224629418472, 'eval_f1': 0.289871944121071, 'eval_accuracy': 0.9353817648421215, 'eval_runtime': 2.3638, 'eval_samples_per_second': 647.691, 'eval_steps_per_second': 81.226, 'epoch': 23.0, 'step': 4968}, {'loss': 0.0004, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 5184}, {'eval_loss': 0.5243465304374695, 'eval_precision': 0.29308323563892147, 'eval_recall': 0.28506271379703535, 'eval_f1': 0.2890173410404624, 'eval_accuracy': 0.9344553385316143, 'eval_runtime': 2.3636, 'eval_samples_per_second': 647.729, 'eval_steps_per_second': 81.231, 'epoch': 24.0, 'step': 5184}, {'train_runtime': 861.0283, 'train_samples_per_second': 240.661, 'train_steps_per_second': 6.021, 'total_flos': 8233685173548936.0, 'train_loss': 0.039540994754291056, 'epoch': 24.0, 'step': 5184}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9371
  predict_f1                 =     0.1925
  predict_loss               =     0.2301
  predict_precision          =     0.2195
  predict_recall             =     0.1714
  predict_runtime            = 0:00:01.96
  predict_samples_per_second =    647.267
  predict_steps_per_second   =     80.908
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NB-BERT_base_11.json completed. F1: 0.1925133689839572
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_13.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01170944_tsa-intensity_NorBERT_3_large_13.json
01170944_tsa-intensity_NorBERT_3_large Our label2id: {'O': 0, 'B-targ-Negative-0': 1, 'I-targ-Negative-0': 2, 'B-targ-Negative-1': 3, 'I-targ-Negative-1': 4, 'B-targ-Negative-2': 5, 'I-targ-Negative-2': 6, 'B-targ-Negative-3': 7, 'I-targ-Negative-3': 8, 'B-targ-Positive-0': 9, 'I-targ-Positive-0': 10, 'B-targ-Positive-1': 11, 'I-targ-Positive-1': 12, 'B-targ-Positive-2': 13, 'I-targ-Positive-2': 14, 'B-targ-Positive-3': 15, 'I-targ-Positive-3': 16}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 6017.91 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:00, 6763.70 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6712.85 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6800.24 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6856.82 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 6986.04 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 7155.17 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 6260.51 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 6627.37 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 7148.06 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6819.85 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 6912.41 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 6748.41 examples/s]
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
