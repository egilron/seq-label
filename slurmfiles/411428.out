Starting job 411428 on gpu-1 at Thu Jan 25 02:03:40 CET 2024

/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_14.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_14.json
01191518_tsa-bin_NB-BERT_base_14.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_45.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_45.json
01191518_tsa-intensity_NorBERT_3_large_45.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_06.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_06.json
01191518_tsa-bin_NB-BERT_base_06.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_39.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_39.json
01191518_tsa-intensity_NB-BERT_large_39.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_34.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_34.json
01191518_tsa-intensity_NB-BERT_base_34.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_00.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_00.json
01191518_tsa-bin_NorBERT_3_base_00.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_20.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_20.json
01191518_tsa-intensity_NorBERT_3_base_20.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_24.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_24.json
01191518_tsa-bin_NorBERT_3_base_24.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_29.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_29.json
01191518_tsa-intensity_NorBERT_3_large_29.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_25.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_25.json
01191518_tsa-intensity_NorBERT_3_large_25.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_41.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_41.json
01191518_tsa-bin_NorBERT_3_large_41.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_26.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_26.json
01191518_tsa-bin_NB-BERT_base_26.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_45.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_45.json
01191518_tsa-bin_NorBERT_3_large_45.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_26.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_26.json
01191518_tsa-intensity_NB-BERT_base_26.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_24.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_24.json
01191518_tsa-intensity_NorBERT_3_base_24.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_31.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_31.json
01191518_tsa-bin_NB-BERT_large_31.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_35.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_35.json
01191518_tsa-intensity_NB-BERT_large_35.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_20.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_20.json
01191518_tsa-bin_NorBERT_3_base_20.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_09.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_09.json
01191518_tsa-intensity_NorBERT_3_large_09.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_40.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_40.json
01191518_tsa-intensity_NorBERT_3_base_40.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_43.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_43.json
01191518_tsa-intensity_NB-BERT_large_43.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_32.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_32.json
01191518_tsa-intensity_NorBERT_3_base_32.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_23.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_23.json
01191518_tsa-intensity_NB-BERT_large_23.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_07-b.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_07-b.json
01191518_tsa-bin_NB-BERT_large_07-b.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_18.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_18.json
01191518_tsa-intensity_NB-BERT_base_18.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_13.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_13.json
01191518_tsa-bin_NorBERT_3_large_13.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_18.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_18.json
01191518_tsa-bin_NB-BERT_base_18.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_36.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_36.json
01191518_tsa-bin_NorBERT_3_base_36.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_04.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_04.json
01191518_tsa-intensity_NorBERT_3_base_04.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_19.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_19.json
01191518_tsa-bin_NB-BERT_large_19.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_07.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_07.json
01191518_tsa-bin_NB-BERT_large_07.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_44.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_44.json
01191518_tsa-bin_NorBERT_3_base_44.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_30.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_30.json
01191518_tsa-intensity_NB-BERT_base_30.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_07.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_07.json
01191518_tsa-intensity_NB-BERT_large_07.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_03.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_03.json
01191518_tsa-bin_NB-BERT_large_03.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_46.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_46.json
01191518_tsa-bin_NB-BERT_base_46.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_17.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_17.json
01191518_tsa-intensity_NorBERT_3_large_17.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_36.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_36.json
01191518_tsa-intensity_NorBERT_3_base_36.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_35.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_35.json
01191518_tsa-bin_NB-BERT_large_35.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_22.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_22.json
01191518_tsa-intensity_NB-BERT_base_22.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_39.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_39.json
01191518_tsa-bin_NB-BERT_large_39.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_37.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_37.json
01191518_tsa-intensity_NorBERT_3_large_37.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_33.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_33.json
01191518_tsa-intensity_NorBERT_3_large_33.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_01.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_01.json
01191518_tsa-bin_NorBERT_3_large_01.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_08.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_08.json
01191518_tsa-intensity_NorBERT_3_base_08.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_42.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_42.json
01191518_tsa-bin_NB-BERT_base_42.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_41.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_41.json
01191518_tsa-intensity_NorBERT_3_large_41.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_12.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_12.json
01191518_tsa-intensity_NorBERT_3_base_12.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_44.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_44.json
01191518_tsa-intensity_NorBERT_3_base_44.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_04.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_04.json
01191518_tsa-bin_NorBERT_3_base_04.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_03.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_03.json
01191518_tsa-intensity_NB-BERT_large_03.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_37.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_37.json
01191518_tsa-bin_NorBERT_3_large_37.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_25.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_25.json
01191518_tsa-bin_NorBERT_3_large_25.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_31.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_31.json
01191518_tsa-intensity_NB-BERT_large_31.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_46.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_46.json
01191518_tsa-intensity_NB-BERT_base_46.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_12.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_12.json
01191518_tsa-bin_NorBERT_3_base_12.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_02.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_02.json
01191518_tsa-intensity_NB-BERT_base_02.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_14.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_14.json
01191518_tsa-intensity_NB-BERT_base_14.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_16.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_16.json
01191518_tsa-bin_NorBERT_3_base_16.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_16.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_16.json
01191518_tsa-intensity_NorBERT_3_base_16.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_01.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_01.json
01191518_tsa-intensity_NorBERT_3_large_01.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_22.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_22.json
01191518_tsa-bin_NB-BERT_base_22.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_38.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_38.json
01191518_tsa-bin_NB-BERT_base_38.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_28.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_28.json
01191518_tsa-bin_NorBERT_3_base_28.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_00.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_00.json
01191518_tsa-intensity_NorBERT_3_base_00.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_27.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_27.json
01191518_tsa-bin_NB-BERT_large_27.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_05.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_05.json
01191518_tsa-intensity_NorBERT_3_large_05.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_10.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_10.json
01191518_tsa-bin_NB-BERT_base_10.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_30.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_30.json
01191518_tsa-bin_NB-BERT_base_30.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_32.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_32.json
01191518_tsa-bin_NorBERT_3_base_32.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_33.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_33.json
01191518_tsa-bin_NorBERT_3_large_33.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_13.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_13.json
01191518_tsa-intensity_NorBERT_3_large_13.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_06.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_06.json
01191518_tsa-intensity_NB-BERT_base_06.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_08.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_08.json
01191518_tsa-bin_NorBERT_3_base_08.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_38.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_38.json
01191518_tsa-intensity_NB-BERT_base_38.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_05.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_05.json
01191518_tsa-bin_NorBERT_3_large_05.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_09.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_09.json
01191518_tsa-bin_NorBERT_3_large_09.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_47.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_47.json
01191518_tsa-bin_NB-BERT_large_47.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_17.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_17.json
01191518_tsa-bin_NorBERT_3_large_17.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_47.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_47.json
01191518_tsa-intensity_NB-BERT_large_47.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_43.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_43.json
01191518_tsa-bin_NB-BERT_large_43.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_10.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_10.json
01191518_tsa-intensity_NB-BERT_base_10.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_34.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_34.json
01191518_tsa-bin_NB-BERT_base_34.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_19.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_19.json
01191518_tsa-intensity_NB-BERT_large_19.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_29.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_29.json
01191518_tsa-bin_NorBERT_3_large_29.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_15.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_15.json
01191518_tsa-bin_NB-BERT_large_15.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_28.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_base_28.json
01191518_tsa-intensity_NorBERT_3_base_28.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_15.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_15.json
01191518_tsa-intensity_NB-BERT_large_15.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_40.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_base_40.json
01191518_tsa-bin_NorBERT_3_base_40.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_21.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NorBERT_3_large_21.json
01191518_tsa-intensity_NorBERT_3_large_21.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_11.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_11.json
01191518_tsa-intensity_NB-BERT_large_11.json seems to be completed. Exiting
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_42.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_42.json
01191518_tsa-intensity_NB-BERT_base Our label2id: {'O': 0, 'B-targ-Negative-1': 1, 'I-targ-Negative-1': 2, 'B-targ-Negative-2': 3, 'I-targ-Negative-2': 4, 'B-targ-Negative-3': 5, 'I-targ-Negative-3': 6, 'B-targ-Positive-1': 7, 'I-targ-Positive-1': 8, 'B-targ-Positive-2': 9, 'I-targ-Positive-2': 10, 'B-targ-Positive-3': 11, 'I-targ-Positive-3': 12}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 5186.31 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 6152.65 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6243.18 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6437.14 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6454.42 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 6684.91 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 5629.06 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 6039.20 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 6099.97 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 6100.68 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 6828.22 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6451.60 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 6423.44 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 6179.74 examples/s]
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
01191518_tsa-intensity_NB-BERT_base Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.3625, 'learning_rate': 1.916666666666667e-05, 'epoch': 1.0}
{'eval_loss': 0.23723077774047852, 'eval_precision': 0.2045929018789144, 'eval_recall': 0.11174458380843785, 'eval_f1': 0.14454277286135694, 'eval_accuracy': 0.9362695900563576, 'eval_runtime': 2.3661, 'eval_samples_per_second': 647.05, 'eval_steps_per_second': 81.145, 'epoch': 1.0}
{'loss': 0.2262, 'learning_rate': 1.8333333333333333e-05, 'epoch': 2.0}
{'eval_loss': 0.21466363966464996, 'eval_precision': 0.23090586145648312, 'eval_recall': 0.14823261117445838, 'eval_f1': 0.18055555555555558, 'eval_accuracy': 0.9377750328109319, 'eval_runtime': 2.4571, 'eval_samples_per_second': 623.094, 'eval_steps_per_second': 78.141, 'epoch': 2.0}
{'loss': 0.1773, 'learning_rate': 1.7500000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.21426670253276825, 'eval_precision': 0.2783357245337159, 'eval_recall': 0.22120866590649943, 'eval_f1': 0.2465057179161372, 'eval_accuracy': 0.9381996448699143, 'eval_runtime': 2.4949, 'eval_samples_per_second': 613.649, 'eval_steps_per_second': 76.957, 'epoch': 3.0}
{'loss': 0.14, 'learning_rate': 1.6666666666666667e-05, 'epoch': 4.0}
{'eval_loss': 0.23259517550468445, 'eval_precision': 0.258, 'eval_recall': 0.29418472063854045, 'eval_f1': 0.27490676611614284, 'eval_accuracy': 0.9350343549756813, 'eval_runtime': 2.5784, 'eval_samples_per_second': 593.786, 'eval_steps_per_second': 74.466, 'epoch': 4.0}
{'loss': 0.1124, 'learning_rate': 1.5833333333333333e-05, 'epoch': 5.0}
{'eval_loss': 0.2566007971763611, 'eval_precision': 0.26204238921001927, 'eval_recall': 0.31014823261117447, 'eval_f1': 0.2840731070496083, 'eval_accuracy': 0.931791862888906, 'eval_runtime': 2.4817, 'eval_samples_per_second': 616.924, 'eval_steps_per_second': 77.367, 'epoch': 5.0}
{'loss': 0.0925, 'learning_rate': 1.5000000000000002e-05, 'epoch': 6.0}
{'eval_loss': 0.25822117924690247, 'eval_precision': 0.23400191021967526, 'eval_recall': 0.27936145952109465, 'eval_f1': 0.25467775467775466, 'eval_accuracy': 0.9327954913919555, 'eval_runtime': 2.4504, 'eval_samples_per_second': 624.807, 'eval_steps_per_second': 78.356, 'epoch': 6.0}
{'loss': 0.0744, 'learning_rate': 1.416666666666667e-05, 'epoch': 7.0}
{'eval_loss': 0.2832488417625427, 'eval_precision': 0.3033126293995859, 'eval_recall': 0.33409350057012543, 'eval_f1': 0.31795984807379274, 'eval_accuracy': 0.9373504207519494, 'eval_runtime': 2.4936, 'eval_samples_per_second': 613.969, 'eval_steps_per_second': 76.997, 'epoch': 7.0}
{'loss': 0.0601, 'learning_rate': 1.3333333333333333e-05, 'epoch': 8.0}
{'eval_loss': 0.29833051562309265, 'eval_precision': 0.27735849056603773, 'eval_recall': 0.3352337514253136, 'eval_f1': 0.3035622096024781, 'eval_accuracy': 0.9341079286651741, 'eval_runtime': 2.8639, 'eval_samples_per_second': 534.587, 'eval_steps_per_second': 67.042, 'epoch': 8.0}
{'loss': 0.0508, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.31090080738067627, 'eval_precision': 0.2826086956521739, 'eval_recall': 0.3409350057012543, 'eval_f1': 0.3090439276485788, 'eval_accuracy': 0.9337219177024627, 'eval_runtime': 2.6835, 'eval_samples_per_second': 570.533, 'eval_steps_per_second': 71.55, 'epoch': 9.0}
{'loss': 0.0431, 'learning_rate': 1.1666666666666668e-05, 'epoch': 10.0}
{'eval_loss': 0.3180064260959625, 'eval_precision': 0.2865612648221344, 'eval_recall': 0.330672748004561, 'eval_f1': 0.30704076230809957, 'eval_accuracy': 0.9354203659383926, 'eval_runtime': 2.6939, 'eval_samples_per_second': 568.317, 'eval_steps_per_second': 71.272, 'epoch': 10.0}
{'loss': 0.0356, 'learning_rate': 1.0833333333333334e-05, 'epoch': 11.0}
{'eval_loss': 0.340051531791687, 'eval_precision': 0.27520435967302453, 'eval_recall': 0.34549600912200684, 'eval_f1': 0.30637007077856415, 'eval_accuracy': 0.9340693275689029, 'eval_runtime': 2.6888, 'eval_samples_per_second': 569.39, 'eval_steps_per_second': 71.406, 'epoch': 11.0}
{'loss': 0.0302, 'learning_rate': 1e-05, 'epoch': 12.0}
{'eval_loss': 0.34384652972221375, 'eval_precision': 0.28852119958634953, 'eval_recall': 0.3181299885974915, 'eval_f1': 0.30260303687635576, 'eval_accuracy': 0.9357677758048328, 'eval_runtime': 2.4911, 'eval_samples_per_second': 614.577, 'eval_steps_per_second': 77.073, 'epoch': 12.0}
{'loss': 0.0259, 'learning_rate': 9.166666666666666e-06, 'epoch': 13.0}
{'eval_loss': 0.355572909116745, 'eval_precision': 0.282103134479272, 'eval_recall': 0.3181299885974915, 'eval_f1': 0.2990353697749196, 'eval_accuracy': 0.9345325407241566, 'eval_runtime': 2.4984, 'eval_samples_per_second': 612.787, 'eval_steps_per_second': 76.849, 'epoch': 13.0}
{'loss': 0.0227, 'learning_rate': 8.333333333333334e-06, 'epoch': 14.0}
{'eval_loss': 0.37095242738723755, 'eval_precision': 0.29273504273504275, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.302261445118588, 'eval_accuracy': 0.9366169999227978, 'eval_runtime': 2.6036, 'eval_samples_per_second': 588.022, 'eval_steps_per_second': 73.743, 'epoch': 14.0}
{'loss': 0.0198, 'learning_rate': 7.500000000000001e-06, 'epoch': 15.0}
{'eval_loss': 0.37745529413223267, 'eval_precision': 0.27958015267175573, 'eval_recall': 0.33409350057012543, 'eval_f1': 0.3044155844155844, 'eval_accuracy': 0.9335289122211071, 'eval_runtime': 2.4007, 'eval_samples_per_second': 637.734, 'eval_steps_per_second': 79.977, 'epoch': 15.0}
{'loss': 0.017, 'learning_rate': 6.666666666666667e-06, 'epoch': 16.0}
{'eval_loss': 0.3879989683628082, 'eval_precision': 0.27565392354124746, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.2928915018706574, 'eval_accuracy': 0.9344939396278854, 'eval_runtime': 2.66, 'eval_samples_per_second': 575.567, 'eval_steps_per_second': 72.181, 'epoch': 16.0}
{'loss': 0.015, 'learning_rate': 5.833333333333334e-06, 'epoch': 17.0}
{'eval_loss': 0.3911624252796173, 'eval_precision': 0.25555555555555554, 'eval_recall': 0.314709236031927, 'eval_f1': 0.2820643842616249, 'eval_accuracy': 0.9327954913919555, 'eval_runtime': 2.7396, 'eval_samples_per_second': 558.85, 'eval_steps_per_second': 70.084, 'epoch': 17.0}
{'loss': 0.0131, 'learning_rate': 5e-06, 'epoch': 18.0}
{'eval_loss': 0.39326176047325134, 'eval_precision': 0.2793969849246231, 'eval_recall': 0.3169897377423033, 'eval_f1': 0.297008547008547, 'eval_accuracy': 0.9347255462055122, 'eval_runtime': 2.3314, 'eval_samples_per_second': 656.682, 'eval_steps_per_second': 82.353, 'epoch': 18.0}
{'loss': 0.0117, 'learning_rate': 4.166666666666667e-06, 'epoch': 19.0}
{'eval_loss': 0.4022771716117859, 'eval_precision': 0.2765335929892892, 'eval_recall': 0.3238312428734322, 'eval_f1': 0.2983193277310925, 'eval_accuracy': 0.9340307264726319, 'eval_runtime': 2.4684, 'eval_samples_per_second': 620.236, 'eval_steps_per_second': 77.783, 'epoch': 19.0}
{'loss': 0.0112, 'learning_rate': 3.3333333333333333e-06, 'epoch': 20.0}
{'eval_loss': 0.40655291080474854, 'eval_precision': 0.2755980861244019, 'eval_recall': 0.32839224629418473, 'eval_f1': 0.29968782518210196, 'eval_accuracy': 0.9332973056434802, 'eval_runtime': 2.4496, 'eval_samples_per_second': 624.995, 'eval_steps_per_second': 78.38, 'epoch': 20.0}
{'loss': 0.0095, 'learning_rate': 2.5e-06, 'epoch': 21.0}
{'eval_loss': 0.40963348746299744, 'eval_precision': 0.2780586450960566, 'eval_recall': 0.31356898517673887, 'eval_f1': 0.2947481243301179, 'eval_accuracy': 0.9343395352428009, 'eval_runtime': 2.5568, 'eval_samples_per_second': 598.802, 'eval_steps_per_second': 75.095, 'epoch': 21.0}
{'loss': 0.009, 'learning_rate': 1.6666666666666667e-06, 'epoch': 22.0}
{'eval_loss': 0.415143758058548, 'eval_precision': 0.28056872037914693, 'eval_recall': 0.33751425313568983, 'eval_f1': 0.3064182194616977, 'eval_accuracy': 0.9336447155099205, 'eval_runtime': 2.59, 'eval_samples_per_second': 591.124, 'eval_steps_per_second': 74.132, 'epoch': 22.0}
{'loss': 0.0086, 'learning_rate': 8.333333333333333e-07, 'epoch': 23.0}
{'eval_loss': 0.4157745838165283, 'eval_precision': 0.2844400396432111, 'eval_recall': 0.3272519954389966, 'eval_f1': 0.3043478260869565, 'eval_accuracy': 0.9343395352428009, 'eval_runtime': 2.6438, 'eval_samples_per_second': 579.08, 'eval_steps_per_second': 72.621, 'epoch': 23.0}
{'loss': 0.0083, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': 0.4157332181930542, 'eval_precision': 0.28313253012048195, 'eval_recall': 0.3215507411630559, 'eval_f1': 0.30112119594233855, 'eval_accuracy': 0.9346869451092411, 'eval_runtime': 2.5306, 'eval_samples_per_second': 604.997, 'eval_steps_per_second': 75.872, 'epoch': 24.0}
{'train_runtime': 885.0743, 'train_samples_per_second': 234.123, 'train_steps_per_second': 3.661, 'train_loss': 0.0656980221286232, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =     0.0657
  train_runtime            = 0:14:45.07
  train_samples            =       8634
  train_samples_per_second =    234.123
  train_steps_per_second   =      3.661
[{'loss': 0.3625, 'learning_rate': 1.916666666666667e-05, 'epoch': 1.0, 'step': 135}, {'eval_loss': 0.23723077774047852, 'eval_precision': 0.2045929018789144, 'eval_recall': 0.11174458380843785, 'eval_f1': 0.14454277286135694, 'eval_accuracy': 0.9362695900563576, 'eval_runtime': 2.3661, 'eval_samples_per_second': 647.05, 'eval_steps_per_second': 81.145, 'epoch': 1.0, 'step': 135}, {'loss': 0.2262, 'learning_rate': 1.8333333333333333e-05, 'epoch': 2.0, 'step': 270}, {'eval_loss': 0.21466363966464996, 'eval_precision': 0.23090586145648312, 'eval_recall': 0.14823261117445838, 'eval_f1': 0.18055555555555558, 'eval_accuracy': 0.9377750328109319, 'eval_runtime': 2.4571, 'eval_samples_per_second': 623.094, 'eval_steps_per_second': 78.141, 'epoch': 2.0, 'step': 270}, {'loss': 0.1773, 'learning_rate': 1.7500000000000002e-05, 'epoch': 3.0, 'step': 405}, {'eval_loss': 0.21426670253276825, 'eval_precision': 0.2783357245337159, 'eval_recall': 0.22120866590649943, 'eval_f1': 0.2465057179161372, 'eval_accuracy': 0.9381996448699143, 'eval_runtime': 2.4949, 'eval_samples_per_second': 613.649, 'eval_steps_per_second': 76.957, 'epoch': 3.0, 'step': 405}, {'loss': 0.14, 'learning_rate': 1.6666666666666667e-05, 'epoch': 4.0, 'step': 540}, {'eval_loss': 0.23259517550468445, 'eval_precision': 0.258, 'eval_recall': 0.29418472063854045, 'eval_f1': 0.27490676611614284, 'eval_accuracy': 0.9350343549756813, 'eval_runtime': 2.5784, 'eval_samples_per_second': 593.786, 'eval_steps_per_second': 74.466, 'epoch': 4.0, 'step': 540}, {'loss': 0.1124, 'learning_rate': 1.5833333333333333e-05, 'epoch': 5.0, 'step': 675}, {'eval_loss': 0.2566007971763611, 'eval_precision': 0.26204238921001927, 'eval_recall': 0.31014823261117447, 'eval_f1': 0.2840731070496083, 'eval_accuracy': 0.931791862888906, 'eval_runtime': 2.4817, 'eval_samples_per_second': 616.924, 'eval_steps_per_second': 77.367, 'epoch': 5.0, 'step': 675}, {'loss': 0.0925, 'learning_rate': 1.5000000000000002e-05, 'epoch': 6.0, 'step': 810}, {'eval_loss': 0.25822117924690247, 'eval_precision': 0.23400191021967526, 'eval_recall': 0.27936145952109465, 'eval_f1': 0.25467775467775466, 'eval_accuracy': 0.9327954913919555, 'eval_runtime': 2.4504, 'eval_samples_per_second': 624.807, 'eval_steps_per_second': 78.356, 'epoch': 6.0, 'step': 810}, {'loss': 0.0744, 'learning_rate': 1.416666666666667e-05, 'epoch': 7.0, 'step': 945}, {'eval_loss': 0.2832488417625427, 'eval_precision': 0.3033126293995859, 'eval_recall': 0.33409350057012543, 'eval_f1': 0.31795984807379274, 'eval_accuracy': 0.9373504207519494, 'eval_runtime': 2.4936, 'eval_samples_per_second': 613.969, 'eval_steps_per_second': 76.997, 'epoch': 7.0, 'step': 945}, {'loss': 0.0601, 'learning_rate': 1.3333333333333333e-05, 'epoch': 8.0, 'step': 1080}, {'eval_loss': 0.29833051562309265, 'eval_precision': 0.27735849056603773, 'eval_recall': 0.3352337514253136, 'eval_f1': 0.3035622096024781, 'eval_accuracy': 0.9341079286651741, 'eval_runtime': 2.8639, 'eval_samples_per_second': 534.587, 'eval_steps_per_second': 67.042, 'epoch': 8.0, 'step': 1080}, {'loss': 0.0508, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1215}, {'eval_loss': 0.31090080738067627, 'eval_precision': 0.2826086956521739, 'eval_recall': 0.3409350057012543, 'eval_f1': 0.3090439276485788, 'eval_accuracy': 0.9337219177024627, 'eval_runtime': 2.6835, 'eval_samples_per_second': 570.533, 'eval_steps_per_second': 71.55, 'epoch': 9.0, 'step': 1215}, {'loss': 0.0431, 'learning_rate': 1.1666666666666668e-05, 'epoch': 10.0, 'step': 1350}, {'eval_loss': 0.3180064260959625, 'eval_precision': 0.2865612648221344, 'eval_recall': 0.330672748004561, 'eval_f1': 0.30704076230809957, 'eval_accuracy': 0.9354203659383926, 'eval_runtime': 2.6939, 'eval_samples_per_second': 568.317, 'eval_steps_per_second': 71.272, 'epoch': 10.0, 'step': 1350}, {'loss': 0.0356, 'learning_rate': 1.0833333333333334e-05, 'epoch': 11.0, 'step': 1485}, {'eval_loss': 0.340051531791687, 'eval_precision': 0.27520435967302453, 'eval_recall': 0.34549600912200684, 'eval_f1': 0.30637007077856415, 'eval_accuracy': 0.9340693275689029, 'eval_runtime': 2.6888, 'eval_samples_per_second': 569.39, 'eval_steps_per_second': 71.406, 'epoch': 11.0, 'step': 1485}, {'loss': 0.0302, 'learning_rate': 1e-05, 'epoch': 12.0, 'step': 1620}, {'eval_loss': 0.34384652972221375, 'eval_precision': 0.28852119958634953, 'eval_recall': 0.3181299885974915, 'eval_f1': 0.30260303687635576, 'eval_accuracy': 0.9357677758048328, 'eval_runtime': 2.4911, 'eval_samples_per_second': 614.577, 'eval_steps_per_second': 77.073, 'epoch': 12.0, 'step': 1620}, {'loss': 0.0259, 'learning_rate': 9.166666666666666e-06, 'epoch': 13.0, 'step': 1755}, {'eval_loss': 0.355572909116745, 'eval_precision': 0.282103134479272, 'eval_recall': 0.3181299885974915, 'eval_f1': 0.2990353697749196, 'eval_accuracy': 0.9345325407241566, 'eval_runtime': 2.4984, 'eval_samples_per_second': 612.787, 'eval_steps_per_second': 76.849, 'epoch': 13.0, 'step': 1755}, {'loss': 0.0227, 'learning_rate': 8.333333333333334e-06, 'epoch': 14.0, 'step': 1890}, {'eval_loss': 0.37095242738723755, 'eval_precision': 0.29273504273504275, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.302261445118588, 'eval_accuracy': 0.9366169999227978, 'eval_runtime': 2.6036, 'eval_samples_per_second': 588.022, 'eval_steps_per_second': 73.743, 'epoch': 14.0, 'step': 1890}, {'loss': 0.0198, 'learning_rate': 7.500000000000001e-06, 'epoch': 15.0, 'step': 2025}, {'eval_loss': 0.37745529413223267, 'eval_precision': 0.27958015267175573, 'eval_recall': 0.33409350057012543, 'eval_f1': 0.3044155844155844, 'eval_accuracy': 0.9335289122211071, 'eval_runtime': 2.4007, 'eval_samples_per_second': 637.734, 'eval_steps_per_second': 79.977, 'epoch': 15.0, 'step': 2025}, {'loss': 0.017, 'learning_rate': 6.666666666666667e-06, 'epoch': 16.0, 'step': 2160}, {'eval_loss': 0.3879989683628082, 'eval_precision': 0.27565392354124746, 'eval_recall': 0.3124287343215507, 'eval_f1': 0.2928915018706574, 'eval_accuracy': 0.9344939396278854, 'eval_runtime': 2.66, 'eval_samples_per_second': 575.567, 'eval_steps_per_second': 72.181, 'epoch': 16.0, 'step': 2160}, {'loss': 0.015, 'learning_rate': 5.833333333333334e-06, 'epoch': 17.0, 'step': 2295}, {'eval_loss': 0.3911624252796173, 'eval_precision': 0.25555555555555554, 'eval_recall': 0.314709236031927, 'eval_f1': 0.2820643842616249, 'eval_accuracy': 0.9327954913919555, 'eval_runtime': 2.7396, 'eval_samples_per_second': 558.85, 'eval_steps_per_second': 70.084, 'epoch': 17.0, 'step': 2295}, {'loss': 0.0131, 'learning_rate': 5e-06, 'epoch': 18.0, 'step': 2430}, {'eval_loss': 0.39326176047325134, 'eval_precision': 0.2793969849246231, 'eval_recall': 0.3169897377423033, 'eval_f1': 0.297008547008547, 'eval_accuracy': 0.9347255462055122, 'eval_runtime': 2.3314, 'eval_samples_per_second': 656.682, 'eval_steps_per_second': 82.353, 'epoch': 18.0, 'step': 2430}, {'loss': 0.0117, 'learning_rate': 4.166666666666667e-06, 'epoch': 19.0, 'step': 2565}, {'eval_loss': 0.4022771716117859, 'eval_precision': 0.2765335929892892, 'eval_recall': 0.3238312428734322, 'eval_f1': 0.2983193277310925, 'eval_accuracy': 0.9340307264726319, 'eval_runtime': 2.4684, 'eval_samples_per_second': 620.236, 'eval_steps_per_second': 77.783, 'epoch': 19.0, 'step': 2565}, {'loss': 0.0112, 'learning_rate': 3.3333333333333333e-06, 'epoch': 20.0, 'step': 2700}, {'eval_loss': 0.40655291080474854, 'eval_precision': 0.2755980861244019, 'eval_recall': 0.32839224629418473, 'eval_f1': 0.29968782518210196, 'eval_accuracy': 0.9332973056434802, 'eval_runtime': 2.4496, 'eval_samples_per_second': 624.995, 'eval_steps_per_second': 78.38, 'epoch': 20.0, 'step': 2700}, {'loss': 0.0095, 'learning_rate': 2.5e-06, 'epoch': 21.0, 'step': 2835}, {'eval_loss': 0.40963348746299744, 'eval_precision': 0.2780586450960566, 'eval_recall': 0.31356898517673887, 'eval_f1': 0.2947481243301179, 'eval_accuracy': 0.9343395352428009, 'eval_runtime': 2.5568, 'eval_samples_per_second': 598.802, 'eval_steps_per_second': 75.095, 'epoch': 21.0, 'step': 2835}, {'loss': 0.009, 'learning_rate': 1.6666666666666667e-06, 'epoch': 22.0, 'step': 2970}, {'eval_loss': 0.415143758058548, 'eval_precision': 0.28056872037914693, 'eval_recall': 0.33751425313568983, 'eval_f1': 0.3064182194616977, 'eval_accuracy': 0.9336447155099205, 'eval_runtime': 2.59, 'eval_samples_per_second': 591.124, 'eval_steps_per_second': 74.132, 'epoch': 22.0, 'step': 2970}, {'loss': 0.0086, 'learning_rate': 8.333333333333333e-07, 'epoch': 23.0, 'step': 3105}, {'eval_loss': 0.4157745838165283, 'eval_precision': 0.2844400396432111, 'eval_recall': 0.3272519954389966, 'eval_f1': 0.3043478260869565, 'eval_accuracy': 0.9343395352428009, 'eval_runtime': 2.6438, 'eval_samples_per_second': 579.08, 'eval_steps_per_second': 72.621, 'epoch': 23.0, 'step': 3105}, {'loss': 0.0083, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 3240}, {'eval_loss': 0.4157332181930542, 'eval_precision': 0.28313253012048195, 'eval_recall': 0.3215507411630559, 'eval_f1': 0.30112119594233855, 'eval_accuracy': 0.9346869451092411, 'eval_runtime': 2.5306, 'eval_samples_per_second': 604.997, 'eval_steps_per_second': 75.872, 'epoch': 24.0, 'step': 3240}, {'train_runtime': 885.0743, 'train_samples_per_second': 234.123, 'train_steps_per_second': 3.661, 'total_flos': 8936969334926268.0, 'train_loss': 0.0656980221286232, 'epoch': 24.0, 'step': 3240}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9375
  predict_f1                 =     0.2236
  predict_loss               =     0.2215
  predict_precision          =     0.2534
  predict_recall             =        0.2
  predict_runtime            = 0:00:02.21
  predict_samples_per_second =    575.102
  predict_steps_per_second   =     71.888
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_base_42.json completed. F1: 0.22357414448669202
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_27.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_27.json
01191518_tsa-intensity_NB-BERT_large Our label2id: {'O': 0, 'B-targ-Negative-1': 1, 'I-targ-Negative-1': 2, 'B-targ-Negative-2': 3, 'I-targ-Negative-2': 4, 'B-targ-Negative-3': 5, 'I-targ-Negative-3': 6, 'B-targ-Positive-1': 7, 'I-targ-Positive-1': 8, 'B-targ-Positive-2': 9, 'I-targ-Positive-2': 10, 'B-targ-Positive-3': 11, 'I-targ-Positive-3': 12}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 5277.38 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 6497.33 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6709.16 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6922.52 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 7113.36 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 7316.57 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:00<00:00, 7535.35 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 6276.42 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 6699.41 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 7645.05 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 7264.31 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 7348.09 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 7152.90 examples/s]
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
01191518_tsa-intensity_NB-BERT_large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.3304, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.0}
{'eval_loss': 0.21766607463359833, 'eval_precision': 0.22281449893390193, 'eval_recall': 0.23831242873432154, 'eval_f1': 0.2303030303030303, 'eval_accuracy': 0.93669420211534, 'eval_runtime': 4.7818, 'eval_samples_per_second': 320.174, 'eval_steps_per_second': 40.152, 'epoch': 1.0}
{'loss': 0.2044, 'learning_rate': 9.166666666666666e-06, 'epoch': 2.0}
{'eval_loss': 0.19468894600868225, 'eval_precision': 0.2537313432835821, 'eval_recall': 0.21322690992018245, 'eval_f1': 0.23172242874845106, 'eval_accuracy': 0.9415965413417741, 'eval_runtime': 4.9488, 'eval_samples_per_second': 309.371, 'eval_steps_per_second': 38.798, 'epoch': 2.0}
{'loss': 0.1601, 'learning_rate': 8.750000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.19752196967601776, 'eval_precision': 0.30565167243367936, 'eval_recall': 0.30216647662485746, 'eval_f1': 0.3038990825688073, 'eval_accuracy': 0.9415193391492318, 'eval_runtime': 4.9676, 'eval_samples_per_second': 308.198, 'eval_steps_per_second': 38.651, 'epoch': 3.0}
{'loss': 0.1256, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.0}
{'eval_loss': 0.2177705466747284, 'eval_precision': 0.26325581395348835, 'eval_recall': 0.322690992018244, 'eval_f1': 0.2899590163934426, 'eval_accuracy': 0.936115185671273, 'eval_runtime': 4.6652, 'eval_samples_per_second': 328.178, 'eval_steps_per_second': 41.156, 'epoch': 4.0}
{'loss': 0.0991, 'learning_rate': 7.916666666666667e-06, 'epoch': 5.0}
{'eval_loss': 0.22865131497383118, 'eval_precision': 0.3, 'eval_recall': 0.25655644241733183, 'eval_f1': 0.27658266748617083, 'eval_accuracy': 0.9436810005404154, 'eval_runtime': 4.7997, 'eval_samples_per_second': 318.979, 'eval_steps_per_second': 40.003, 'epoch': 5.0}
{'loss': 0.0772, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.0}
{'eval_loss': 0.2537287175655365, 'eval_precision': 0.2740963855421687, 'eval_recall': 0.3112884834663626, 'eval_f1': 0.2915109450080086, 'eval_accuracy': 0.9391260711804216, 'eval_runtime': 4.9731, 'eval_samples_per_second': 307.857, 'eval_steps_per_second': 38.608, 'epoch': 6.0}
{'loss': 0.0621, 'learning_rate': 7.083333333333335e-06, 'epoch': 7.0}
{'eval_loss': 0.26101091504096985, 'eval_precision': 0.29958677685950413, 'eval_recall': 0.330672748004561, 'eval_f1': 0.3143631436314363, 'eval_accuracy': 0.9409017216088937, 'eval_runtime': 5.1054, 'eval_samples_per_second': 299.877, 'eval_steps_per_second': 37.607, 'epoch': 7.0}
{'loss': 0.0506, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.2845684885978699, 'eval_precision': 0.2961210974456008, 'eval_recall': 0.35689851767388825, 'eval_f1': 0.32368148914167527, 'eval_accuracy': 0.93839265035127, 'eval_runtime': 4.7915, 'eval_samples_per_second': 319.521, 'eval_steps_per_second': 40.071, 'epoch': 8.0}
{'loss': 0.0409, 'learning_rate': 6.25e-06, 'epoch': 9.0}
{'eval_loss': 0.30253735184669495, 'eval_precision': 0.3132530120481928, 'eval_recall': 0.3557582668187001, 'eval_f1': 0.3331553657234383, 'eval_accuracy': 0.9399366942021153, 'eval_runtime': 4.8587, 'eval_samples_per_second': 315.106, 'eval_steps_per_second': 39.517, 'epoch': 9.0}
{'loss': 0.031, 'learning_rate': 5.833333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.32049810886383057, 'eval_precision': 0.28882575757575757, 'eval_recall': 0.34777651083238315, 'eval_f1': 0.3155716502845318, 'eval_accuracy': 0.9387014591214391, 'eval_runtime': 4.9131, 'eval_samples_per_second': 311.617, 'eval_steps_per_second': 39.079, 'epoch': 10.0}
{'loss': 0.0268, 'learning_rate': 5.416666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.33157068490982056, 'eval_precision': 0.3227848101265823, 'eval_recall': 0.34891676168757124, 'eval_f1': 0.3353424657534247, 'eval_accuracy': 0.9414421369566895, 'eval_runtime': 4.8722, 'eval_samples_per_second': 314.229, 'eval_steps_per_second': 39.407, 'epoch': 11.0}
{'loss': 0.022, 'learning_rate': 5e-06, 'epoch': 12.0}
{'eval_loss': 0.346794992685318, 'eval_precision': 0.3002028397565923, 'eval_recall': 0.33751425313568983, 'eval_f1': 0.31776704240472353, 'eval_accuracy': 0.9405157106461823, 'eval_runtime': 4.8612, 'eval_samples_per_second': 314.941, 'eval_steps_per_second': 39.496, 'epoch': 12.0}
{'loss': 0.0179, 'learning_rate': 4.583333333333333e-06, 'epoch': 13.0}
{'eval_loss': 0.3599942922592163, 'eval_precision': 0.32858837485172004, 'eval_recall': 0.31584948688711517, 'eval_f1': 0.32209302325581396, 'eval_accuracy': 0.9433721917702462, 'eval_runtime': 4.9809, 'eval_samples_per_second': 307.377, 'eval_steps_per_second': 38.548, 'epoch': 13.0}
{'loss': 0.0154, 'learning_rate': 4.166666666666667e-06, 'epoch': 14.0}
{'eval_loss': 0.37207162380218506, 'eval_precision': 0.32383966244725737, 'eval_recall': 0.3500570125427594, 'eval_f1': 0.33643835616438356, 'eval_accuracy': 0.941287732571605, 'eval_runtime': 5.0258, 'eval_samples_per_second': 304.626, 'eval_steps_per_second': 38.203, 'epoch': 14.0}
{'loss': 0.0124, 'learning_rate': 3.7500000000000005e-06, 'epoch': 15.0}
{'eval_loss': 0.39046892523765564, 'eval_precision': 0.3133047210300429, 'eval_recall': 0.3329532497149373, 'eval_f1': 0.3228302929795467, 'eval_accuracy': 0.9405543117424535, 'eval_runtime': 4.9289, 'eval_samples_per_second': 310.617, 'eval_steps_per_second': 38.954, 'epoch': 15.0}
{'loss': 0.011, 'learning_rate': 3.3333333333333333e-06, 'epoch': 16.0}
{'eval_loss': 0.3911888599395752, 'eval_precision': 0.3057324840764331, 'eval_recall': 0.32839224629418473, 'eval_f1': 0.3166575041231446, 'eval_accuracy': 0.9413263336678762, 'eval_runtime': 4.8318, 'eval_samples_per_second': 316.858, 'eval_steps_per_second': 39.737, 'epoch': 16.0}
{'loss': 0.0086, 'learning_rate': 2.916666666666667e-06, 'epoch': 17.0}
{'eval_loss': 0.4063163101673126, 'eval_precision': 0.3073727933541018, 'eval_recall': 0.33751425313568983, 'eval_f1': 0.32173913043478264, 'eval_accuracy': 0.9395120821431329, 'eval_runtime': 4.68, 'eval_samples_per_second': 327.135, 'eval_steps_per_second': 41.025, 'epoch': 17.0}
{'loss': 0.0078, 'learning_rate': 2.5e-06, 'epoch': 18.0}
{'eval_loss': 0.4145314395427704, 'eval_precision': 0.3033932135728543, 'eval_recall': 0.346636259977195, 'eval_f1': 0.3235763704097924, 'eval_accuracy': 0.9390488689878792, 'eval_runtime': 4.8929, 'eval_samples_per_second': 312.901, 'eval_steps_per_second': 39.24, 'epoch': 18.0}
{'loss': 0.0059, 'learning_rate': 2.0833333333333334e-06, 'epoch': 19.0}
{'eval_loss': 0.42329490184783936, 'eval_precision': 0.30870445344129555, 'eval_recall': 0.34777651083238315, 'eval_f1': 0.3270777479892762, 'eval_accuracy': 0.9402455029722844, 'eval_runtime': 4.9833, 'eval_samples_per_second': 307.228, 'eval_steps_per_second': 38.529, 'epoch': 19.0}
{'loss': 0.0061, 'learning_rate': 1.6666666666666667e-06, 'epoch': 20.0}
{'eval_loss': 0.42456039786338806, 'eval_precision': 0.3089005235602094, 'eval_recall': 0.3363740022805017, 'eval_f1': 0.3220524017467249, 'eval_accuracy': 0.9410947270902493, 'eval_runtime': 4.8407, 'eval_samples_per_second': 316.277, 'eval_steps_per_second': 39.664, 'epoch': 20.0}
{'loss': 0.0051, 'learning_rate': 1.25e-06, 'epoch': 21.0}
{'eval_loss': 0.4332047402858734, 'eval_precision': 0.3143459915611814, 'eval_recall': 0.33979475484606614, 'eval_f1': 0.3265753424657534, 'eval_accuracy': 0.9403227051648266, 'eval_runtime': 4.908, 'eval_samples_per_second': 311.937, 'eval_steps_per_second': 39.119, 'epoch': 21.0}
{'loss': 0.0044, 'learning_rate': 8.333333333333333e-07, 'epoch': 22.0}
{'eval_loss': 0.43527311086654663, 'eval_precision': 0.31671858774662515, 'eval_recall': 0.34777651083238315, 'eval_f1': 0.33152173913043476, 'eval_accuracy': 0.9406315139349958, 'eval_runtime': 4.821, 'eval_samples_per_second': 317.571, 'eval_steps_per_second': 39.826, 'epoch': 22.0}
{'loss': 0.0041, 'learning_rate': 4.1666666666666667e-07, 'epoch': 23.0}
{'eval_loss': 0.4365788996219635, 'eval_precision': 0.32150537634408605, 'eval_recall': 0.3409350057012543, 'eval_f1': 0.33093525179856115, 'eval_accuracy': 0.9412491314753338, 'eval_runtime': 4.8856, 'eval_samples_per_second': 313.367, 'eval_steps_per_second': 39.299, 'epoch': 23.0}
{'loss': 0.0039, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': 0.4371550679206848, 'eval_precision': 0.31735889243876464, 'eval_recall': 0.33979475484606614, 'eval_f1': 0.328193832599119, 'eval_accuracy': 0.940708716127538, 'eval_runtime': 4.8529, 'eval_samples_per_second': 315.483, 'eval_steps_per_second': 39.564, 'epoch': 24.0}
{'train_runtime': 2218.1562, 'train_samples_per_second': 93.418, 'train_steps_per_second': 2.921, 'train_loss': 0.05552831189132031, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =     0.0555
  train_runtime            = 0:36:58.15
  train_samples            =       8634
  train_samples_per_second =     93.418
  train_steps_per_second   =      2.921
[{'loss': 0.3304, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.0, 'step': 270}, {'eval_loss': 0.21766607463359833, 'eval_precision': 0.22281449893390193, 'eval_recall': 0.23831242873432154, 'eval_f1': 0.2303030303030303, 'eval_accuracy': 0.93669420211534, 'eval_runtime': 4.7818, 'eval_samples_per_second': 320.174, 'eval_steps_per_second': 40.152, 'epoch': 1.0, 'step': 270}, {'loss': 0.2044, 'learning_rate': 9.166666666666666e-06, 'epoch': 2.0, 'step': 540}, {'eval_loss': 0.19468894600868225, 'eval_precision': 0.2537313432835821, 'eval_recall': 0.21322690992018245, 'eval_f1': 0.23172242874845106, 'eval_accuracy': 0.9415965413417741, 'eval_runtime': 4.9488, 'eval_samples_per_second': 309.371, 'eval_steps_per_second': 38.798, 'epoch': 2.0, 'step': 540}, {'loss': 0.1601, 'learning_rate': 8.750000000000001e-06, 'epoch': 3.0, 'step': 810}, {'eval_loss': 0.19752196967601776, 'eval_precision': 0.30565167243367936, 'eval_recall': 0.30216647662485746, 'eval_f1': 0.3038990825688073, 'eval_accuracy': 0.9415193391492318, 'eval_runtime': 4.9676, 'eval_samples_per_second': 308.198, 'eval_steps_per_second': 38.651, 'epoch': 3.0, 'step': 810}, {'loss': 0.1256, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.0, 'step': 1080}, {'eval_loss': 0.2177705466747284, 'eval_precision': 0.26325581395348835, 'eval_recall': 0.322690992018244, 'eval_f1': 0.2899590163934426, 'eval_accuracy': 0.936115185671273, 'eval_runtime': 4.6652, 'eval_samples_per_second': 328.178, 'eval_steps_per_second': 41.156, 'epoch': 4.0, 'step': 1080}, {'loss': 0.0991, 'learning_rate': 7.916666666666667e-06, 'epoch': 5.0, 'step': 1350}, {'eval_loss': 0.22865131497383118, 'eval_precision': 0.3, 'eval_recall': 0.25655644241733183, 'eval_f1': 0.27658266748617083, 'eval_accuracy': 0.9436810005404154, 'eval_runtime': 4.7997, 'eval_samples_per_second': 318.979, 'eval_steps_per_second': 40.003, 'epoch': 5.0, 'step': 1350}, {'loss': 0.0772, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.0, 'step': 1620}, {'eval_loss': 0.2537287175655365, 'eval_precision': 0.2740963855421687, 'eval_recall': 0.3112884834663626, 'eval_f1': 0.2915109450080086, 'eval_accuracy': 0.9391260711804216, 'eval_runtime': 4.9731, 'eval_samples_per_second': 307.857, 'eval_steps_per_second': 38.608, 'epoch': 6.0, 'step': 1620}, {'loss': 0.0621, 'learning_rate': 7.083333333333335e-06, 'epoch': 7.0, 'step': 1890}, {'eval_loss': 0.26101091504096985, 'eval_precision': 0.29958677685950413, 'eval_recall': 0.330672748004561, 'eval_f1': 0.3143631436314363, 'eval_accuracy': 0.9409017216088937, 'eval_runtime': 5.1054, 'eval_samples_per_second': 299.877, 'eval_steps_per_second': 37.607, 'epoch': 7.0, 'step': 1890}, {'loss': 0.0506, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0, 'step': 2160}, {'eval_loss': 0.2845684885978699, 'eval_precision': 0.2961210974456008, 'eval_recall': 0.35689851767388825, 'eval_f1': 0.32368148914167527, 'eval_accuracy': 0.93839265035127, 'eval_runtime': 4.7915, 'eval_samples_per_second': 319.521, 'eval_steps_per_second': 40.071, 'epoch': 8.0, 'step': 2160}, {'loss': 0.0409, 'learning_rate': 6.25e-06, 'epoch': 9.0, 'step': 2430}, {'eval_loss': 0.30253735184669495, 'eval_precision': 0.3132530120481928, 'eval_recall': 0.3557582668187001, 'eval_f1': 0.3331553657234383, 'eval_accuracy': 0.9399366942021153, 'eval_runtime': 4.8587, 'eval_samples_per_second': 315.106, 'eval_steps_per_second': 39.517, 'epoch': 9.0, 'step': 2430}, {'loss': 0.031, 'learning_rate': 5.833333333333334e-06, 'epoch': 10.0, 'step': 2700}, {'eval_loss': 0.32049810886383057, 'eval_precision': 0.28882575757575757, 'eval_recall': 0.34777651083238315, 'eval_f1': 0.3155716502845318, 'eval_accuracy': 0.9387014591214391, 'eval_runtime': 4.9131, 'eval_samples_per_second': 311.617, 'eval_steps_per_second': 39.079, 'epoch': 10.0, 'step': 2700}, {'loss': 0.0268, 'learning_rate': 5.416666666666667e-06, 'epoch': 11.0, 'step': 2970}, {'eval_loss': 0.33157068490982056, 'eval_precision': 0.3227848101265823, 'eval_recall': 0.34891676168757124, 'eval_f1': 0.3353424657534247, 'eval_accuracy': 0.9414421369566895, 'eval_runtime': 4.8722, 'eval_samples_per_second': 314.229, 'eval_steps_per_second': 39.407, 'epoch': 11.0, 'step': 2970}, {'loss': 0.022, 'learning_rate': 5e-06, 'epoch': 12.0, 'step': 3240}, {'eval_loss': 0.346794992685318, 'eval_precision': 0.3002028397565923, 'eval_recall': 0.33751425313568983, 'eval_f1': 0.31776704240472353, 'eval_accuracy': 0.9405157106461823, 'eval_runtime': 4.8612, 'eval_samples_per_second': 314.941, 'eval_steps_per_second': 39.496, 'epoch': 12.0, 'step': 3240}, {'loss': 0.0179, 'learning_rate': 4.583333333333333e-06, 'epoch': 13.0, 'step': 3510}, {'eval_loss': 0.3599942922592163, 'eval_precision': 0.32858837485172004, 'eval_recall': 0.31584948688711517, 'eval_f1': 0.32209302325581396, 'eval_accuracy': 0.9433721917702462, 'eval_runtime': 4.9809, 'eval_samples_per_second': 307.377, 'eval_steps_per_second': 38.548, 'epoch': 13.0, 'step': 3510}, {'loss': 0.0154, 'learning_rate': 4.166666666666667e-06, 'epoch': 14.0, 'step': 3780}, {'eval_loss': 0.37207162380218506, 'eval_precision': 0.32383966244725737, 'eval_recall': 0.3500570125427594, 'eval_f1': 0.33643835616438356, 'eval_accuracy': 0.941287732571605, 'eval_runtime': 5.0258, 'eval_samples_per_second': 304.626, 'eval_steps_per_second': 38.203, 'epoch': 14.0, 'step': 3780}, {'loss': 0.0124, 'learning_rate': 3.7500000000000005e-06, 'epoch': 15.0, 'step': 4050}, {'eval_loss': 0.39046892523765564, 'eval_precision': 0.3133047210300429, 'eval_recall': 0.3329532497149373, 'eval_f1': 0.3228302929795467, 'eval_accuracy': 0.9405543117424535, 'eval_runtime': 4.9289, 'eval_samples_per_second': 310.617, 'eval_steps_per_second': 38.954, 'epoch': 15.0, 'step': 4050}, {'loss': 0.011, 'learning_rate': 3.3333333333333333e-06, 'epoch': 16.0, 'step': 4320}, {'eval_loss': 0.3911888599395752, 'eval_precision': 0.3057324840764331, 'eval_recall': 0.32839224629418473, 'eval_f1': 0.3166575041231446, 'eval_accuracy': 0.9413263336678762, 'eval_runtime': 4.8318, 'eval_samples_per_second': 316.858, 'eval_steps_per_second': 39.737, 'epoch': 16.0, 'step': 4320}, {'loss': 0.0086, 'learning_rate': 2.916666666666667e-06, 'epoch': 17.0, 'step': 4590}, {'eval_loss': 0.4063163101673126, 'eval_precision': 0.3073727933541018, 'eval_recall': 0.33751425313568983, 'eval_f1': 0.32173913043478264, 'eval_accuracy': 0.9395120821431329, 'eval_runtime': 4.68, 'eval_samples_per_second': 327.135, 'eval_steps_per_second': 41.025, 'epoch': 17.0, 'step': 4590}, {'loss': 0.0078, 'learning_rate': 2.5e-06, 'epoch': 18.0, 'step': 4860}, {'eval_loss': 0.4145314395427704, 'eval_precision': 0.3033932135728543, 'eval_recall': 0.346636259977195, 'eval_f1': 0.3235763704097924, 'eval_accuracy': 0.9390488689878792, 'eval_runtime': 4.8929, 'eval_samples_per_second': 312.901, 'eval_steps_per_second': 39.24, 'epoch': 18.0, 'step': 4860}, {'loss': 0.0059, 'learning_rate': 2.0833333333333334e-06, 'epoch': 19.0, 'step': 5130}, {'eval_loss': 0.42329490184783936, 'eval_precision': 0.30870445344129555, 'eval_recall': 0.34777651083238315, 'eval_f1': 0.3270777479892762, 'eval_accuracy': 0.9402455029722844, 'eval_runtime': 4.9833, 'eval_samples_per_second': 307.228, 'eval_steps_per_second': 38.529, 'epoch': 19.0, 'step': 5130}, {'loss': 0.0061, 'learning_rate': 1.6666666666666667e-06, 'epoch': 20.0, 'step': 5400}, {'eval_loss': 0.42456039786338806, 'eval_precision': 0.3089005235602094, 'eval_recall': 0.3363740022805017, 'eval_f1': 0.3220524017467249, 'eval_accuracy': 0.9410947270902493, 'eval_runtime': 4.8407, 'eval_samples_per_second': 316.277, 'eval_steps_per_second': 39.664, 'epoch': 20.0, 'step': 5400}, {'loss': 0.0051, 'learning_rate': 1.25e-06, 'epoch': 21.0, 'step': 5670}, {'eval_loss': 0.4332047402858734, 'eval_precision': 0.3143459915611814, 'eval_recall': 0.33979475484606614, 'eval_f1': 0.3265753424657534, 'eval_accuracy': 0.9403227051648266, 'eval_runtime': 4.908, 'eval_samples_per_second': 311.937, 'eval_steps_per_second': 39.119, 'epoch': 21.0, 'step': 5670}, {'loss': 0.0044, 'learning_rate': 8.333333333333333e-07, 'epoch': 22.0, 'step': 5940}, {'eval_loss': 0.43527311086654663, 'eval_precision': 0.31671858774662515, 'eval_recall': 0.34777651083238315, 'eval_f1': 0.33152173913043476, 'eval_accuracy': 0.9406315139349958, 'eval_runtime': 4.821, 'eval_samples_per_second': 317.571, 'eval_steps_per_second': 39.826, 'epoch': 22.0, 'step': 5940}, {'loss': 0.0041, 'learning_rate': 4.1666666666666667e-07, 'epoch': 23.0, 'step': 6210}, {'eval_loss': 0.4365788996219635, 'eval_precision': 0.32150537634408605, 'eval_recall': 0.3409350057012543, 'eval_f1': 0.33093525179856115, 'eval_accuracy': 0.9412491314753338, 'eval_runtime': 4.8856, 'eval_samples_per_second': 313.367, 'eval_steps_per_second': 39.299, 'epoch': 23.0, 'step': 6210}, {'loss': 0.0039, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 6480}, {'eval_loss': 0.4371550679206848, 'eval_precision': 0.31735889243876464, 'eval_recall': 0.33979475484606614, 'eval_f1': 0.328193832599119, 'eval_accuracy': 0.940708716127538, 'eval_runtime': 4.8529, 'eval_samples_per_second': 315.483, 'eval_steps_per_second': 39.564, 'epoch': 24.0, 'step': 6480}, {'train_runtime': 2218.1562, 'train_samples_per_second': 93.418, 'train_steps_per_second': 2.921, 'total_flos': 2.2848926950165572e+16, 'train_loss': 0.05552831189132031, 'epoch': 24.0, 'step': 6480}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9427
  predict_f1                 =     0.2454
  predict_loss               =     0.2024
  predict_precision          =     0.2705
  predict_recall             =     0.2245
  predict_runtime            = 0:00:04.24
  predict_samples_per_second =    299.376
  predict_steps_per_second   =     37.422
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-intensity_NB-BERT_large_27.json completed. F1: 0.24535315985130113
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_21.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_21.json
01191518_tsa-bin_NorBERT_3_large Our label2id: {'O': 0, 'B-targ-Negative': 1, 'I-targ-Negative': 2, 'B-targ-Positive': 3, 'I-targ-Positive': 4}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 5111.68 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 6001.12 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6029.61 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6171.11 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6238.71 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 6437.36 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 6637.65 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 5555.29 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 5947.82 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 6391.41 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6156.16 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 6328.62 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 5978.00 examples/s]
You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/homes01/u01/ec-egilron/venvs/transformers/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
01191518_tsa-bin_NorBERT_3_large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9686, 'eval_samples_per_second': 256.509, 'eval_steps_per_second': 32.168, 'epoch': 1.0}
{'loss': 0.0, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0925, 'eval_samples_per_second': 251.291, 'eval_steps_per_second': 31.514, 'epoch': 2.0}
{'loss': 0.0, 'learning_rate': 4.375e-05, 'epoch': 3.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8382, 'eval_samples_per_second': 262.239, 'eval_steps_per_second': 32.887, 'epoch': 3.0}
{'loss': 0.0, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9001, 'eval_samples_per_second': 259.489, 'eval_steps_per_second': 32.542, 'epoch': 4.0}
{'loss': 0.0, 'learning_rate': 3.958333333333333e-05, 'epoch': 5.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9042, 'eval_samples_per_second': 259.305, 'eval_steps_per_second': 32.519, 'epoch': 5.0}
{'loss': 0.0, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9243, 'eval_samples_per_second': 258.426, 'eval_steps_per_second': 32.409, 'epoch': 6.0}
{'loss': 0.0, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.1199, 'eval_samples_per_second': 250.169, 'eval_steps_per_second': 31.373, 'epoch': 7.0}
{'loss': 0.0, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0578, 'eval_samples_per_second': 252.733, 'eval_steps_per_second': 31.695, 'epoch': 8.0}
{'loss': 0.0, 'learning_rate': 3.125e-05, 'epoch': 9.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.7563, 'eval_samples_per_second': 226.602, 'eval_steps_per_second': 28.418, 'epoch': 9.0}
{'loss': 0.0, 'learning_rate': 2.916666666666667e-05, 'epoch': 10.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.2391, 'eval_samples_per_second': 245.388, 'eval_steps_per_second': 30.774, 'epoch': 10.0}
{'loss': 0.0, 'learning_rate': 2.7083333333333332e-05, 'epoch': 11.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8446, 'eval_samples_per_second': 261.95, 'eval_steps_per_second': 32.851, 'epoch': 11.0}
{'loss': 0.0, 'learning_rate': 2.5e-05, 'epoch': 12.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0422, 'eval_samples_per_second': 253.386, 'eval_steps_per_second': 31.777, 'epoch': 12.0}
{'loss': 0.0, 'learning_rate': 2.2916666666666667e-05, 'epoch': 13.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8083, 'eval_samples_per_second': 263.589, 'eval_steps_per_second': 33.056, 'epoch': 13.0}
{'loss': 0.0, 'learning_rate': 2.0833333333333336e-05, 'epoch': 14.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9112, 'eval_samples_per_second': 259.0, 'eval_steps_per_second': 32.481, 'epoch': 14.0}
{'loss': 0.0, 'learning_rate': 1.8750000000000002e-05, 'epoch': 15.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0586, 'eval_samples_per_second': 252.7, 'eval_steps_per_second': 31.691, 'epoch': 15.0}
{'loss': 0.0, 'learning_rate': 1.6666666666666667e-05, 'epoch': 16.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9168, 'eval_samples_per_second': 258.756, 'eval_steps_per_second': 32.45, 'epoch': 16.0}
{'loss': 0.0, 'learning_rate': 1.4583333333333335e-05, 'epoch': 17.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9054, 'eval_samples_per_second': 259.256, 'eval_steps_per_second': 32.513, 'epoch': 17.0}
{'loss': 0.0, 'learning_rate': 1.25e-05, 'epoch': 18.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.2539, 'eval_samples_per_second': 244.806, 'eval_steps_per_second': 30.701, 'epoch': 18.0}
{'loss': 0.0, 'learning_rate': 1.0416666666666668e-05, 'epoch': 19.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0086, 'eval_samples_per_second': 254.802, 'eval_steps_per_second': 31.954, 'epoch': 19.0}
{'loss': 0.0, 'learning_rate': 8.333333333333334e-06, 'epoch': 20.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.043, 'eval_samples_per_second': 253.35, 'eval_steps_per_second': 31.772, 'epoch': 20.0}
{'loss': 0.0, 'learning_rate': 6.25e-06, 'epoch': 21.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9831, 'eval_samples_per_second': 255.886, 'eval_steps_per_second': 32.09, 'epoch': 21.0}
{'loss': 0.0, 'learning_rate': 4.166666666666667e-06, 'epoch': 22.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8985, 'eval_samples_per_second': 259.558, 'eval_steps_per_second': 32.551, 'epoch': 22.0}
{'loss': 0.0, 'learning_rate': 2.0833333333333334e-06, 'epoch': 23.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8737, 'eval_samples_per_second': 260.652, 'eval_steps_per_second': 32.688, 'epoch': 23.0}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8287, 'eval_samples_per_second': 262.667, 'eval_steps_per_second': 32.941, 'epoch': 24.0}
{'train_runtime': 2741.0654, 'train_samples_per_second': 75.597, 'train_steps_per_second': 4.728, 'train_loss': 0.0, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =        0.0
  train_runtime            = 0:45:41.06
  train_samples            =       8634
  train_samples_per_second =     75.597
  train_steps_per_second   =      4.728
[{'loss': 0.0, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.0, 'step': 540}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9686, 'eval_samples_per_second': 256.509, 'eval_steps_per_second': 32.168, 'epoch': 1.0, 'step': 540}, {'loss': 0.0, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.0, 'step': 1080}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0925, 'eval_samples_per_second': 251.291, 'eval_steps_per_second': 31.514, 'epoch': 2.0, 'step': 1080}, {'loss': 0.0, 'learning_rate': 4.375e-05, 'epoch': 3.0, 'step': 1620}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8382, 'eval_samples_per_second': 262.239, 'eval_steps_per_second': 32.887, 'epoch': 3.0, 'step': 1620}, {'loss': 0.0, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.0, 'step': 2160}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9001, 'eval_samples_per_second': 259.489, 'eval_steps_per_second': 32.542, 'epoch': 4.0, 'step': 2160}, {'loss': 0.0, 'learning_rate': 3.958333333333333e-05, 'epoch': 5.0, 'step': 2700}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9042, 'eval_samples_per_second': 259.305, 'eval_steps_per_second': 32.519, 'epoch': 5.0, 'step': 2700}, {'loss': 0.0, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.0, 'step': 3240}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9243, 'eval_samples_per_second': 258.426, 'eval_steps_per_second': 32.409, 'epoch': 6.0, 'step': 3240}, {'loss': 0.0, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.0, 'step': 3780}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.1199, 'eval_samples_per_second': 250.169, 'eval_steps_per_second': 31.373, 'epoch': 7.0, 'step': 3780}, {'loss': 0.0, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.0, 'step': 4320}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0578, 'eval_samples_per_second': 252.733, 'eval_steps_per_second': 31.695, 'epoch': 8.0, 'step': 4320}, {'loss': 0.0, 'learning_rate': 3.125e-05, 'epoch': 9.0, 'step': 4860}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.7563, 'eval_samples_per_second': 226.602, 'eval_steps_per_second': 28.418, 'epoch': 9.0, 'step': 4860}, {'loss': 0.0, 'learning_rate': 2.916666666666667e-05, 'epoch': 10.0, 'step': 5400}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.2391, 'eval_samples_per_second': 245.388, 'eval_steps_per_second': 30.774, 'epoch': 10.0, 'step': 5400}, {'loss': 0.0, 'learning_rate': 2.7083333333333332e-05, 'epoch': 11.0, 'step': 5940}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8446, 'eval_samples_per_second': 261.95, 'eval_steps_per_second': 32.851, 'epoch': 11.0, 'step': 5940}, {'loss': 0.0, 'learning_rate': 2.5e-05, 'epoch': 12.0, 'step': 6480}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0422, 'eval_samples_per_second': 253.386, 'eval_steps_per_second': 31.777, 'epoch': 12.0, 'step': 6480}, {'loss': 0.0, 'learning_rate': 2.2916666666666667e-05, 'epoch': 13.0, 'step': 7020}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8083, 'eval_samples_per_second': 263.589, 'eval_steps_per_second': 33.056, 'epoch': 13.0, 'step': 7020}, {'loss': 0.0, 'learning_rate': 2.0833333333333336e-05, 'epoch': 14.0, 'step': 7560}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9112, 'eval_samples_per_second': 259.0, 'eval_steps_per_second': 32.481, 'epoch': 14.0, 'step': 7560}, {'loss': 0.0, 'learning_rate': 1.8750000000000002e-05, 'epoch': 15.0, 'step': 8100}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0586, 'eval_samples_per_second': 252.7, 'eval_steps_per_second': 31.691, 'epoch': 15.0, 'step': 8100}, {'loss': 0.0, 'learning_rate': 1.6666666666666667e-05, 'epoch': 16.0, 'step': 8640}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9168, 'eval_samples_per_second': 258.756, 'eval_steps_per_second': 32.45, 'epoch': 16.0, 'step': 8640}, {'loss': 0.0, 'learning_rate': 1.4583333333333335e-05, 'epoch': 17.0, 'step': 9180}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9054, 'eval_samples_per_second': 259.256, 'eval_steps_per_second': 32.513, 'epoch': 17.0, 'step': 9180}, {'loss': 0.0, 'learning_rate': 1.25e-05, 'epoch': 18.0, 'step': 9720}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.2539, 'eval_samples_per_second': 244.806, 'eval_steps_per_second': 30.701, 'epoch': 18.0, 'step': 9720}, {'loss': 0.0, 'learning_rate': 1.0416666666666668e-05, 'epoch': 19.0, 'step': 10260}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.0086, 'eval_samples_per_second': 254.802, 'eval_steps_per_second': 31.954, 'epoch': 19.0, 'step': 10260}, {'loss': 0.0, 'learning_rate': 8.333333333333334e-06, 'epoch': 20.0, 'step': 10800}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 6.043, 'eval_samples_per_second': 253.35, 'eval_steps_per_second': 31.772, 'epoch': 20.0, 'step': 10800}, {'loss': 0.0, 'learning_rate': 6.25e-06, 'epoch': 21.0, 'step': 11340}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.9831, 'eval_samples_per_second': 255.886, 'eval_steps_per_second': 32.09, 'epoch': 21.0, 'step': 11340}, {'loss': 0.0, 'learning_rate': 4.166666666666667e-06, 'epoch': 22.0, 'step': 11880}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8985, 'eval_samples_per_second': 259.558, 'eval_steps_per_second': 32.551, 'epoch': 22.0, 'step': 11880}, {'loss': 0.0, 'learning_rate': 2.0833333333333334e-06, 'epoch': 23.0, 'step': 12420}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8737, 'eval_samples_per_second': 260.652, 'eval_steps_per_second': 32.688, 'epoch': 23.0, 'step': 12420}, {'loss': 0.0, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 12960}, {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.9341851308577164, 'eval_runtime': 5.8287, 'eval_samples_per_second': 262.667, 'eval_steps_per_second': 32.941, 'epoch': 24.0, 'step': 12960}, {'train_runtime': 2741.0654, 'train_samples_per_second': 75.597, 'train_steps_per_second': 4.728, 'total_flos': 2.054990708407374e+16, 'train_loss': 0.0, 'epoch': 24.0, 'step': 12960}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9327
  predict_f1                 =        0.0
  predict_loss               =        nan
  predict_precision          =        0.0
  predict_recall             =        0.0
  predict_runtime            = 0:00:04.97
  predict_samples_per_second =    255.683
  predict_steps_per_second   =      31.96
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NorBERT_3_large_21.json completed. F1: 0.0
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_02.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_02.json
01191518_tsa-bin_NB-BERT_base Our label2id: {'O': 0, 'B-targ-Negative': 1, 'I-targ-Negative': 2, 'B-targ-Positive': 3, 'I-targ-Positive': 4}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 5017.18 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 6019.03 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6172.07 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6279.32 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6450.60 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 6581.27 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 5415.20 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 5794.98 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 5972.16 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 6730.89 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6338.59 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 6349.13 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 6095.69 examples/s]
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
01191518_tsa-bin_NB-BERT_base Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.2094, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.0}
{'eval_loss': 0.15972205996513367, 'eval_precision': 0.43703703703703706, 'eval_recall': 0.4709236031927024, 'eval_f1': 0.45334796926454446, 'eval_accuracy': 0.9471550992048174, 'eval_runtime': 2.5381, 'eval_samples_per_second': 603.207, 'eval_steps_per_second': 75.647, 'epoch': 1.0}
{'loss': 0.1383, 'learning_rate': 9.166666666666666e-06, 'epoch': 2.0}
{'eval_loss': 0.15685197710990906, 'eval_precision': 0.5453237410071943, 'eval_recall': 0.4321550741163056, 'eval_f1': 0.4821882951653944, 'eval_accuracy': 0.9524820504902339, 'eval_runtime': 2.568, 'eval_samples_per_second': 596.194, 'eval_steps_per_second': 74.768, 'epoch': 2.0}
{'loss': 0.0981, 'learning_rate': 8.750000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.164340540766716, 'eval_precision': 0.49733759318423854, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5143171806167401, 'eval_accuracy': 0.9494325638848143, 'eval_runtime': 2.6105, 'eval_samples_per_second': 586.487, 'eval_steps_per_second': 73.55, 'epoch': 3.0}
{'loss': 0.0683, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.0}
{'eval_loss': 0.20289303362369537, 'eval_precision': 0.47347347347347346, 'eval_recall': 0.5393386545039909, 'eval_f1': 0.5042643923240938, 'eval_accuracy': 0.9458040608353278, 'eval_runtime': 2.6695, 'eval_samples_per_second': 573.521, 'eval_steps_per_second': 71.924, 'epoch': 4.0}
{'loss': 0.0475, 'learning_rate': 7.916666666666667e-06, 'epoch': 5.0}
{'eval_loss': 0.2261238545179367, 'eval_precision': 0.4868559411146162, 'eval_recall': 0.5279361459521095, 'eval_f1': 0.5065645514223195, 'eval_accuracy': 0.9482359299004092, 'eval_runtime': 2.4941, 'eval_samples_per_second': 613.843, 'eval_steps_per_second': 76.981, 'epoch': 5.0}
{'loss': 0.0336, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.0}
{'eval_loss': 0.2532905638217926, 'eval_precision': 0.5158264947245017, 'eval_recall': 0.5017103762827823, 'eval_f1': 0.508670520231214, 'eval_accuracy': 0.9498571759437968, 'eval_runtime': 2.6618, 'eval_samples_per_second': 575.166, 'eval_steps_per_second': 72.131, 'epoch': 6.0}
{'loss': 0.0219, 'learning_rate': 7.083333333333335e-06, 'epoch': 7.0}
{'eval_loss': 0.28981101512908936, 'eval_precision': 0.45471014492753625, 'eval_recall': 0.572405929304447, 'eval_f1': 0.5068147400302878, 'eval_accuracy': 0.9446460279471937, 'eval_runtime': 2.3255, 'eval_samples_per_second': 658.348, 'eval_steps_per_second': 82.562, 'epoch': 7.0}
{'loss': 0.0179, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.2931973338127136, 'eval_precision': 0.48443983402489627, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5073329712112982, 'eval_accuracy': 0.9483903342854937, 'eval_runtime': 2.5642, 'eval_samples_per_second': 597.072, 'eval_steps_per_second': 74.878, 'epoch': 8.0}
{'loss': 0.0133, 'learning_rate': 6.25e-06, 'epoch': 9.0}
{'eval_loss': 0.3248162567615509, 'eval_precision': 0.49025641025641026, 'eval_recall': 0.5450399087799316, 'eval_f1': 0.5161987041036717, 'eval_accuracy': 0.9495869682698989, 'eval_runtime': 2.387, 'eval_samples_per_second': 641.403, 'eval_steps_per_second': 80.437, 'epoch': 9.0}
{'loss': 0.0117, 'learning_rate': 5.833333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.3346257507801056, 'eval_precision': 0.47711088504577825, 'eval_recall': 0.5347776510832383, 'eval_f1': 0.5043010752688171, 'eval_accuracy': 0.9475025090712577, 'eval_runtime': 2.4569, 'eval_samples_per_second': 623.134, 'eval_steps_per_second': 78.146, 'epoch': 10.0}
{'loss': 0.008, 'learning_rate': 5.416666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.3535896837711334, 'eval_precision': 0.4742967992240543, 'eval_recall': 0.5575826681870011, 'eval_f1': 0.5125786163522013, 'eval_accuracy': 0.9486219408631205, 'eval_runtime': 2.3344, 'eval_samples_per_second': 655.854, 'eval_steps_per_second': 82.25, 'epoch': 11.0}
{'loss': 0.0075, 'learning_rate': 5e-06, 'epoch': 12.0}
{'eval_loss': 0.372742623090744, 'eval_precision': 0.49376299376299376, 'eval_recall': 0.5416191562143672, 'eval_f1': 0.5165851005981511, 'eval_accuracy': 0.9492009573071876, 'eval_runtime': 2.449, 'eval_samples_per_second': 625.16, 'eval_steps_per_second': 78.4, 'epoch': 12.0}
{'loss': 0.0055, 'learning_rate': 4.583333333333333e-06, 'epoch': 13.0}
{'eval_loss': 0.39421698451042175, 'eval_precision': 0.5187713310580204, 'eval_recall': 0.5199543899657925, 'eval_f1': 0.5193621867881548, 'eval_accuracy': 0.9510924110244731, 'eval_runtime': 2.6134, 'eval_samples_per_second': 585.831, 'eval_steps_per_second': 73.468, 'epoch': 13.0}
{'loss': 0.0038, 'learning_rate': 4.166666666666667e-06, 'epoch': 14.0}
{'eval_loss': 0.40183690190315247, 'eval_precision': 0.5185185185185185, 'eval_recall': 0.5108323831242874, 'eval_f1': 0.5146467547386561, 'eval_accuracy': 0.9512468154095576, 'eval_runtime': 2.4557, 'eval_samples_per_second': 623.456, 'eval_steps_per_second': 78.186, 'epoch': 14.0}
{'loss': 0.0046, 'learning_rate': 3.7500000000000005e-06, 'epoch': 15.0}
{'eval_loss': 0.40451306104660034, 'eval_precision': 0.49527806925498424, 'eval_recall': 0.5381984036488028, 'eval_f1': 0.5158469945355191, 'eval_accuracy': 0.9500501814251525, 'eval_runtime': 2.491, 'eval_samples_per_second': 614.62, 'eval_steps_per_second': 77.078, 'epoch': 15.0}
{'loss': 0.0029, 'learning_rate': 3.3333333333333333e-06, 'epoch': 16.0}
{'eval_loss': 0.4149148464202881, 'eval_precision': 0.5168918918918919, 'eval_recall': 0.5233751425313569, 'eval_f1': 0.520113314447592, 'eval_accuracy': 0.9513240176021, 'eval_runtime': 2.5466, 'eval_samples_per_second': 601.191, 'eval_steps_per_second': 75.394, 'epoch': 16.0}
{'loss': 0.0031, 'learning_rate': 2.916666666666667e-06, 'epoch': 17.0}
{'eval_loss': 0.41380545496940613, 'eval_precision': 0.5168408826945412, 'eval_recall': 0.507411630558723, 'eval_f1': 0.5120828538550058, 'eval_accuracy': 0.9519030340461669, 'eval_runtime': 2.4993, 'eval_samples_per_second': 612.581, 'eval_steps_per_second': 76.823, 'epoch': 17.0}
{'loss': 0.002, 'learning_rate': 2.5e-06, 'epoch': 18.0}
{'eval_loss': 0.4236869513988495, 'eval_precision': 0.5010869565217392, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5130773511407902, 'eval_accuracy': 0.9498571759437968, 'eval_runtime': 2.58, 'eval_samples_per_second': 593.407, 'eval_steps_per_second': 74.418, 'epoch': 18.0}
{'loss': 0.0019, 'learning_rate': 2.0833333333333334e-06, 'epoch': 19.0}
{'eval_loss': 0.4356597363948822, 'eval_precision': 0.4962805526036132, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5137513751375138, 'eval_accuracy': 0.9490079518258319, 'eval_runtime': 2.7572, 'eval_samples_per_second': 555.28, 'eval_steps_per_second': 69.637, 'epoch': 19.0}
{'loss': 0.0019, 'learning_rate': 1.6666666666666667e-06, 'epoch': 20.0}
{'eval_loss': 0.4274821877479553, 'eval_precision': 0.48194014447884415, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.505958829902492, 'eval_accuracy': 0.9488921485370184, 'eval_runtime': 2.5931, 'eval_samples_per_second': 590.414, 'eval_steps_per_second': 74.043, 'epoch': 20.0}
{'loss': 0.0015, 'learning_rate': 1.25e-06, 'epoch': 21.0}
{'eval_loss': 0.4277479350566864, 'eval_precision': 0.5010373443983402, 'eval_recall': 0.5507411630558723, 'eval_f1': 0.5247148288973383, 'eval_accuracy': 0.9500115803288813, 'eval_runtime': 2.5921, 'eval_samples_per_second': 590.643, 'eval_steps_per_second': 74.072, 'epoch': 21.0}
{'loss': 0.001, 'learning_rate': 8.333333333333333e-07, 'epoch': 22.0}
{'eval_loss': 0.4313080906867981, 'eval_precision': 0.5159165751920965, 'eval_recall': 0.5359179019384265, 'eval_f1': 0.5257270693512304, 'eval_accuracy': 0.9506291978692195, 'eval_runtime': 2.8617, 'eval_samples_per_second': 534.991, 'eval_steps_per_second': 67.092, 'epoch': 22.0}
{'loss': 0.001, 'learning_rate': 4.1666666666666667e-07, 'epoch': 23.0}
{'eval_loss': 0.432518869638443, 'eval_precision': 0.4989270386266094, 'eval_recall': 0.5302166476624858, 'eval_f1': 0.5140961857379768, 'eval_accuracy': 0.9508222033505751, 'eval_runtime': 2.5086, 'eval_samples_per_second': 610.304, 'eval_steps_per_second': 76.537, 'epoch': 23.0}
{'loss': 0.001, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': 0.43199750781059265, 'eval_precision': 0.5005417118093174, 'eval_recall': 0.5267958950969214, 'eval_f1': 0.5133333333333333, 'eval_accuracy': 0.950474793484135, 'eval_runtime': 2.5068, 'eval_samples_per_second': 610.732, 'eval_steps_per_second': 76.591, 'epoch': 24.0}
{'train_runtime': 1589.7848, 'train_samples_per_second': 130.342, 'train_steps_per_second': 16.304, 'train_loss': 0.029394966704242024, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =     0.0294
  train_runtime            = 0:26:29.78
  train_samples            =       8634
  train_samples_per_second =    130.342
  train_steps_per_second   =     16.304
[{'loss': 0.2094, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.0, 'step': 1080}, {'eval_loss': 0.15972205996513367, 'eval_precision': 0.43703703703703706, 'eval_recall': 0.4709236031927024, 'eval_f1': 0.45334796926454446, 'eval_accuracy': 0.9471550992048174, 'eval_runtime': 2.5381, 'eval_samples_per_second': 603.207, 'eval_steps_per_second': 75.647, 'epoch': 1.0, 'step': 1080}, {'loss': 0.1383, 'learning_rate': 9.166666666666666e-06, 'epoch': 2.0, 'step': 2160}, {'eval_loss': 0.15685197710990906, 'eval_precision': 0.5453237410071943, 'eval_recall': 0.4321550741163056, 'eval_f1': 0.4821882951653944, 'eval_accuracy': 0.9524820504902339, 'eval_runtime': 2.568, 'eval_samples_per_second': 596.194, 'eval_steps_per_second': 74.768, 'epoch': 2.0, 'step': 2160}, {'loss': 0.0981, 'learning_rate': 8.750000000000001e-06, 'epoch': 3.0, 'step': 3240}, {'eval_loss': 0.164340540766716, 'eval_precision': 0.49733759318423854, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5143171806167401, 'eval_accuracy': 0.9494325638848143, 'eval_runtime': 2.6105, 'eval_samples_per_second': 586.487, 'eval_steps_per_second': 73.55, 'epoch': 3.0, 'step': 3240}, {'loss': 0.0683, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.0, 'step': 4320}, {'eval_loss': 0.20289303362369537, 'eval_precision': 0.47347347347347346, 'eval_recall': 0.5393386545039909, 'eval_f1': 0.5042643923240938, 'eval_accuracy': 0.9458040608353278, 'eval_runtime': 2.6695, 'eval_samples_per_second': 573.521, 'eval_steps_per_second': 71.924, 'epoch': 4.0, 'step': 4320}, {'loss': 0.0475, 'learning_rate': 7.916666666666667e-06, 'epoch': 5.0, 'step': 5400}, {'eval_loss': 0.2261238545179367, 'eval_precision': 0.4868559411146162, 'eval_recall': 0.5279361459521095, 'eval_f1': 0.5065645514223195, 'eval_accuracy': 0.9482359299004092, 'eval_runtime': 2.4941, 'eval_samples_per_second': 613.843, 'eval_steps_per_second': 76.981, 'epoch': 5.0, 'step': 5400}, {'loss': 0.0336, 'learning_rate': 7.500000000000001e-06, 'epoch': 6.0, 'step': 6480}, {'eval_loss': 0.2532905638217926, 'eval_precision': 0.5158264947245017, 'eval_recall': 0.5017103762827823, 'eval_f1': 0.508670520231214, 'eval_accuracy': 0.9498571759437968, 'eval_runtime': 2.6618, 'eval_samples_per_second': 575.166, 'eval_steps_per_second': 72.131, 'epoch': 6.0, 'step': 6480}, {'loss': 0.0219, 'learning_rate': 7.083333333333335e-06, 'epoch': 7.0, 'step': 7560}, {'eval_loss': 0.28981101512908936, 'eval_precision': 0.45471014492753625, 'eval_recall': 0.572405929304447, 'eval_f1': 0.5068147400302878, 'eval_accuracy': 0.9446460279471937, 'eval_runtime': 2.3255, 'eval_samples_per_second': 658.348, 'eval_steps_per_second': 82.562, 'epoch': 7.0, 'step': 7560}, {'loss': 0.0179, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0, 'step': 8640}, {'eval_loss': 0.2931973338127136, 'eval_precision': 0.48443983402489627, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5073329712112982, 'eval_accuracy': 0.9483903342854937, 'eval_runtime': 2.5642, 'eval_samples_per_second': 597.072, 'eval_steps_per_second': 74.878, 'epoch': 8.0, 'step': 8640}, {'loss': 0.0133, 'learning_rate': 6.25e-06, 'epoch': 9.0, 'step': 9720}, {'eval_loss': 0.3248162567615509, 'eval_precision': 0.49025641025641026, 'eval_recall': 0.5450399087799316, 'eval_f1': 0.5161987041036717, 'eval_accuracy': 0.9495869682698989, 'eval_runtime': 2.387, 'eval_samples_per_second': 641.403, 'eval_steps_per_second': 80.437, 'epoch': 9.0, 'step': 9720}, {'loss': 0.0117, 'learning_rate': 5.833333333333334e-06, 'epoch': 10.0, 'step': 10800}, {'eval_loss': 0.3346257507801056, 'eval_precision': 0.47711088504577825, 'eval_recall': 0.5347776510832383, 'eval_f1': 0.5043010752688171, 'eval_accuracy': 0.9475025090712577, 'eval_runtime': 2.4569, 'eval_samples_per_second': 623.134, 'eval_steps_per_second': 78.146, 'epoch': 10.0, 'step': 10800}, {'loss': 0.008, 'learning_rate': 5.416666666666667e-06, 'epoch': 11.0, 'step': 11880}, {'eval_loss': 0.3535896837711334, 'eval_precision': 0.4742967992240543, 'eval_recall': 0.5575826681870011, 'eval_f1': 0.5125786163522013, 'eval_accuracy': 0.9486219408631205, 'eval_runtime': 2.3344, 'eval_samples_per_second': 655.854, 'eval_steps_per_second': 82.25, 'epoch': 11.0, 'step': 11880}, {'loss': 0.0075, 'learning_rate': 5e-06, 'epoch': 12.0, 'step': 12960}, {'eval_loss': 0.372742623090744, 'eval_precision': 0.49376299376299376, 'eval_recall': 0.5416191562143672, 'eval_f1': 0.5165851005981511, 'eval_accuracy': 0.9492009573071876, 'eval_runtime': 2.449, 'eval_samples_per_second': 625.16, 'eval_steps_per_second': 78.4, 'epoch': 12.0, 'step': 12960}, {'loss': 0.0055, 'learning_rate': 4.583333333333333e-06, 'epoch': 13.0, 'step': 14040}, {'eval_loss': 0.39421698451042175, 'eval_precision': 0.5187713310580204, 'eval_recall': 0.5199543899657925, 'eval_f1': 0.5193621867881548, 'eval_accuracy': 0.9510924110244731, 'eval_runtime': 2.6134, 'eval_samples_per_second': 585.831, 'eval_steps_per_second': 73.468, 'epoch': 13.0, 'step': 14040}, {'loss': 0.0038, 'learning_rate': 4.166666666666667e-06, 'epoch': 14.0, 'step': 15120}, {'eval_loss': 0.40183690190315247, 'eval_precision': 0.5185185185185185, 'eval_recall': 0.5108323831242874, 'eval_f1': 0.5146467547386561, 'eval_accuracy': 0.9512468154095576, 'eval_runtime': 2.4557, 'eval_samples_per_second': 623.456, 'eval_steps_per_second': 78.186, 'epoch': 14.0, 'step': 15120}, {'loss': 0.0046, 'learning_rate': 3.7500000000000005e-06, 'epoch': 15.0, 'step': 16200}, {'eval_loss': 0.40451306104660034, 'eval_precision': 0.49527806925498424, 'eval_recall': 0.5381984036488028, 'eval_f1': 0.5158469945355191, 'eval_accuracy': 0.9500501814251525, 'eval_runtime': 2.491, 'eval_samples_per_second': 614.62, 'eval_steps_per_second': 77.078, 'epoch': 15.0, 'step': 16200}, {'loss': 0.0029, 'learning_rate': 3.3333333333333333e-06, 'epoch': 16.0, 'step': 17280}, {'eval_loss': 0.4149148464202881, 'eval_precision': 0.5168918918918919, 'eval_recall': 0.5233751425313569, 'eval_f1': 0.520113314447592, 'eval_accuracy': 0.9513240176021, 'eval_runtime': 2.5466, 'eval_samples_per_second': 601.191, 'eval_steps_per_second': 75.394, 'epoch': 16.0, 'step': 17280}, {'loss': 0.0031, 'learning_rate': 2.916666666666667e-06, 'epoch': 17.0, 'step': 18360}, {'eval_loss': 0.41380545496940613, 'eval_precision': 0.5168408826945412, 'eval_recall': 0.507411630558723, 'eval_f1': 0.5120828538550058, 'eval_accuracy': 0.9519030340461669, 'eval_runtime': 2.4993, 'eval_samples_per_second': 612.581, 'eval_steps_per_second': 76.823, 'epoch': 17.0, 'step': 18360}, {'loss': 0.002, 'learning_rate': 2.5e-06, 'epoch': 18.0, 'step': 19440}, {'eval_loss': 0.4236869513988495, 'eval_precision': 0.5010869565217392, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5130773511407902, 'eval_accuracy': 0.9498571759437968, 'eval_runtime': 2.58, 'eval_samples_per_second': 593.407, 'eval_steps_per_second': 74.418, 'epoch': 18.0, 'step': 19440}, {'loss': 0.0019, 'learning_rate': 2.0833333333333334e-06, 'epoch': 19.0, 'step': 20520}, {'eval_loss': 0.4356597363948822, 'eval_precision': 0.4962805526036132, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5137513751375138, 'eval_accuracy': 0.9490079518258319, 'eval_runtime': 2.7572, 'eval_samples_per_second': 555.28, 'eval_steps_per_second': 69.637, 'epoch': 19.0, 'step': 20520}, {'loss': 0.0019, 'learning_rate': 1.6666666666666667e-06, 'epoch': 20.0, 'step': 21600}, {'eval_loss': 0.4274821877479553, 'eval_precision': 0.48194014447884415, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.505958829902492, 'eval_accuracy': 0.9488921485370184, 'eval_runtime': 2.5931, 'eval_samples_per_second': 590.414, 'eval_steps_per_second': 74.043, 'epoch': 20.0, 'step': 21600}, {'loss': 0.0015, 'learning_rate': 1.25e-06, 'epoch': 21.0, 'step': 22680}, {'eval_loss': 0.4277479350566864, 'eval_precision': 0.5010373443983402, 'eval_recall': 0.5507411630558723, 'eval_f1': 0.5247148288973383, 'eval_accuracy': 0.9500115803288813, 'eval_runtime': 2.5921, 'eval_samples_per_second': 590.643, 'eval_steps_per_second': 74.072, 'epoch': 21.0, 'step': 22680}, {'loss': 0.001, 'learning_rate': 8.333333333333333e-07, 'epoch': 22.0, 'step': 23760}, {'eval_loss': 0.4313080906867981, 'eval_precision': 0.5159165751920965, 'eval_recall': 0.5359179019384265, 'eval_f1': 0.5257270693512304, 'eval_accuracy': 0.9506291978692195, 'eval_runtime': 2.8617, 'eval_samples_per_second': 534.991, 'eval_steps_per_second': 67.092, 'epoch': 22.0, 'step': 23760}, {'loss': 0.001, 'learning_rate': 4.1666666666666667e-07, 'epoch': 23.0, 'step': 24840}, {'eval_loss': 0.432518869638443, 'eval_precision': 0.4989270386266094, 'eval_recall': 0.5302166476624858, 'eval_f1': 0.5140961857379768, 'eval_accuracy': 0.9508222033505751, 'eval_runtime': 2.5086, 'eval_samples_per_second': 610.304, 'eval_steps_per_second': 76.537, 'epoch': 23.0, 'step': 24840}, {'loss': 0.001, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 25920}, {'eval_loss': 0.43199750781059265, 'eval_precision': 0.5005417118093174, 'eval_recall': 0.5267958950969214, 'eval_f1': 0.5133333333333333, 'eval_accuracy': 0.950474793484135, 'eval_runtime': 2.5068, 'eval_samples_per_second': 610.732, 'eval_steps_per_second': 76.591, 'epoch': 24.0, 'step': 25920}, {'train_runtime': 1589.7848, 'train_samples_per_second': 130.342, 'train_steps_per_second': 16.304, 'total_flos': 5908755624631080.0, 'train_loss': 0.029394966704242024, 'epoch': 24.0, 'step': 25920}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9532
  predict_f1                 =     0.4721
  predict_loss               =     0.1593
  predict_precision          =     0.5476
  predict_recall             =      0.415
  predict_runtime            = 0:00:02.18
  predict_samples_per_second =    582.968
  predict_steps_per_second   =     72.871
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_base_02.json completed. F1: 0.47213622291021673
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_23.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_23.json
01191518_tsa-bin_NB-BERT_large Our label2id: {'O': 0, 'B-targ-Negative': 1, 'I-targ-Negative': 2, 'B-targ-Positive': 3, 'I-targ-Positive': 4}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 6192.77 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:00, 7089.97 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 7063.26 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 7189.59 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 7280.83 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 7459.17 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:00<00:00, 7628.27 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 6281.51 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 6858.09 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 7685.11 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 7289.73 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 7359.99 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 7155.81 examples/s]
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Checkpoint destination directory /cluster/work/projects/ec30/egilron/tsa-hf/01191518_tsa-bin_NB-BERT_large/checkpoint-540 already exists and is non-empty.Saving will proceed but saved results may be invalid.
01191518_tsa-bin_NB-BERT_large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1953, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.0}
{'eval_loss': 0.1352165788412094, 'eval_precision': 0.54337899543379, 'eval_recall': 0.5427594070695553, 'eval_f1': 0.5430690245293781, 'eval_accuracy': 0.9550683239403999, 'eval_runtime': 4.9595, 'eval_samples_per_second': 308.702, 'eval_steps_per_second': 38.714, 'epoch': 1.0}
{'loss': 0.1092, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.0}
{'eval_loss': 0.14775601029396057, 'eval_precision': 0.504875406283857, 'eval_recall': 0.5313568985176739, 'eval_f1': 0.5177777777777778, 'eval_accuracy': 0.9535628811858257, 'eval_runtime': 4.9612, 'eval_samples_per_second': 308.594, 'eval_steps_per_second': 38.7, 'epoch': 2.0}
{'loss': 0.059, 'learning_rate': 4.375e-05, 'epoch': 3.0}
{'eval_loss': 0.18642449378967285, 'eval_precision': 0.427986906710311, 'eval_recall': 0.5963511972633979, 'eval_f1': 0.4983325393044306, 'eval_accuracy': 0.9392032733729638, 'eval_runtime': 5.0648, 'eval_samples_per_second': 302.281, 'eval_steps_per_second': 37.909, 'epoch': 3.0}
{'loss': 0.033, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.0}
{'eval_loss': 0.21818073093891144, 'eval_precision': 0.5140845070422535, 'eval_recall': 0.49942987457240595, 'eval_f1': 0.5066512434933488, 'eval_accuracy': 0.9532540724156566, 'eval_runtime': 4.8296, 'eval_samples_per_second': 317.006, 'eval_steps_per_second': 39.755, 'epoch': 4.0}
{'loss': 0.0202, 'learning_rate': 3.958333333333333e-05, 'epoch': 5.0}
{'eval_loss': 0.23130841553211212, 'eval_precision': 0.47863247863247865, 'eval_recall': 0.5746864310148233, 'eval_f1': 0.5222797927461141, 'eval_accuracy': 0.9497799737512546, 'eval_runtime': 5.0282, 'eval_samples_per_second': 304.481, 'eval_steps_per_second': 38.184, 'epoch': 5.0}
{'loss': 0.0125, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.0}
{'eval_loss': 0.2540224492549896, 'eval_precision': 0.5175438596491229, 'eval_recall': 0.5381984036488028, 'eval_f1': 0.5276690888764674, 'eval_accuracy': 0.9529066625492164, 'eval_runtime': 4.8889, 'eval_samples_per_second': 313.157, 'eval_steps_per_second': 39.272, 'epoch': 6.0}
{'loss': 0.0087, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.0}
{'eval_loss': 0.28267598152160645, 'eval_precision': 0.5494636471990465, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5372960372960374, 'eval_accuracy': 0.9547595151702308, 'eval_runtime': 4.8835, 'eval_samples_per_second': 313.507, 'eval_steps_per_second': 39.316, 'epoch': 7.0}
{'loss': 0.0082, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.0}
{'eval_loss': 0.30147379636764526, 'eval_precision': 0.5666666666666667, 'eval_recall': 0.5039908779931584, 'eval_f1': 0.5334942667471334, 'eval_accuracy': 0.9552227283254845, 'eval_runtime': 4.7791, 'eval_samples_per_second': 320.351, 'eval_steps_per_second': 40.175, 'epoch': 8.0}
{'loss': 0.0053, 'learning_rate': 3.125e-05, 'epoch': 9.0}
{'eval_loss': 0.307770699262619, 'eval_precision': 0.5075528700906344, 'eval_recall': 0.5746864310148233, 'eval_f1': 0.5390374331550801, 'eval_accuracy': 0.9512082143132865, 'eval_runtime': 4.9088, 'eval_samples_per_second': 311.89, 'eval_steps_per_second': 39.114, 'epoch': 9.0}
{'loss': 0.0053, 'learning_rate': 2.916666666666667e-05, 'epoch': 10.0}
{'eval_loss': 0.33844828605651855, 'eval_precision': 0.5405405405405406, 'eval_recall': 0.5017103762827823, 'eval_f1': 0.5204021289178002, 'eval_accuracy': 0.9544121053037906, 'eval_runtime': 4.831, 'eval_samples_per_second': 316.911, 'eval_steps_per_second': 39.743, 'epoch': 10.0}
{'loss': 0.0035, 'learning_rate': 2.7083333333333332e-05, 'epoch': 11.0}
{'eval_loss': 0.3317042887210846, 'eval_precision': 0.5291622481442205, 'eval_recall': 0.5689851767388826, 'eval_f1': 0.5483516483516483, 'eval_accuracy': 0.9536786844746391, 'eval_runtime': 4.9674, 'eval_samples_per_second': 308.207, 'eval_steps_per_second': 38.652, 'epoch': 11.0}
{'loss': 0.0036, 'learning_rate': 2.5e-05, 'epoch': 12.0}
{'eval_loss': 0.33428916335105896, 'eval_precision': 0.5171339563862928, 'eval_recall': 0.5678449258836944, 'eval_f1': 0.5413043478260868, 'eval_accuracy': 0.9534084768007411, 'eval_runtime': 4.8739, 'eval_samples_per_second': 314.124, 'eval_steps_per_second': 39.394, 'epoch': 12.0}
{'loss': 0.0029, 'learning_rate': 2.2916666666666667e-05, 'epoch': 13.0}
{'eval_loss': 0.34919336438179016, 'eval_precision': 0.5738705738705738, 'eval_recall': 0.5359179019384265, 'eval_f1': 0.554245283018868, 'eval_accuracy': 0.9553385316142978, 'eval_runtime': 5.0198, 'eval_samples_per_second': 304.991, 'eval_steps_per_second': 38.248, 'epoch': 13.0}
{'loss': 0.0026, 'learning_rate': 2.0833333333333336e-05, 'epoch': 14.0}
{'eval_loss': 0.35676705837249756, 'eval_precision': 0.5119170984455959, 'eval_recall': 0.5632839224629419, 'eval_f1': 0.5363735070575462, 'eval_accuracy': 0.9508994055431175, 'eval_runtime': 4.947, 'eval_samples_per_second': 309.479, 'eval_steps_per_second': 38.811, 'epoch': 14.0}
{'loss': 0.0013, 'learning_rate': 1.8750000000000002e-05, 'epoch': 15.0}
{'eval_loss': 0.34986647963523865, 'eval_precision': 0.5707133917396746, 'eval_recall': 0.5199543899657925, 'eval_f1': 0.5441527446300716, 'eval_accuracy': 0.9549525206515865, 'eval_runtime': 4.9296, 'eval_samples_per_second': 310.573, 'eval_steps_per_second': 38.948, 'epoch': 15.0}
{'loss': 0.002, 'learning_rate': 1.6666666666666667e-05, 'epoch': 16.0}
{'eval_loss': 0.38194209337234497, 'eval_precision': 0.5802631578947368, 'eval_recall': 0.5028506271379704, 'eval_f1': 0.5387904703726327, 'eval_accuracy': 0.9557631436732803, 'eval_runtime': 5.0311, 'eval_samples_per_second': 304.31, 'eval_steps_per_second': 38.163, 'epoch': 16.0}
{'loss': 0.0021, 'learning_rate': 1.4583333333333335e-05, 'epoch': 17.0}
{'eval_loss': 0.35372182726860046, 'eval_precision': 0.5591882750845547, 'eval_recall': 0.5655644241733181, 'eval_f1': 0.5623582766439909, 'eval_accuracy': 0.9556087392881958, 'eval_runtime': 4.9122, 'eval_samples_per_second': 311.675, 'eval_steps_per_second': 39.087, 'epoch': 17.0}
{'loss': 0.0015, 'learning_rate': 1.25e-05, 'epoch': 18.0}
{'eval_loss': 0.3647896945476532, 'eval_precision': 0.5782556750298686, 'eval_recall': 0.5518814139110604, 'eval_f1': 0.5647607934655776, 'eval_accuracy': 0.9561105535397205, 'eval_runtime': 4.9786, 'eval_samples_per_second': 307.517, 'eval_steps_per_second': 38.565, 'epoch': 18.0}
{'loss': 0.0009, 'learning_rate': 1.0416666666666668e-05, 'epoch': 19.0}
{'eval_loss': 0.3717232346534729, 'eval_precision': 0.5542725173210161, 'eval_recall': 0.5473204104903079, 'eval_f1': 0.5507745266781412, 'eval_accuracy': 0.9552999305180268, 'eval_runtime': 4.8752, 'eval_samples_per_second': 314.036, 'eval_steps_per_second': 39.383, 'epoch': 19.0}
{'loss': 0.0005, 'learning_rate': 8.333333333333334e-06, 'epoch': 20.0}
{'eval_loss': 0.3919816017150879, 'eval_precision': 0.5586907449209932, 'eval_recall': 0.56442417331813, 'eval_f1': 0.5615428247305729, 'eval_accuracy': 0.9537944877634524, 'eval_runtime': 4.9276, 'eval_samples_per_second': 310.702, 'eval_steps_per_second': 38.965, 'epoch': 20.0}
{'loss': 0.0003, 'learning_rate': 6.25e-06, 'epoch': 21.0}
{'eval_loss': 0.4013028144836426, 'eval_precision': 0.5301478953356087, 'eval_recall': 0.5313568985176739, 'eval_f1': 0.530751708428246, 'eval_accuracy': 0.9535242800895546, 'eval_runtime': 4.8523, 'eval_samples_per_second': 315.518, 'eval_steps_per_second': 39.569, 'epoch': 21.0}
{'loss': 0.0004, 'learning_rate': 4.166666666666667e-06, 'epoch': 22.0}
{'eval_loss': 0.4101921021938324, 'eval_precision': 0.5647058823529412, 'eval_recall': 0.5473204104903079, 'eval_f1': 0.555877243775333, 'eval_accuracy': 0.9549911217478576, 'eval_runtime': 4.8017, 'eval_samples_per_second': 318.844, 'eval_steps_per_second': 39.986, 'epoch': 22.0}
{'loss': 0.0003, 'learning_rate': 2.0833333333333334e-06, 'epoch': 23.0}
{'eval_loss': 0.4080328941345215, 'eval_precision': 0.5622119815668203, 'eval_recall': 0.556442417331813, 'eval_f1': 0.5593123209169054, 'eval_accuracy': 0.9549525206515865, 'eval_runtime': 4.9324, 'eval_samples_per_second': 310.396, 'eval_steps_per_second': 38.926, 'epoch': 23.0}
{'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': 0.4104054868221283, 'eval_precision': 0.5563063063063063, 'eval_recall': 0.5632839224629419, 'eval_f1': 0.559773371104816, 'eval_accuracy': 0.9544507064000618, 'eval_runtime': 4.8925, 'eval_samples_per_second': 312.93, 'eval_steps_per_second': 39.244, 'epoch': 24.0}
{'train_runtime': 2517.2007, 'train_samples_per_second': 82.32, 'train_steps_per_second': 5.149, 'train_loss': 0.01994707334533702, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =     0.0199
  train_runtime            = 0:41:57.20
  train_samples            =       8634
  train_samples_per_second =      82.32
  train_steps_per_second   =      5.149
[{'loss': 0.1953, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.0, 'step': 540}, {'eval_loss': 0.1352165788412094, 'eval_precision': 0.54337899543379, 'eval_recall': 0.5427594070695553, 'eval_f1': 0.5430690245293781, 'eval_accuracy': 0.9550683239403999, 'eval_runtime': 4.9595, 'eval_samples_per_second': 308.702, 'eval_steps_per_second': 38.714, 'epoch': 1.0, 'step': 540}, {'loss': 0.1092, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.0, 'step': 1080}, {'eval_loss': 0.14775601029396057, 'eval_precision': 0.504875406283857, 'eval_recall': 0.5313568985176739, 'eval_f1': 0.5177777777777778, 'eval_accuracy': 0.9535628811858257, 'eval_runtime': 4.9612, 'eval_samples_per_second': 308.594, 'eval_steps_per_second': 38.7, 'epoch': 2.0, 'step': 1080}, {'loss': 0.059, 'learning_rate': 4.375e-05, 'epoch': 3.0, 'step': 1620}, {'eval_loss': 0.18642449378967285, 'eval_precision': 0.427986906710311, 'eval_recall': 0.5963511972633979, 'eval_f1': 0.4983325393044306, 'eval_accuracy': 0.9392032733729638, 'eval_runtime': 5.0648, 'eval_samples_per_second': 302.281, 'eval_steps_per_second': 37.909, 'epoch': 3.0, 'step': 1620}, {'loss': 0.033, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.0, 'step': 2160}, {'eval_loss': 0.21818073093891144, 'eval_precision': 0.5140845070422535, 'eval_recall': 0.49942987457240595, 'eval_f1': 0.5066512434933488, 'eval_accuracy': 0.9532540724156566, 'eval_runtime': 4.8296, 'eval_samples_per_second': 317.006, 'eval_steps_per_second': 39.755, 'epoch': 4.0, 'step': 2160}, {'loss': 0.0202, 'learning_rate': 3.958333333333333e-05, 'epoch': 5.0, 'step': 2700}, {'eval_loss': 0.23130841553211212, 'eval_precision': 0.47863247863247865, 'eval_recall': 0.5746864310148233, 'eval_f1': 0.5222797927461141, 'eval_accuracy': 0.9497799737512546, 'eval_runtime': 5.0282, 'eval_samples_per_second': 304.481, 'eval_steps_per_second': 38.184, 'epoch': 5.0, 'step': 2700}, {'loss': 0.0125, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.0, 'step': 3240}, {'eval_loss': 0.2540224492549896, 'eval_precision': 0.5175438596491229, 'eval_recall': 0.5381984036488028, 'eval_f1': 0.5276690888764674, 'eval_accuracy': 0.9529066625492164, 'eval_runtime': 4.8889, 'eval_samples_per_second': 313.157, 'eval_steps_per_second': 39.272, 'epoch': 6.0, 'step': 3240}, {'loss': 0.0087, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.0, 'step': 3780}, {'eval_loss': 0.28267598152160645, 'eval_precision': 0.5494636471990465, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5372960372960374, 'eval_accuracy': 0.9547595151702308, 'eval_runtime': 4.8835, 'eval_samples_per_second': 313.507, 'eval_steps_per_second': 39.316, 'epoch': 7.0, 'step': 3780}, {'loss': 0.0082, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.0, 'step': 4320}, {'eval_loss': 0.30147379636764526, 'eval_precision': 0.5666666666666667, 'eval_recall': 0.5039908779931584, 'eval_f1': 0.5334942667471334, 'eval_accuracy': 0.9552227283254845, 'eval_runtime': 4.7791, 'eval_samples_per_second': 320.351, 'eval_steps_per_second': 40.175, 'epoch': 8.0, 'step': 4320}, {'loss': 0.0053, 'learning_rate': 3.125e-05, 'epoch': 9.0, 'step': 4860}, {'eval_loss': 0.307770699262619, 'eval_precision': 0.5075528700906344, 'eval_recall': 0.5746864310148233, 'eval_f1': 0.5390374331550801, 'eval_accuracy': 0.9512082143132865, 'eval_runtime': 4.9088, 'eval_samples_per_second': 311.89, 'eval_steps_per_second': 39.114, 'epoch': 9.0, 'step': 4860}, {'loss': 0.0053, 'learning_rate': 2.916666666666667e-05, 'epoch': 10.0, 'step': 5400}, {'eval_loss': 0.33844828605651855, 'eval_precision': 0.5405405405405406, 'eval_recall': 0.5017103762827823, 'eval_f1': 0.5204021289178002, 'eval_accuracy': 0.9544121053037906, 'eval_runtime': 4.831, 'eval_samples_per_second': 316.911, 'eval_steps_per_second': 39.743, 'epoch': 10.0, 'step': 5400}, {'loss': 0.0035, 'learning_rate': 2.7083333333333332e-05, 'epoch': 11.0, 'step': 5940}, {'eval_loss': 0.3317042887210846, 'eval_precision': 0.5291622481442205, 'eval_recall': 0.5689851767388826, 'eval_f1': 0.5483516483516483, 'eval_accuracy': 0.9536786844746391, 'eval_runtime': 4.9674, 'eval_samples_per_second': 308.207, 'eval_steps_per_second': 38.652, 'epoch': 11.0, 'step': 5940}, {'loss': 0.0036, 'learning_rate': 2.5e-05, 'epoch': 12.0, 'step': 6480}, {'eval_loss': 0.33428916335105896, 'eval_precision': 0.5171339563862928, 'eval_recall': 0.5678449258836944, 'eval_f1': 0.5413043478260868, 'eval_accuracy': 0.9534084768007411, 'eval_runtime': 4.8739, 'eval_samples_per_second': 314.124, 'eval_steps_per_second': 39.394, 'epoch': 12.0, 'step': 6480}, {'loss': 0.0029, 'learning_rate': 2.2916666666666667e-05, 'epoch': 13.0, 'step': 7020}, {'eval_loss': 0.34919336438179016, 'eval_precision': 0.5738705738705738, 'eval_recall': 0.5359179019384265, 'eval_f1': 0.554245283018868, 'eval_accuracy': 0.9553385316142978, 'eval_runtime': 5.0198, 'eval_samples_per_second': 304.991, 'eval_steps_per_second': 38.248, 'epoch': 13.0, 'step': 7020}, {'loss': 0.0026, 'learning_rate': 2.0833333333333336e-05, 'epoch': 14.0, 'step': 7560}, {'eval_loss': 0.35676705837249756, 'eval_precision': 0.5119170984455959, 'eval_recall': 0.5632839224629419, 'eval_f1': 0.5363735070575462, 'eval_accuracy': 0.9508994055431175, 'eval_runtime': 4.947, 'eval_samples_per_second': 309.479, 'eval_steps_per_second': 38.811, 'epoch': 14.0, 'step': 7560}, {'loss': 0.0013, 'learning_rate': 1.8750000000000002e-05, 'epoch': 15.0, 'step': 8100}, {'eval_loss': 0.34986647963523865, 'eval_precision': 0.5707133917396746, 'eval_recall': 0.5199543899657925, 'eval_f1': 0.5441527446300716, 'eval_accuracy': 0.9549525206515865, 'eval_runtime': 4.9296, 'eval_samples_per_second': 310.573, 'eval_steps_per_second': 38.948, 'epoch': 15.0, 'step': 8100}, {'loss': 0.002, 'learning_rate': 1.6666666666666667e-05, 'epoch': 16.0, 'step': 8640}, {'eval_loss': 0.38194209337234497, 'eval_precision': 0.5802631578947368, 'eval_recall': 0.5028506271379704, 'eval_f1': 0.5387904703726327, 'eval_accuracy': 0.9557631436732803, 'eval_runtime': 5.0311, 'eval_samples_per_second': 304.31, 'eval_steps_per_second': 38.163, 'epoch': 16.0, 'step': 8640}, {'loss': 0.0021, 'learning_rate': 1.4583333333333335e-05, 'epoch': 17.0, 'step': 9180}, {'eval_loss': 0.35372182726860046, 'eval_precision': 0.5591882750845547, 'eval_recall': 0.5655644241733181, 'eval_f1': 0.5623582766439909, 'eval_accuracy': 0.9556087392881958, 'eval_runtime': 4.9122, 'eval_samples_per_second': 311.675, 'eval_steps_per_second': 39.087, 'epoch': 17.0, 'step': 9180}, {'loss': 0.0015, 'learning_rate': 1.25e-05, 'epoch': 18.0, 'step': 9720}, {'eval_loss': 0.3647896945476532, 'eval_precision': 0.5782556750298686, 'eval_recall': 0.5518814139110604, 'eval_f1': 0.5647607934655776, 'eval_accuracy': 0.9561105535397205, 'eval_runtime': 4.9786, 'eval_samples_per_second': 307.517, 'eval_steps_per_second': 38.565, 'epoch': 18.0, 'step': 9720}, {'loss': 0.0009, 'learning_rate': 1.0416666666666668e-05, 'epoch': 19.0, 'step': 10260}, {'eval_loss': 0.3717232346534729, 'eval_precision': 0.5542725173210161, 'eval_recall': 0.5473204104903079, 'eval_f1': 0.5507745266781412, 'eval_accuracy': 0.9552999305180268, 'eval_runtime': 4.8752, 'eval_samples_per_second': 314.036, 'eval_steps_per_second': 39.383, 'epoch': 19.0, 'step': 10260}, {'loss': 0.0005, 'learning_rate': 8.333333333333334e-06, 'epoch': 20.0, 'step': 10800}, {'eval_loss': 0.3919816017150879, 'eval_precision': 0.5586907449209932, 'eval_recall': 0.56442417331813, 'eval_f1': 0.5615428247305729, 'eval_accuracy': 0.9537944877634524, 'eval_runtime': 4.9276, 'eval_samples_per_second': 310.702, 'eval_steps_per_second': 38.965, 'epoch': 20.0, 'step': 10800}, {'loss': 0.0003, 'learning_rate': 6.25e-06, 'epoch': 21.0, 'step': 11340}, {'eval_loss': 0.4013028144836426, 'eval_precision': 0.5301478953356087, 'eval_recall': 0.5313568985176739, 'eval_f1': 0.530751708428246, 'eval_accuracy': 0.9535242800895546, 'eval_runtime': 4.8523, 'eval_samples_per_second': 315.518, 'eval_steps_per_second': 39.569, 'epoch': 21.0, 'step': 11340}, {'loss': 0.0004, 'learning_rate': 4.166666666666667e-06, 'epoch': 22.0, 'step': 11880}, {'eval_loss': 0.4101921021938324, 'eval_precision': 0.5647058823529412, 'eval_recall': 0.5473204104903079, 'eval_f1': 0.555877243775333, 'eval_accuracy': 0.9549911217478576, 'eval_runtime': 4.8017, 'eval_samples_per_second': 318.844, 'eval_steps_per_second': 39.986, 'epoch': 22.0, 'step': 11880}, {'loss': 0.0003, 'learning_rate': 2.0833333333333334e-06, 'epoch': 23.0, 'step': 12420}, {'eval_loss': 0.4080328941345215, 'eval_precision': 0.5622119815668203, 'eval_recall': 0.556442417331813, 'eval_f1': 0.5593123209169054, 'eval_accuracy': 0.9549525206515865, 'eval_runtime': 4.9324, 'eval_samples_per_second': 310.396, 'eval_steps_per_second': 38.926, 'epoch': 23.0, 'step': 12420}, {'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 12960}, {'eval_loss': 0.4104054868221283, 'eval_precision': 0.5563063063063063, 'eval_recall': 0.5632839224629419, 'eval_f1': 0.559773371104816, 'eval_accuracy': 0.9544507064000618, 'eval_runtime': 4.8925, 'eval_samples_per_second': 312.93, 'eval_steps_per_second': 39.244, 'epoch': 24.0, 'step': 12960}, {'train_runtime': 2517.2007, 'train_samples_per_second': 82.32, 'train_steps_per_second': 5.149, 'total_flos': 1.9841997686762016e+16, 'train_loss': 0.01994707334533702, 'epoch': 24.0, 'step': 12960}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9515
  predict_f1                 =     0.5043
  predict_loss               =     0.1532
  predict_precision          =     0.5291
  predict_recall             =     0.4816
  predict_runtime            = 0:00:04.16
  predict_samples_per_second =    305.155
  predict_steps_per_second   =     38.144
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_23.json completed. F1: 0.5042735042735043
/fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_11.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.22.3
PyTorch: 1.12.1
Transformers: 4.36.2



***Loading config file: /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_11.json
01191518_tsa-bin_NB-BERT_large Our label2id: {'O': 0, 'B-targ-Negative': 1, 'I-targ-Negative': 2, 'B-targ-Positive': 3, 'I-targ-Positive': 4}
Running tokenizer on train dataset:   0%|          | 0/8634 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8634 [00:00<00:01, 5494.67 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8634 [00:00<00:01, 6283.86 examples/s]Running tokenizer on train dataset:  35%|███▍      | 3000/8634 [00:00<00:00, 6410.10 examples/s]Running tokenizer on train dataset:  46%|████▋     | 4000/8634 [00:00<00:00, 6536.25 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8634 [00:00<00:00, 6592.24 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 6000/8634 [00:00<00:00, 6169.19 examples/s]Running tokenizer on train dataset:  81%|████████  | 7000/8634 [00:01<00:00, 6489.29 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8634 [00:01<00:00, 5220.37 examples/s]Running tokenizer on train dataset: 100%|██████████| 8634/8634 [00:01<00:00, 5896.27 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1531 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  65%|██████▌   | 1000/1531 [00:00<00:00, 6881.65 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1531/1531 [00:00<00:00, 6512.07 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1272 [00:00<?, ? examples/s]Running tokenizer on test dataset:  79%|███████▊  | 1000/1272 [00:00<00:00, 6521.70 examples/s]Running tokenizer on test dataset: 100%|██████████| 1272/1272 [00:00<00:00, 6332.32 examples/s]
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
01191518_tsa-bin_NB-BERT_large Ready to train. Train dataset labels are now: ['idx', 'tokens', 'tsa_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1946, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.0}
{'eval_loss': 0.15200291574001312, 'eval_precision': 0.5506493506493506, 'eval_recall': 0.48346636259977194, 'eval_f1': 0.5148755312689739, 'eval_accuracy': 0.9530996680305721, 'eval_runtime': 4.9031, 'eval_samples_per_second': 312.25, 'eval_steps_per_second': 39.159, 'epoch': 1.0}
{'loss': 0.1151, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.0}
{'eval_loss': 0.15932069718837738, 'eval_precision': 0.5519568151147098, 'eval_recall': 0.4663625997719498, 'eval_f1': 0.5055624227441285, 'eval_accuracy': 0.9536786844746391, 'eval_runtime': 4.7514, 'eval_samples_per_second': 322.219, 'eval_steps_per_second': 40.409, 'epoch': 2.0}
{'loss': 0.0682, 'learning_rate': 4.375e-05, 'epoch': 3.0}
{'eval_loss': 0.18948908150196075, 'eval_precision': 0.5529131985731273, 'eval_recall': 0.5302166476624858, 'eval_f1': 0.5413271245634459, 'eval_accuracy': 0.9541804987261638, 'eval_runtime': 4.8893, 'eval_samples_per_second': 313.132, 'eval_steps_per_second': 39.269, 'epoch': 3.0}
{'loss': 0.0407, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.0}
{'eval_loss': 0.2405519038438797, 'eval_precision': 0.4684854186265287, 'eval_recall': 0.5678449258836944, 'eval_f1': 0.5134020618556702, 'eval_accuracy': 0.9468462904346483, 'eval_runtime': 4.831, 'eval_samples_per_second': 316.908, 'eval_steps_per_second': 39.743, 'epoch': 4.0}
{'loss': 0.0263, 'learning_rate': 3.958333333333333e-05, 'epoch': 5.0}
{'eval_loss': 0.2783564329147339, 'eval_precision': 0.5789473684210527, 'eval_recall': 0.4766248574686431, 'eval_f1': 0.5228267667292057, 'eval_accuracy': 0.9540260943410793, 'eval_runtime': 4.9169, 'eval_samples_per_second': 311.378, 'eval_steps_per_second': 39.049, 'epoch': 5.0}
{'loss': 0.0175, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.0}
{'eval_loss': 0.30681315064430237, 'eval_precision': 0.5334928229665071, 'eval_recall': 0.508551881413911, 'eval_f1': 0.5207238762405137, 'eval_accuracy': 0.9508994055431175, 'eval_runtime': 4.8022, 'eval_samples_per_second': 318.814, 'eval_steps_per_second': 39.982, 'epoch': 6.0}
{'loss': 0.014, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.0}
{'eval_loss': 0.3398776352405548, 'eval_precision': 0.5214932126696833, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5235661555934128, 'eval_accuracy': 0.9511696132170153, 'eval_runtime': 4.7067, 'eval_samples_per_second': 325.279, 'eval_steps_per_second': 40.793, 'epoch': 7.0}
{'loss': 0.0123, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.0}
{'eval_loss': 0.35497117042541504, 'eval_precision': 0.5336481700118064, 'eval_recall': 0.5153933865450399, 'eval_f1': 0.5243619489559165, 'eval_accuracy': 0.95167142746854, 'eval_runtime': 4.9047, 'eval_samples_per_second': 312.148, 'eval_steps_per_second': 39.146, 'epoch': 8.0}
{'loss': 0.0088, 'learning_rate': 3.125e-05, 'epoch': 9.0}
{'eval_loss': 0.2987205684185028, 'eval_precision': 0.5162374020156775, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5209039548022598, 'eval_accuracy': 0.9506291978692195, 'eval_runtime': 4.9274, 'eval_samples_per_second': 310.709, 'eval_steps_per_second': 38.965, 'epoch': 9.0}
{'loss': 0.0085, 'learning_rate': 2.916666666666667e-05, 'epoch': 10.0}
{'eval_loss': 0.3276006281375885, 'eval_precision': 0.5389784946236559, 'eval_recall': 0.4572405929304447, 'eval_f1': 0.4947563232572486, 'eval_accuracy': 0.9508994055431175, 'eval_runtime': 4.7859, 'eval_samples_per_second': 319.9, 'eval_steps_per_second': 40.118, 'epoch': 10.0}
{'loss': 0.0072, 'learning_rate': 2.7083333333333332e-05, 'epoch': 11.0}
{'eval_loss': 0.3525168299674988, 'eval_precision': 0.5327313769751693, 'eval_recall': 0.5381984036488028, 'eval_f1': 0.5354509359047078, 'eval_accuracy': 0.9529066625492164, 'eval_runtime': 4.9172, 'eval_samples_per_second': 311.355, 'eval_steps_per_second': 39.046, 'epoch': 11.0}
{'loss': 0.0078, 'learning_rate': 2.5e-05, 'epoch': 12.0}
{'eval_loss': 0.39575713872909546, 'eval_precision': 0.5653846153846154, 'eval_recall': 0.5028506271379704, 'eval_f1': 0.5322872661436331, 'eval_accuracy': 0.9527522581641319, 'eval_runtime': 4.9582, 'eval_samples_per_second': 308.783, 'eval_steps_per_second': 38.724, 'epoch': 12.0}
{'loss': 0.0039, 'learning_rate': 2.2916666666666667e-05, 'epoch': 13.0}
{'eval_loss': 0.40147319436073303, 'eval_precision': 0.510548523206751, 'eval_recall': 0.5518814139110604, 'eval_f1': 0.5304109589041096, 'eval_accuracy': 0.9503589901953216, 'eval_runtime': 4.866, 'eval_samples_per_second': 314.632, 'eval_steps_per_second': 39.457, 'epoch': 13.0}
{'loss': 0.0043, 'learning_rate': 2.0833333333333336e-05, 'epoch': 14.0}
{'eval_loss': 0.4153269827365875, 'eval_precision': 0.5572413793103448, 'eval_recall': 0.4606613454960091, 'eval_f1': 0.5043695380774033, 'eval_accuracy': 0.9541418976298927, 'eval_runtime': 4.9055, 'eval_samples_per_second': 312.097, 'eval_steps_per_second': 39.14, 'epoch': 14.0}
{'loss': 0.0038, 'learning_rate': 1.8750000000000002e-05, 'epoch': 15.0}
{'eval_loss': 0.3622564375400543, 'eval_precision': 0.5663600525624178, 'eval_recall': 0.49144811858608894, 'eval_f1': 0.5262515262515262, 'eval_accuracy': 0.9528294603566742, 'eval_runtime': 4.9031, 'eval_samples_per_second': 312.251, 'eval_steps_per_second': 39.159, 'epoch': 15.0}
{'loss': 0.0027, 'learning_rate': 1.6666666666666667e-05, 'epoch': 16.0}
{'eval_loss': 0.38585445284843445, 'eval_precision': 0.5442739079102715, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5348027842227379, 'eval_accuracy': 0.9525592526827762, 'eval_runtime': 4.8452, 'eval_samples_per_second': 315.982, 'eval_steps_per_second': 39.627, 'epoch': 16.0}
{'loss': 0.0033, 'learning_rate': 1.4583333333333335e-05, 'epoch': 17.0}
{'eval_loss': 0.40605276823043823, 'eval_precision': 0.5364077669902912, 'eval_recall': 0.5039908779931584, 'eval_f1': 0.5196942974720752, 'eval_accuracy': 0.9514398208909133, 'eval_runtime': 4.8787, 'eval_samples_per_second': 313.815, 'eval_steps_per_second': 39.355, 'epoch': 17.0}
{'loss': 0.0013, 'learning_rate': 1.25e-05, 'epoch': 18.0}
{'eval_loss': 0.4425526559352875, 'eval_precision': 0.5457920792079208, 'eval_recall': 0.5028506271379704, 'eval_f1': 0.5234421364985163, 'eval_accuracy': 0.9529066625492164, 'eval_runtime': 4.8469, 'eval_samples_per_second': 315.875, 'eval_steps_per_second': 39.613, 'epoch': 18.0}
{'loss': 0.0018, 'learning_rate': 1.0416666666666668e-05, 'epoch': 19.0}
{'eval_loss': 0.4259006083011627, 'eval_precision': 0.5525059665871122, 'eval_recall': 0.5279361459521095, 'eval_f1': 0.5399416909620991, 'eval_accuracy': 0.9534084768007411, 'eval_runtime': 4.931, 'eval_samples_per_second': 310.483, 'eval_steps_per_second': 38.937, 'epoch': 19.0}
{'loss': 0.001, 'learning_rate': 8.333333333333334e-06, 'epoch': 20.0}
{'eval_loss': 0.4405198395252228, 'eval_precision': 0.5426267281105991, 'eval_recall': 0.5370581527936146, 'eval_f1': 0.5398280802292263, 'eval_accuracy': 0.9534470778970123, 'eval_runtime': 4.7928, 'eval_samples_per_second': 319.435, 'eval_steps_per_second': 40.06, 'epoch': 20.0}
{'loss': 0.0003, 'learning_rate': 6.25e-06, 'epoch': 21.0}
{'eval_loss': 0.46529248356819153, 'eval_precision': 0.5815789473684211, 'eval_recall': 0.5039908779931584, 'eval_f1': 0.5400122174709835, 'eval_accuracy': 0.954798116266502, 'eval_runtime': 4.899, 'eval_samples_per_second': 312.514, 'eval_steps_per_second': 39.192, 'epoch': 21.0}
{'loss': 0.0003, 'learning_rate': 4.166666666666667e-06, 'epoch': 22.0}
{'eval_loss': 0.4531136453151703, 'eval_precision': 0.5744157441574416, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5526627218934912, 'eval_accuracy': 0.9542190998224349, 'eval_runtime': 4.9202, 'eval_samples_per_second': 311.166, 'eval_steps_per_second': 39.023, 'epoch': 22.0}
{'loss': 0.0002, 'learning_rate': 2.0833333333333334e-06, 'epoch': 23.0}
{'eval_loss': 0.4697648286819458, 'eval_precision': 0.5764854614412137, 'eval_recall': 0.5199543899657925, 'eval_f1': 0.5467625899280576, 'eval_accuracy': 0.9543349031112484, 'eval_runtime': 4.9144, 'eval_samples_per_second': 311.537, 'eval_steps_per_second': 39.069, 'epoch': 23.0}
{'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 24.0}
{'eval_loss': 0.46931976079940796, 'eval_precision': 0.5651644336175395, 'eval_recall': 0.5290763968072976, 'eval_f1': 0.5465253239104829, 'eval_accuracy': 0.9541418976298927, 'eval_runtime': 4.8915, 'eval_samples_per_second': 312.994, 'eval_steps_per_second': 39.252, 'epoch': 24.0}
{'train_runtime': 3313.0797, 'train_samples_per_second': 62.545, 'train_steps_per_second': 7.824, 'train_loss': 0.02266793668149384, 'epoch': 24.0}
***** train metrics *****
  epoch                    =       24.0
  train_loss               =     0.0227
  train_runtime            = 0:55:13.07
  train_samples            =       8634
  train_samples_per_second =     62.545
  train_steps_per_second   =      7.824
[{'loss': 0.1946, 'learning_rate': 4.791666666666667e-05, 'epoch': 1.0, 'step': 1080}, {'eval_loss': 0.15200291574001312, 'eval_precision': 0.5506493506493506, 'eval_recall': 0.48346636259977194, 'eval_f1': 0.5148755312689739, 'eval_accuracy': 0.9530996680305721, 'eval_runtime': 4.9031, 'eval_samples_per_second': 312.25, 'eval_steps_per_second': 39.159, 'epoch': 1.0, 'step': 1080}, {'loss': 0.1151, 'learning_rate': 4.5833333333333334e-05, 'epoch': 2.0, 'step': 2160}, {'eval_loss': 0.15932069718837738, 'eval_precision': 0.5519568151147098, 'eval_recall': 0.4663625997719498, 'eval_f1': 0.5055624227441285, 'eval_accuracy': 0.9536786844746391, 'eval_runtime': 4.7514, 'eval_samples_per_second': 322.219, 'eval_steps_per_second': 40.409, 'epoch': 2.0, 'step': 2160}, {'loss': 0.0682, 'learning_rate': 4.375e-05, 'epoch': 3.0, 'step': 3240}, {'eval_loss': 0.18948908150196075, 'eval_precision': 0.5529131985731273, 'eval_recall': 0.5302166476624858, 'eval_f1': 0.5413271245634459, 'eval_accuracy': 0.9541804987261638, 'eval_runtime': 4.8893, 'eval_samples_per_second': 313.132, 'eval_steps_per_second': 39.269, 'epoch': 3.0, 'step': 3240}, {'loss': 0.0407, 'learning_rate': 4.166666666666667e-05, 'epoch': 4.0, 'step': 4320}, {'eval_loss': 0.2405519038438797, 'eval_precision': 0.4684854186265287, 'eval_recall': 0.5678449258836944, 'eval_f1': 0.5134020618556702, 'eval_accuracy': 0.9468462904346483, 'eval_runtime': 4.831, 'eval_samples_per_second': 316.908, 'eval_steps_per_second': 39.743, 'epoch': 4.0, 'step': 4320}, {'loss': 0.0263, 'learning_rate': 3.958333333333333e-05, 'epoch': 5.0, 'step': 5400}, {'eval_loss': 0.2783564329147339, 'eval_precision': 0.5789473684210527, 'eval_recall': 0.4766248574686431, 'eval_f1': 0.5228267667292057, 'eval_accuracy': 0.9540260943410793, 'eval_runtime': 4.9169, 'eval_samples_per_second': 311.378, 'eval_steps_per_second': 39.049, 'epoch': 5.0, 'step': 5400}, {'loss': 0.0175, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.0, 'step': 6480}, {'eval_loss': 0.30681315064430237, 'eval_precision': 0.5334928229665071, 'eval_recall': 0.508551881413911, 'eval_f1': 0.5207238762405137, 'eval_accuracy': 0.9508994055431175, 'eval_runtime': 4.8022, 'eval_samples_per_second': 318.814, 'eval_steps_per_second': 39.982, 'epoch': 6.0, 'step': 6480}, {'loss': 0.014, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.0, 'step': 7560}, {'eval_loss': 0.3398776352405548, 'eval_precision': 0.5214932126696833, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5235661555934128, 'eval_accuracy': 0.9511696132170153, 'eval_runtime': 4.7067, 'eval_samples_per_second': 325.279, 'eval_steps_per_second': 40.793, 'epoch': 7.0, 'step': 7560}, {'loss': 0.0123, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.0, 'step': 8640}, {'eval_loss': 0.35497117042541504, 'eval_precision': 0.5336481700118064, 'eval_recall': 0.5153933865450399, 'eval_f1': 0.5243619489559165, 'eval_accuracy': 0.95167142746854, 'eval_runtime': 4.9047, 'eval_samples_per_second': 312.148, 'eval_steps_per_second': 39.146, 'epoch': 8.0, 'step': 8640}, {'loss': 0.0088, 'learning_rate': 3.125e-05, 'epoch': 9.0, 'step': 9720}, {'eval_loss': 0.2987205684185028, 'eval_precision': 0.5162374020156775, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5209039548022598, 'eval_accuracy': 0.9506291978692195, 'eval_runtime': 4.9274, 'eval_samples_per_second': 310.709, 'eval_steps_per_second': 38.965, 'epoch': 9.0, 'step': 9720}, {'loss': 0.0085, 'learning_rate': 2.916666666666667e-05, 'epoch': 10.0, 'step': 10800}, {'eval_loss': 0.3276006281375885, 'eval_precision': 0.5389784946236559, 'eval_recall': 0.4572405929304447, 'eval_f1': 0.4947563232572486, 'eval_accuracy': 0.9508994055431175, 'eval_runtime': 4.7859, 'eval_samples_per_second': 319.9, 'eval_steps_per_second': 40.118, 'epoch': 10.0, 'step': 10800}, {'loss': 0.0072, 'learning_rate': 2.7083333333333332e-05, 'epoch': 11.0, 'step': 11880}, {'eval_loss': 0.3525168299674988, 'eval_precision': 0.5327313769751693, 'eval_recall': 0.5381984036488028, 'eval_f1': 0.5354509359047078, 'eval_accuracy': 0.9529066625492164, 'eval_runtime': 4.9172, 'eval_samples_per_second': 311.355, 'eval_steps_per_second': 39.046, 'epoch': 11.0, 'step': 11880}, {'loss': 0.0078, 'learning_rate': 2.5e-05, 'epoch': 12.0, 'step': 12960}, {'eval_loss': 0.39575713872909546, 'eval_precision': 0.5653846153846154, 'eval_recall': 0.5028506271379704, 'eval_f1': 0.5322872661436331, 'eval_accuracy': 0.9527522581641319, 'eval_runtime': 4.9582, 'eval_samples_per_second': 308.783, 'eval_steps_per_second': 38.724, 'epoch': 12.0, 'step': 12960}, {'loss': 0.0039, 'learning_rate': 2.2916666666666667e-05, 'epoch': 13.0, 'step': 14040}, {'eval_loss': 0.40147319436073303, 'eval_precision': 0.510548523206751, 'eval_recall': 0.5518814139110604, 'eval_f1': 0.5304109589041096, 'eval_accuracy': 0.9503589901953216, 'eval_runtime': 4.866, 'eval_samples_per_second': 314.632, 'eval_steps_per_second': 39.457, 'epoch': 13.0, 'step': 14040}, {'loss': 0.0043, 'learning_rate': 2.0833333333333336e-05, 'epoch': 14.0, 'step': 15120}, {'eval_loss': 0.4153269827365875, 'eval_precision': 0.5572413793103448, 'eval_recall': 0.4606613454960091, 'eval_f1': 0.5043695380774033, 'eval_accuracy': 0.9541418976298927, 'eval_runtime': 4.9055, 'eval_samples_per_second': 312.097, 'eval_steps_per_second': 39.14, 'epoch': 14.0, 'step': 15120}, {'loss': 0.0038, 'learning_rate': 1.8750000000000002e-05, 'epoch': 15.0, 'step': 16200}, {'eval_loss': 0.3622564375400543, 'eval_precision': 0.5663600525624178, 'eval_recall': 0.49144811858608894, 'eval_f1': 0.5262515262515262, 'eval_accuracy': 0.9528294603566742, 'eval_runtime': 4.9031, 'eval_samples_per_second': 312.251, 'eval_steps_per_second': 39.159, 'epoch': 15.0, 'step': 16200}, {'loss': 0.0027, 'learning_rate': 1.6666666666666667e-05, 'epoch': 16.0, 'step': 17280}, {'eval_loss': 0.38585445284843445, 'eval_precision': 0.5442739079102715, 'eval_recall': 0.5256556442417332, 'eval_f1': 0.5348027842227379, 'eval_accuracy': 0.9525592526827762, 'eval_runtime': 4.8452, 'eval_samples_per_second': 315.982, 'eval_steps_per_second': 39.627, 'epoch': 16.0, 'step': 17280}, {'loss': 0.0033, 'learning_rate': 1.4583333333333335e-05, 'epoch': 17.0, 'step': 18360}, {'eval_loss': 0.40605276823043823, 'eval_precision': 0.5364077669902912, 'eval_recall': 0.5039908779931584, 'eval_f1': 0.5196942974720752, 'eval_accuracy': 0.9514398208909133, 'eval_runtime': 4.8787, 'eval_samples_per_second': 313.815, 'eval_steps_per_second': 39.355, 'epoch': 17.0, 'step': 18360}, {'loss': 0.0013, 'learning_rate': 1.25e-05, 'epoch': 18.0, 'step': 19440}, {'eval_loss': 0.4425526559352875, 'eval_precision': 0.5457920792079208, 'eval_recall': 0.5028506271379704, 'eval_f1': 0.5234421364985163, 'eval_accuracy': 0.9529066625492164, 'eval_runtime': 4.8469, 'eval_samples_per_second': 315.875, 'eval_steps_per_second': 39.613, 'epoch': 18.0, 'step': 19440}, {'loss': 0.0018, 'learning_rate': 1.0416666666666668e-05, 'epoch': 19.0, 'step': 20520}, {'eval_loss': 0.4259006083011627, 'eval_precision': 0.5525059665871122, 'eval_recall': 0.5279361459521095, 'eval_f1': 0.5399416909620991, 'eval_accuracy': 0.9534084768007411, 'eval_runtime': 4.931, 'eval_samples_per_second': 310.483, 'eval_steps_per_second': 38.937, 'epoch': 19.0, 'step': 20520}, {'loss': 0.001, 'learning_rate': 8.333333333333334e-06, 'epoch': 20.0, 'step': 21600}, {'eval_loss': 0.4405198395252228, 'eval_precision': 0.5426267281105991, 'eval_recall': 0.5370581527936146, 'eval_f1': 0.5398280802292263, 'eval_accuracy': 0.9534470778970123, 'eval_runtime': 4.7928, 'eval_samples_per_second': 319.435, 'eval_steps_per_second': 40.06, 'epoch': 20.0, 'step': 21600}, {'loss': 0.0003, 'learning_rate': 6.25e-06, 'epoch': 21.0, 'step': 22680}, {'eval_loss': 0.46529248356819153, 'eval_precision': 0.5815789473684211, 'eval_recall': 0.5039908779931584, 'eval_f1': 0.5400122174709835, 'eval_accuracy': 0.954798116266502, 'eval_runtime': 4.899, 'eval_samples_per_second': 312.514, 'eval_steps_per_second': 39.192, 'epoch': 21.0, 'step': 22680}, {'loss': 0.0003, 'learning_rate': 4.166666666666667e-06, 'epoch': 22.0, 'step': 23760}, {'eval_loss': 0.4531136453151703, 'eval_precision': 0.5744157441574416, 'eval_recall': 0.5324971493728621, 'eval_f1': 0.5526627218934912, 'eval_accuracy': 0.9542190998224349, 'eval_runtime': 4.9202, 'eval_samples_per_second': 311.166, 'eval_steps_per_second': 39.023, 'epoch': 22.0, 'step': 23760}, {'loss': 0.0002, 'learning_rate': 2.0833333333333334e-06, 'epoch': 23.0, 'step': 24840}, {'eval_loss': 0.4697648286819458, 'eval_precision': 0.5764854614412137, 'eval_recall': 0.5199543899657925, 'eval_f1': 0.5467625899280576, 'eval_accuracy': 0.9543349031112484, 'eval_runtime': 4.9144, 'eval_samples_per_second': 311.537, 'eval_steps_per_second': 39.069, 'epoch': 23.0, 'step': 24840}, {'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 24.0, 'step': 25920}, {'eval_loss': 0.46931976079940796, 'eval_precision': 0.5651644336175395, 'eval_recall': 0.5290763968072976, 'eval_f1': 0.5465253239104829, 'eval_accuracy': 0.9541418976298927, 'eval_runtime': 4.8915, 'eval_samples_per_second': 312.994, 'eval_steps_per_second': 39.252, 'epoch': 24.0, 'step': 25920}, {'train_runtime': 3313.0797, 'train_samples_per_second': 62.545, 'train_steps_per_second': 7.824, 'total_flos': 1.693654095901446e+16, 'train_loss': 0.02266793668149384, 'epoch': 24.0, 'step': 25920}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =      0.951
  predict_f1                 =     0.4885
  predict_loss               =     0.1689
  predict_precision          =     0.5395
  predict_recall             =     0.4463
  predict_runtime            = 0:00:04.26
  predict_samples_per_second =    298.536
  predict_steps_per_second   =     37.317
Train and save best epoch to /fp/homes01/u01/ec-egilron/sqlabel-github/configs/fox/01191518_tsa-bin_NB-BERT_large_11.json completed. F1: 0.48845867460908415

Task and CPU usage stats:
JobID           JobName  AllocCPUS   NTasks     MinCPU MinCPUTask     AveCPU    Elapsed ExitCode 
------------ ---------- ---------- -------- ---------- ---------- ---------- ---------- -------- 
411428       tsa_norbe+          1                                             03:58:46      0:0 
411428.batch      batch          1        1   03:44:52          0   03:44:52   03:58:46      0:0 
411428.exte+     extern          1        1   00:00:00          0   00:00:00   03:58:46      0:0 

Memory usage stats:
JobID            MaxRSS MaxRSSTask     AveRSS MaxPages   MaxPagesTask   AvePages 
------------ ---------- ---------- ---------- -------- -------------- ---------- 
411428                                                                           
411428.batch   4306084K          0   4306084K        0              0          0 
411428.exte+          0          0          0        0              0          0 

Disk usage stats:
JobID         MaxDiskRead MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteTask   AveDiskWrite 
------------ ------------ --------------- -------------- ------------ ---------------- -------------- 
411428                                                                                                
411428.batch     5112.88M               0       5112.88M   489457.07M                0     489457.07M 
411428.exte+        0.01M               0          0.01M        0.00M                0          0.00M 

GPU usage stats:
Successfully retrieved statistics for job: 411428. 
+------------------------------------------------------------------------------+
| GPU ID: 2                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Thu Jan 25 02:03:40 2024                |
| End Time                           | Thu Jan 25 06:02:26 2024                |
| Total Execution Time (sec)         | 14326                                   |
| No. of Processes                   | 1                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 648187                                  |
| Power Usage (Watts)                | Avg: 198.995, Max: 327.856, Min: 46.909 |
| Max GPU Memory Used (bytes)        | 33791410176                             |
| SM Clock (MHz)                     | Avg: 1279, Max: 1410, Min: 765          |
| Memory Clock (MHz)                 | Avg: 1215, Max: 1215, Min: 1215         |
| SM Utilization (%)                 | Avg: 69, Max: 100, Min: 0               |
| Memory Utilization (%)             | Avg: 30, Max: 45, Min: 0                |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 4004319                                 |
|     Avg SM Utilization (%)         | 67                                      |
|     Avg Memory Utilization (%)     | 26                                      |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+


Job 411428 completed at Thu Jan 25 06:02:26 CET 2024
