Starting job 10922760 on gpu-12-7 on saga at Tue Mar 12 15:55:55 CET 2024

/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_101.json
03121434_elsa-polarity_norbert3-large_03_101.json seems to be completed. Exiting
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_202.json
03121434_elsa-polarity_nb-bert_base_11_202.json seems to be completed. Exiting
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_303.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3428.90 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4003.00 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4663.75 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5043.51 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5699.49 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6275.81 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6275.87 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 3568.29 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 3865.35 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 3674.29 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6342.60 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4393.69 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 1202.74 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:01<00:00, 1215.54 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:01<00:00, 1208.81 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-1876 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1001, 'grad_norm': 0.33722248673439026, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.041277918964624405, 'eval_precision': 0.6, 'eval_recall': 0.6618357487922706, 'eval_f1': 0.6294027565084227, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 5.7523, 'eval_samples_per_second': 263.025, 'eval_steps_per_second': 33.03, 'epoch': 1.0}
{'loss': 0.0299, 'grad_norm': 1.0515904426574707, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.037382498383522034, 'eval_precision': 0.6972477064220184, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7152941176470589, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.5977, 'eval_samples_per_second': 270.29, 'eval_steps_per_second': 33.943, 'epoch': 2.0}
{'loss': 0.0181, 'grad_norm': 0.8455807566642761, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03803006932139397, 'eval_precision': 0.6877828054298643, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7102803738317759, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 5.5697, 'eval_samples_per_second': 271.649, 'eval_steps_per_second': 34.113, 'epoch': 3.0}
{'loss': 0.01, 'grad_norm': 0.11997029930353165, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04462335631251335, 'eval_precision': 0.7258566978193146, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7379255740300871, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 6.9881, 'eval_samples_per_second': 216.51, 'eval_steps_per_second': 27.189, 'epoch': 4.0}
{'loss': 0.0052, 'grad_norm': 0.31224143505096436, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04642817750573158, 'eval_precision': 0.7342549923195084, 'eval_recall': 0.7697262479871175, 'eval_f1': 0.7515723270440252, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 5.6089, 'eval_samples_per_second': 269.749, 'eval_steps_per_second': 33.875, 'epoch': 5.0}
{'loss': 0.0026, 'grad_norm': 0.3446512818336487, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04995260760188103, 'eval_precision': 0.7413793103448276, 'eval_recall': 0.7616747181964574, 'eval_f1': 0.7513899920571884, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 5.6163, 'eval_samples_per_second': 269.392, 'eval_steps_per_second': 33.83, 'epoch': 6.0}
{'loss': 0.0017, 'grad_norm': 0.04738480970263481, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05414402857422829, 'eval_precision': 0.7396166134185304, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7425821972734563, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.7267, 'eval_samples_per_second': 264.2, 'eval_steps_per_second': 33.178, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.06230771169066429, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05799846723675728, 'eval_precision': 0.7169811320754716, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7255369928400954, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.7439, 'eval_samples_per_second': 263.409, 'eval_steps_per_second': 33.079, 'epoch': 8.0}
{'loss': 0.0007, 'grad_norm': 0.02051304653286934, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05757671222090721, 'eval_precision': 0.7492063492063492, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7545963229416467, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 6.112, 'eval_samples_per_second': 247.544, 'eval_steps_per_second': 31.086, 'epoch': 9.0}
{'loss': 0.0006, 'grad_norm': 0.0007355866837315261, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.058816682547330856, 'eval_precision': 0.7592891760904685, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7580645161290324, 'eval_accuracy': 0.9903857427599954, 'eval_runtime': 5.7694, 'eval_samples_per_second': 262.247, 'eval_steps_per_second': 32.932, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.15471915900707245, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06016366928815842, 'eval_precision': 0.7609046849757674, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7596774193548387, 'eval_accuracy': 0.9904639074530035, 'eval_runtime': 5.7424, 'eval_samples_per_second': 263.476, 'eval_steps_per_second': 33.087, 'epoch': 11.0}
{'loss': 0.0003, 'grad_norm': 0.05512375012040138, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.060093529522418976, 'eval_precision': 0.7523659305993691, 'eval_recall': 0.7681159420289855, 'eval_f1': 0.7601593625498008, 'eval_accuracy': 0.990268495720483, 'eval_runtime': 5.7677, 'eval_samples_per_second': 262.321, 'eval_steps_per_second': 32.942, 'epoch': 12.0}
{'train_runtime': 1790.0864, 'train_samples_per_second': 57.45, 'train_steps_per_second': 1.797, 'train_loss': 0.014217957421043767, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0142
  train_runtime            = 0:29:50.08
  train_samples            =       8570
  train_samples_per_second =      57.45
  train_steps_per_second   =      1.797
[{'loss': 0.1001, 'grad_norm': 0.33722248673439026, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.041277918964624405, 'eval_precision': 0.6, 'eval_recall': 0.6618357487922706, 'eval_f1': 0.6294027565084227, 'eval_accuracy': 0.9863602610700747, 'eval_runtime': 5.7523, 'eval_samples_per_second': 263.025, 'eval_steps_per_second': 33.03, 'epoch': 1.0, 'step': 268}, {'loss': 0.0299, 'grad_norm': 1.0515904426574707, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.037382498383522034, 'eval_precision': 0.6972477064220184, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7152941176470589, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.5977, 'eval_samples_per_second': 270.29, 'eval_steps_per_second': 33.943, 'epoch': 2.0, 'step': 536}, {'loss': 0.0181, 'grad_norm': 0.8455807566642761, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.03803006932139397, 'eval_precision': 0.6877828054298643, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7102803738317759, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 5.5697, 'eval_samples_per_second': 271.649, 'eval_steps_per_second': 34.113, 'epoch': 3.0, 'step': 804}, {'loss': 0.01, 'grad_norm': 0.11997029930353165, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04462335631251335, 'eval_precision': 0.7258566978193146, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7379255740300871, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 6.9881, 'eval_samples_per_second': 216.51, 'eval_steps_per_second': 27.189, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0052, 'grad_norm': 0.31224143505096436, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.04642817750573158, 'eval_precision': 0.7342549923195084, 'eval_recall': 0.7697262479871175, 'eval_f1': 0.7515723270440252, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 5.6089, 'eval_samples_per_second': 269.749, 'eval_steps_per_second': 33.875, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0026, 'grad_norm': 0.3446512818336487, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.04995260760188103, 'eval_precision': 0.7413793103448276, 'eval_recall': 0.7616747181964574, 'eval_f1': 0.7513899920571884, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 5.6163, 'eval_samples_per_second': 269.392, 'eval_steps_per_second': 33.83, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0017, 'grad_norm': 0.04738480970263481, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05414402857422829, 'eval_precision': 0.7396166134185304, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7425821972734563, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.7267, 'eval_samples_per_second': 264.2, 'eval_steps_per_second': 33.178, 'epoch': 7.0, 'step': 1876}, {'loss': 0.001, 'grad_norm': 0.06230771169066429, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05799846723675728, 'eval_precision': 0.7169811320754716, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7255369928400954, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.7439, 'eval_samples_per_second': 263.409, 'eval_steps_per_second': 33.079, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0007, 'grad_norm': 0.02051304653286934, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05757671222090721, 'eval_precision': 0.7492063492063492, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7545963229416467, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 6.112, 'eval_samples_per_second': 247.544, 'eval_steps_per_second': 31.086, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0006, 'grad_norm': 0.0007355866837315261, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.058816682547330856, 'eval_precision': 0.7592891760904685, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7580645161290324, 'eval_accuracy': 0.9903857427599954, 'eval_runtime': 5.7694, 'eval_samples_per_second': 262.247, 'eval_steps_per_second': 32.932, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0004, 'grad_norm': 0.15471915900707245, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06016366928815842, 'eval_precision': 0.7609046849757674, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7596774193548387, 'eval_accuracy': 0.9904639074530035, 'eval_runtime': 5.7424, 'eval_samples_per_second': 263.476, 'eval_steps_per_second': 33.087, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0003, 'grad_norm': 0.05512375012040138, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.060093529522418976, 'eval_precision': 0.7523659305993691, 'eval_recall': 0.7681159420289855, 'eval_f1': 0.7601593625498008, 'eval_accuracy': 0.990268495720483, 'eval_runtime': 5.7677, 'eval_samples_per_second': 262.321, 'eval_steps_per_second': 32.942, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1790.0864, 'train_samples_per_second': 57.45, 'train_steps_per_second': 1.797, 'total_flos': 1.1778916405982184e+16, 'train_loss': 0.014217957421043767, 'epoch': 12.0, 'step': 3216}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =      0.991
  predict_f1                 =     0.7315
  predict_loss               =     0.0311
  predict_precision          =     0.7061
  predict_recall             =     0.7588
  predict_runtime            = 0:00:04.85
  predict_samples_per_second =    257.793
  predict_steps_per_second   =     32.327
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_303.json completed. F1: 0.7315010570824524
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_202.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4496.24 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4415.63 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5360.22 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 4948.28 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5603.62 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6171.29 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6460.51 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6815.11 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5969.99 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6965.27 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5065.01 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7269.86 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4672.43 examples/s]
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0635, 'grad_norm': 0.38194674253463745, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.0424802266061306, 'eval_precision': 0.6133518776077886, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6582089552238806, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 4.673, 'eval_samples_per_second': 323.778, 'eval_steps_per_second': 40.659, 'epoch': 1.0}
{'loss': 0.0277, 'grad_norm': 0.7162355184555054, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.03838856518268585, 'eval_precision': 0.6908212560386473, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6908212560386473, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.577, 'eval_samples_per_second': 330.564, 'eval_steps_per_second': 41.512, 'epoch': 2.0}
{'loss': 0.0142, 'grad_norm': 0.4311715066432953, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04240937530994415, 'eval_precision': 0.7218543046357616, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.7118367346938775, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5751, 'eval_samples_per_second': 330.704, 'eval_steps_per_second': 41.529, 'epoch': 3.0}
{'loss': 0.0062, 'grad_norm': 0.14959830045700073, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05032643303275108, 'eval_precision': 0.762684124386252, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7564935064935066, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 4.7109, 'eval_samples_per_second': 321.171, 'eval_steps_per_second': 40.332, 'epoch': 4.0}
{'loss': 0.0036, 'grad_norm': 0.2566470503807068, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.0457819327712059, 'eval_precision': 0.7408, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7431781701444623, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 4.5707, 'eval_samples_per_second': 331.019, 'eval_steps_per_second': 41.569, 'epoch': 5.0}
{'loss': 0.0016, 'grad_norm': 0.003911319654434919, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.056913819164037704, 'eval_precision': 0.7165354330708661, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7245222929936306, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.8111, 'eval_samples_per_second': 314.481, 'eval_steps_per_second': 39.492, 'epoch': 6.0}
{'loss': 0.0008, 'grad_norm': 0.010101248510181904, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06120152026414871, 'eval_precision': 0.7275590551181103, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7356687898089173, 'eval_accuracy': 0.9891351076718646, 'eval_runtime': 4.6451, 'eval_samples_per_second': 325.717, 'eval_steps_per_second': 40.903, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.0010835914872586727, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06482048332691193, 'eval_precision': 0.7598039215686274, 'eval_recall': 0.748792270531401, 'eval_f1': 0.754257907542579, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.6366, 'eval_samples_per_second': 326.315, 'eval_steps_per_second': 40.978, 'epoch': 8.0}
{'loss': 0.0003, 'grad_norm': 0.8396602869033813, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06528203934431076, 'eval_precision': 0.7452229299363057, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7493995196156926, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 4.9005, 'eval_samples_per_second': 308.742, 'eval_steps_per_second': 38.771, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.011526551097631454, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.0666452944278717, 'eval_precision': 0.7370078740157481, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7452229299363057, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6246, 'eval_samples_per_second': 327.164, 'eval_steps_per_second': 41.085, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.00020442645472940058, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07215966284275055, 'eval_precision': 0.7301587301587301, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7354116706634692, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.8517, 'eval_samples_per_second': 311.849, 'eval_steps_per_second': 39.162, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 0.00057220458984375, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07261250913143158, 'eval_precision': 0.7364217252396166, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.739374498797113, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.6338, 'eval_samples_per_second': 326.511, 'eval_steps_per_second': 41.003, 'epoch': 12.0}
{'train_runtime': 1024.2771, 'train_samples_per_second': 100.403, 'train_steps_per_second': 3.14, 'train_loss': 0.00993923171491719, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0099
  train_runtime            = 0:17:04.27
  train_samples            =       8570
  train_samples_per_second =    100.403
  train_steps_per_second   =       3.14
[{'loss': 0.0635, 'grad_norm': 0.38194674253463745, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.0424802266061306, 'eval_precision': 0.6133518776077886, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6582089552238806, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 4.673, 'eval_samples_per_second': 323.778, 'eval_steps_per_second': 40.659, 'epoch': 1.0, 'step': 268}, {'loss': 0.0277, 'grad_norm': 0.7162355184555054, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03838856518268585, 'eval_precision': 0.6908212560386473, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6908212560386473, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.577, 'eval_samples_per_second': 330.564, 'eval_steps_per_second': 41.512, 'epoch': 2.0, 'step': 536}, {'loss': 0.0142, 'grad_norm': 0.4311715066432953, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04240937530994415, 'eval_precision': 0.7218543046357616, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.7118367346938775, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5751, 'eval_samples_per_second': 330.704, 'eval_steps_per_second': 41.529, 'epoch': 3.0, 'step': 804}, {'loss': 0.0062, 'grad_norm': 0.14959830045700073, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05032643303275108, 'eval_precision': 0.762684124386252, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7564935064935066, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 4.7109, 'eval_samples_per_second': 321.171, 'eval_steps_per_second': 40.332, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0036, 'grad_norm': 0.2566470503807068, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.0457819327712059, 'eval_precision': 0.7408, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7431781701444623, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 4.5707, 'eval_samples_per_second': 331.019, 'eval_steps_per_second': 41.569, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0016, 'grad_norm': 0.003911319654434919, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.056913819164037704, 'eval_precision': 0.7165354330708661, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7245222929936306, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.8111, 'eval_samples_per_second': 314.481, 'eval_steps_per_second': 39.492, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0008, 'grad_norm': 0.010101248510181904, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.06120152026414871, 'eval_precision': 0.7275590551181103, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7356687898089173, 'eval_accuracy': 0.9891351076718646, 'eval_runtime': 4.6451, 'eval_samples_per_second': 325.717, 'eval_steps_per_second': 40.903, 'epoch': 7.0, 'step': 1876}, {'loss': 0.001, 'grad_norm': 0.0010835914872586727, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06482048332691193, 'eval_precision': 0.7598039215686274, 'eval_recall': 0.748792270531401, 'eval_f1': 0.754257907542579, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.6366, 'eval_samples_per_second': 326.315, 'eval_steps_per_second': 40.978, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0003, 'grad_norm': 0.8396602869033813, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06528203934431076, 'eval_precision': 0.7452229299363057, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7493995196156926, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 4.9005, 'eval_samples_per_second': 308.742, 'eval_steps_per_second': 38.771, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.011526551097631454, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.0666452944278717, 'eval_precision': 0.7370078740157481, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7452229299363057, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6246, 'eval_samples_per_second': 327.164, 'eval_steps_per_second': 41.085, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0001, 'grad_norm': 0.00020442645472940058, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.07215966284275055, 'eval_precision': 0.7301587301587301, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7354116706634692, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.8517, 'eval_samples_per_second': 311.849, 'eval_steps_per_second': 39.162, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0, 'grad_norm': 0.00057220458984375, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.07261250913143158, 'eval_precision': 0.7364217252396166, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.739374498797113, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.6338, 'eval_samples_per_second': 326.511, 'eval_steps_per_second': 41.003, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1024.2771, 'train_samples_per_second': 100.403, 'train_steps_per_second': 3.14, 'total_flos': 1.1358239156790372e+16, 'train_loss': 0.00993923171491719, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9904
  predict_f1                 =     0.7295
  predict_loss               =     0.0342
  predict_precision          =     0.7287
  predict_recall             =     0.7303
  predict_runtime            = 0:00:03.91
  predict_samples_per_second =    319.827
  predict_steps_per_second   =     40.106
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_202.json completed. F1: 0.7294633077765609
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_101.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4384.39 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5007.05 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5608.59 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6166.17 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6467.74 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6773.59 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6839.66 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7033.55 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6368.85 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7110.34 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 6173.46 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6705.97 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4737.01 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1803, 'grad_norm': 0.3021170496940613, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.056314174085855484, 'eval_precision': 0.5709779179810726, 'eval_recall': 0.5829307568438004, 'eval_f1': 0.5768924302788845, 'eval_accuracy': 0.981904873568609, 'eval_runtime': 2.3883, 'eval_samples_per_second': 633.511, 'eval_steps_per_second': 79.555, 'epoch': 1.0}
{'loss': 0.0479, 'grad_norm': 0.31072548031806946, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04788731411099434, 'eval_precision': 0.4574468085106383, 'eval_recall': 0.5539452495974235, 'eval_f1': 0.5010924981791697, 'eval_accuracy': 0.9833900027357643, 'eval_runtime': 2.3292, 'eval_samples_per_second': 649.571, 'eval_steps_per_second': 81.572, 'epoch': 2.0}
{'loss': 0.0371, 'grad_norm': 0.4368046820163727, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04349105805158615, 'eval_precision': 0.6388467374810318, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6578125, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3363, 'eval_samples_per_second': 647.61, 'eval_steps_per_second': 81.326, 'epoch': 3.0}
{'loss': 0.0301, 'grad_norm': 0.34863704442977905, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04428482428193092, 'eval_precision': 0.6504559270516718, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6692728694292416, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 2.3418, 'eval_samples_per_second': 646.093, 'eval_steps_per_second': 81.135, 'epoch': 4.0}
{'loss': 0.0259, 'grad_norm': 0.4777488708496094, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04485379904508591, 'eval_precision': 0.656441717791411, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6724273369992144, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.334, 'eval_samples_per_second': 648.241, 'eval_steps_per_second': 81.405, 'epoch': 5.0}
{'loss': 0.0217, 'grad_norm': 0.418973833322525, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.048840105533599854, 'eval_precision': 0.648036253776435, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6687451286048324, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3761, 'eval_samples_per_second': 636.762, 'eval_steps_per_second': 79.964, 'epoch': 6.0}
{'loss': 0.0184, 'grad_norm': 0.19477644562721252, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04675609618425369, 'eval_precision': 0.6565809379727685, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6770670826833073, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.3348, 'eval_samples_per_second': 648.031, 'eval_steps_per_second': 81.379, 'epoch': 7.0}
{'loss': 0.0153, 'grad_norm': 0.5616666674613953, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.0481591671705246, 'eval_precision': 0.6743119266055045, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.691764705882353, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.3103, 'eval_samples_per_second': 654.901, 'eval_steps_per_second': 82.241, 'epoch': 8.0}
{'loss': 0.0134, 'grad_norm': 0.6107197403907776, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.04953425005078316, 'eval_precision': 0.6676737160120846, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6890101325019486, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 2.4174, 'eval_samples_per_second': 625.878, 'eval_steps_per_second': 78.597, 'epoch': 9.0}
{'loss': 0.0114, 'grad_norm': 0.3673584461212158, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05111922696232796, 'eval_precision': 0.6758409785932722, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6933333333333334, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 2.3402, 'eval_samples_per_second': 646.517, 'eval_steps_per_second': 81.188, 'epoch': 10.0}
{'loss': 0.0103, 'grad_norm': 0.3822803199291229, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.0529426671564579, 'eval_precision': 0.6748091603053435, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6927899686520377, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.3116, 'eval_samples_per_second': 654.534, 'eval_steps_per_second': 82.195, 'epoch': 11.0}
{'loss': 0.0096, 'grad_norm': 0.3379829525947571, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05318070575594902, 'eval_precision': 0.6763803680981595, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6928515318146111, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.5241, 'eval_samples_per_second': 599.411, 'eval_steps_per_second': 75.273, 'epoch': 12.0}
{'train_runtime': 509.4264, 'train_samples_per_second': 201.874, 'train_steps_per_second': 3.156, 'train_loss': 0.03510577050014515, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0351
  train_runtime            = 0:08:29.42
  train_samples            =       8570
  train_samples_per_second =    201.874
  train_steps_per_second   =      3.156
[{'loss': 0.1803, 'grad_norm': 0.3021170496940613, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.056314174085855484, 'eval_precision': 0.5709779179810726, 'eval_recall': 0.5829307568438004, 'eval_f1': 0.5768924302788845, 'eval_accuracy': 0.981904873568609, 'eval_runtime': 2.3883, 'eval_samples_per_second': 633.511, 'eval_steps_per_second': 79.555, 'epoch': 1.0, 'step': 134}, {'loss': 0.0479, 'grad_norm': 0.31072548031806946, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04788731411099434, 'eval_precision': 0.4574468085106383, 'eval_recall': 0.5539452495974235, 'eval_f1': 0.5010924981791697, 'eval_accuracy': 0.9833900027357643, 'eval_runtime': 2.3292, 'eval_samples_per_second': 649.571, 'eval_steps_per_second': 81.572, 'epoch': 2.0, 'step': 268}, {'loss': 0.0371, 'grad_norm': 0.4368046820163727, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04349105805158615, 'eval_precision': 0.6388467374810318, 'eval_recall': 0.677938808373591, 'eval_f1': 0.6578125, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 2.3363, 'eval_samples_per_second': 647.61, 'eval_steps_per_second': 81.326, 'epoch': 3.0, 'step': 402}, {'loss': 0.0301, 'grad_norm': 0.34863704442977905, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04428482428193092, 'eval_precision': 0.6504559270516718, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6692728694292416, 'eval_accuracy': 0.9861257669910501, 'eval_runtime': 2.3418, 'eval_samples_per_second': 646.093, 'eval_steps_per_second': 81.135, 'epoch': 4.0, 'step': 536}, {'loss': 0.0259, 'grad_norm': 0.4777488708496094, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04485379904508591, 'eval_precision': 0.656441717791411, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6724273369992144, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.334, 'eval_samples_per_second': 648.241, 'eval_steps_per_second': 81.405, 'epoch': 5.0, 'step': 670}, {'loss': 0.0217, 'grad_norm': 0.418973833322525, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.048840105533599854, 'eval_precision': 0.648036253776435, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6687451286048324, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3761, 'eval_samples_per_second': 636.762, 'eval_steps_per_second': 79.964, 'epoch': 6.0, 'step': 804}, {'loss': 0.0184, 'grad_norm': 0.19477644562721252, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.04675609618425369, 'eval_precision': 0.6565809379727685, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6770670826833073, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.3348, 'eval_samples_per_second': 648.031, 'eval_steps_per_second': 81.379, 'epoch': 7.0, 'step': 938}, {'loss': 0.0153, 'grad_norm': 0.5616666674613953, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.0481591671705246, 'eval_precision': 0.6743119266055045, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.691764705882353, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.3103, 'eval_samples_per_second': 654.901, 'eval_steps_per_second': 82.241, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0134, 'grad_norm': 0.6107197403907776, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.04953425005078316, 'eval_precision': 0.6676737160120846, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6890101325019486, 'eval_accuracy': 0.9871419080001563, 'eval_runtime': 2.4174, 'eval_samples_per_second': 625.878, 'eval_steps_per_second': 78.597, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0114, 'grad_norm': 0.3673584461212158, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05111922696232796, 'eval_precision': 0.6758409785932722, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6933333333333334, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 2.3402, 'eval_samples_per_second': 646.517, 'eval_steps_per_second': 81.188, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0103, 'grad_norm': 0.3822803199291229, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.0529426671564579, 'eval_precision': 0.6748091603053435, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6927899686520377, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.3116, 'eval_samples_per_second': 654.534, 'eval_steps_per_second': 82.195, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0096, 'grad_norm': 0.3379829525947571, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05318070575594902, 'eval_precision': 0.6763803680981595, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6928515318146111, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.5241, 'eval_samples_per_second': 599.411, 'eval_steps_per_second': 75.273, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 509.4264, 'train_samples_per_second': 201.874, 'train_steps_per_second': 3.156, 'total_flos': 4438602656880876.0, 'train_loss': 0.03510577050014515, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9886
  predict_f1                 =     0.6738
  predict_loss               =     0.0391
  predict_precision          =     0.6536
  predict_recall             =     0.6952
  predict_runtime            = 0:00:02.15
  predict_samples_per_second =    581.627
  predict_steps_per_second   =     72.936
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_101.json completed. F1: 0.67375132837407
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_303.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4713.09 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4274.70 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:03, 1732.05 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:02<00:02, 1742.75 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:02<00:01, 2400.33 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:02<00:01, 2179.44 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:03<00:00, 1794.10 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:03<00:00, 2373.56 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:04<00:00, 1906.82 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:05<00:00, 1547.91 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7332.34 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4500.96 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6073.38 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4446.85 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0905, 'grad_norm': 0.40369945764541626, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04131627827882767, 'eval_precision': 0.6011816838995568, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6271186440677967, 'eval_accuracy': 0.9856567788330012, 'eval_runtime': 4.6345, 'eval_samples_per_second': 326.464, 'eval_steps_per_second': 40.997, 'epoch': 1.0}
{'loss': 0.0336, 'grad_norm': 0.6401016712188721, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03773234412074089, 'eval_precision': 0.6414814814814814, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6682098765432097, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6336, 'eval_samples_per_second': 326.526, 'eval_steps_per_second': 41.005, 'epoch': 2.0}
{'loss': 0.0219, 'grad_norm': 1.2175571918487549, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03673283010721207, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7349491790461299, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.5759, 'eval_samples_per_second': 330.645, 'eval_steps_per_second': 41.522, 'epoch': 3.0}
{'loss': 0.0145, 'grad_norm': 0.700518786907196, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04201001673936844, 'eval_precision': 0.722397476340694, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7298804780876494, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.5937, 'eval_samples_per_second': 329.366, 'eval_steps_per_second': 41.361, 'epoch': 4.0}
{'loss': 0.0092, 'grad_norm': 0.26676979660987854, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04345109686255455, 'eval_precision': 0.7229832572298326, 'eval_recall': 0.7648953301127214, 'eval_f1': 0.7433489827856025, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.5697, 'eval_samples_per_second': 331.094, 'eval_steps_per_second': 41.578, 'epoch': 5.0}
{'loss': 0.0057, 'grad_norm': 0.41163507103919983, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04947509616613388, 'eval_precision': 0.7228346456692913, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7308917197452229, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.5852, 'eval_samples_per_second': 329.978, 'eval_steps_per_second': 41.438, 'epoch': 6.0}
{'loss': 0.0037, 'grad_norm': 0.18499651551246643, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.051767949014902115, 'eval_precision': 0.7399678972712681, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7411575562700964, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 4.5897, 'eval_samples_per_second': 329.648, 'eval_steps_per_second': 41.397, 'epoch': 7.0}
{'loss': 0.0025, 'grad_norm': 0.07017809897661209, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05381917953491211, 'eval_precision': 0.7268518518518519, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7423167848699764, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6125, 'eval_samples_per_second': 328.024, 'eval_steps_per_second': 41.193, 'epoch': 8.0}
{'loss': 0.0018, 'grad_norm': 0.8903162479400635, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05464112013578415, 'eval_precision': 0.7348837209302326, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7488151658767771, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 4.6038, 'eval_samples_per_second': 328.638, 'eval_steps_per_second': 41.27, 'epoch': 9.0}
{'loss': 0.0016, 'grad_norm': 0.0038706830237060785, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.058647189289331436, 'eval_precision': 0.744408945686901, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7473937449879712, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 4.612, 'eval_samples_per_second': 328.055, 'eval_steps_per_second': 41.197, 'epoch': 10.0}
{'loss': 0.0014, 'grad_norm': 0.9373582601547241, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05853423848748207, 'eval_precision': 0.739268680445151, 'eval_recall': 0.748792270531401, 'eval_f1': 0.744, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 4.5677, 'eval_samples_per_second': 331.241, 'eval_steps_per_second': 41.597, 'epoch': 11.0}
{'loss': 0.001, 'grad_norm': 0.03364407271146774, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05943245068192482, 'eval_precision': 0.7408585055643879, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7456, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.563, 'eval_samples_per_second': 331.583, 'eval_steps_per_second': 41.64, 'epoch': 12.0}
{'train_runtime': 1586.9476, 'train_samples_per_second': 64.804, 'train_steps_per_second': 2.027, 'train_loss': 0.015621364403942331, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0156
  train_runtime            = 0:26:26.94
  train_samples            =       8570
  train_samples_per_second =     64.804
  train_steps_per_second   =      2.027
[{'loss': 0.0905, 'grad_norm': 0.40369945764541626, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04131627827882767, 'eval_precision': 0.6011816838995568, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6271186440677967, 'eval_accuracy': 0.9856567788330012, 'eval_runtime': 4.6345, 'eval_samples_per_second': 326.464, 'eval_steps_per_second': 40.997, 'epoch': 1.0, 'step': 268}, {'loss': 0.0336, 'grad_norm': 0.6401016712188721, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03773234412074089, 'eval_precision': 0.6414814814814814, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6682098765432097, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6336, 'eval_samples_per_second': 326.526, 'eval_steps_per_second': 41.005, 'epoch': 2.0, 'step': 536}, {'loss': 0.0219, 'grad_norm': 1.2175571918487549, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.03673283010721207, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7349491790461299, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.5759, 'eval_samples_per_second': 330.645, 'eval_steps_per_second': 41.522, 'epoch': 3.0, 'step': 804}, {'loss': 0.0145, 'grad_norm': 0.700518786907196, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04201001673936844, 'eval_precision': 0.722397476340694, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7298804780876494, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.5937, 'eval_samples_per_second': 329.366, 'eval_steps_per_second': 41.361, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0092, 'grad_norm': 0.26676979660987854, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.04345109686255455, 'eval_precision': 0.7229832572298326, 'eval_recall': 0.7648953301127214, 'eval_f1': 0.7433489827856025, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.5697, 'eval_samples_per_second': 331.094, 'eval_steps_per_second': 41.578, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0057, 'grad_norm': 0.41163507103919983, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.04947509616613388, 'eval_precision': 0.7228346456692913, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7308917197452229, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.5852, 'eval_samples_per_second': 329.978, 'eval_steps_per_second': 41.438, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0037, 'grad_norm': 0.18499651551246643, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.051767949014902115, 'eval_precision': 0.7399678972712681, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7411575562700964, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 4.5897, 'eval_samples_per_second': 329.648, 'eval_steps_per_second': 41.397, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0025, 'grad_norm': 0.07017809897661209, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05381917953491211, 'eval_precision': 0.7268518518518519, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7423167848699764, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6125, 'eval_samples_per_second': 328.024, 'eval_steps_per_second': 41.193, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0018, 'grad_norm': 0.8903162479400635, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05464112013578415, 'eval_precision': 0.7348837209302326, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7488151658767771, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 4.6038, 'eval_samples_per_second': 328.638, 'eval_steps_per_second': 41.27, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0016, 'grad_norm': 0.0038706830237060785, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.058647189289331436, 'eval_precision': 0.744408945686901, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7473937449879712, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 4.612, 'eval_samples_per_second': 328.055, 'eval_steps_per_second': 41.197, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0014, 'grad_norm': 0.9373582601547241, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.05853423848748207, 'eval_precision': 0.739268680445151, 'eval_recall': 0.748792270531401, 'eval_f1': 0.744, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 4.5677, 'eval_samples_per_second': 331.241, 'eval_steps_per_second': 41.597, 'epoch': 11.0, 'step': 2948}, {'loss': 0.001, 'grad_norm': 0.03364407271146774, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.05943245068192482, 'eval_precision': 0.7408585055643879, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7456, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.563, 'eval_samples_per_second': 331.583, 'eval_steps_per_second': 41.64, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1586.9476, 'train_samples_per_second': 64.804, 'train_steps_per_second': 2.027, 'total_flos': 1.1349764561823204e+16, 'train_loss': 0.015621364403942331, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9906
  predict_f1                 =     0.7398
  predict_loss               =     0.0359
  predict_precision          =     0.7066
  predict_recall             =     0.7763
  predict_runtime            = 0:00:03.96
  predict_samples_per_second =    315.963
  predict_steps_per_second   =     39.622
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_303.json completed. F1: 0.7398119122257054
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_101.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 3903.00 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5155.19 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5896.69 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6081.24 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6369.00 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6740.93 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6871.27 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7104.00 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5653.11 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7319.74 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4265.92 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7239.11 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3667.75 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0645, 'grad_norm': 0.7747277021408081, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.03927025571465492, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7027027027027027, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6302, 'eval_samples_per_second': 326.769, 'eval_steps_per_second': 41.035, 'epoch': 1.0}
{'loss': 0.0268, 'grad_norm': 0.7664662003517151, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.03781585767865181, 'eval_precision': 0.682370820668693, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7021110242376857, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.5967, 'eval_samples_per_second': 329.153, 'eval_steps_per_second': 41.334, 'epoch': 2.0}
{'loss': 0.0136, 'grad_norm': 0.0888533964753151, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.0424787737429142, 'eval_precision': 0.6797671033478894, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7140672782874617, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6148, 'eval_samples_per_second': 327.857, 'eval_steps_per_second': 41.172, 'epoch': 3.0}
{'loss': 0.0071, 'grad_norm': 0.3549770414829254, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04846407100558281, 'eval_precision': 0.6992125984251969, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7070063694267517, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6538, 'eval_samples_per_second': 325.113, 'eval_steps_per_second': 40.827, 'epoch': 4.0}
{'loss': 0.0032, 'grad_norm': 0.015424126759171486, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06520699709653854, 'eval_precision': 0.6921875, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7026169706582077, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6361, 'eval_samples_per_second': 326.354, 'eval_steps_per_second': 40.983, 'epoch': 5.0}
{'loss': 0.0023, 'grad_norm': 0.5025643706321716, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.0681082084774971, 'eval_precision': 0.705511811023622, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7133757961783439, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6289, 'eval_samples_per_second': 326.859, 'eval_steps_per_second': 41.046, 'epoch': 6.0}
{'loss': 0.0015, 'grad_norm': 0.009339704178273678, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06926452368497849, 'eval_precision': 0.6852409638554217, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.708171206225681, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.8333, 'eval_samples_per_second': 313.037, 'eval_steps_per_second': 39.311, 'epoch': 7.0}
{'loss': 0.0007, 'grad_norm': 0.002533611375838518, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06682274490594864, 'eval_precision': 0.7242472266244057, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7300319488817891, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.6738, 'eval_samples_per_second': 323.718, 'eval_steps_per_second': 40.652, 'epoch': 8.0}
{'loss': 0.0007, 'grad_norm': 0.007508858572691679, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06956575810909271, 'eval_precision': 0.7216981132075472, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.730310262529833, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.6097, 'eval_samples_per_second': 328.222, 'eval_steps_per_second': 41.218, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.0007875665905885398, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07110041379928589, 'eval_precision': 0.7357594936708861, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7422186751795691, 'eval_accuracy': 0.9896431781764177, 'eval_runtime': 4.5808, 'eval_samples_per_second': 330.294, 'eval_steps_per_second': 41.478, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.003960217349231243, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07275815308094025, 'eval_precision': 0.7311320754716981, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7398568019093079, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6483, 'eval_samples_per_second': 325.496, 'eval_steps_per_second': 40.875, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.001516750198788941, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07349283993244171, 'eval_precision': 0.7269890795631825, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7385103011093502, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.6826, 'eval_samples_per_second': 323.112, 'eval_steps_per_second': 40.576, 'epoch': 12.0}
{'train_runtime': 3536.8248, 'train_samples_per_second': 29.077, 'train_steps_per_second': 0.909, 'train_loss': 0.01009275148210315, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0101
  train_runtime            = 0:58:56.82
  train_samples            =       8570
  train_samples_per_second =     29.077
  train_steps_per_second   =      0.909
[{'loss': 0.0645, 'grad_norm': 0.7747277021408081, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.03927025571465492, 'eval_precision': 0.6938775510204082, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7027027027027027, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6302, 'eval_samples_per_second': 326.769, 'eval_steps_per_second': 41.035, 'epoch': 1.0, 'step': 268}, {'loss': 0.0268, 'grad_norm': 0.7664662003517151, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03781585767865181, 'eval_precision': 0.682370820668693, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7021110242376857, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 4.5967, 'eval_samples_per_second': 329.153, 'eval_steps_per_second': 41.334, 'epoch': 2.0, 'step': 536}, {'loss': 0.0136, 'grad_norm': 0.0888533964753151, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.0424787737429142, 'eval_precision': 0.6797671033478894, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7140672782874617, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6148, 'eval_samples_per_second': 327.857, 'eval_steps_per_second': 41.172, 'epoch': 3.0, 'step': 804}, {'loss': 0.0071, 'grad_norm': 0.3549770414829254, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04846407100558281, 'eval_precision': 0.6992125984251969, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7070063694267517, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6538, 'eval_samples_per_second': 325.113, 'eval_steps_per_second': 40.827, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0032, 'grad_norm': 0.015424126759171486, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06520699709653854, 'eval_precision': 0.6921875, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7026169706582077, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.6361, 'eval_samples_per_second': 326.354, 'eval_steps_per_second': 40.983, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0023, 'grad_norm': 0.5025643706321716, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.0681082084774971, 'eval_precision': 0.705511811023622, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7133757961783439, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6289, 'eval_samples_per_second': 326.859, 'eval_steps_per_second': 41.046, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0015, 'grad_norm': 0.009339704178273678, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.06926452368497849, 'eval_precision': 0.6852409638554217, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.708171206225681, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 4.8333, 'eval_samples_per_second': 313.037, 'eval_steps_per_second': 39.311, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0007, 'grad_norm': 0.002533611375838518, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06682274490594864, 'eval_precision': 0.7242472266244057, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7300319488817891, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.6738, 'eval_samples_per_second': 323.718, 'eval_steps_per_second': 40.652, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0007, 'grad_norm': 0.007508858572691679, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06956575810909271, 'eval_precision': 0.7216981132075472, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.730310262529833, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.6097, 'eval_samples_per_second': 328.222, 'eval_steps_per_second': 41.218, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0003, 'grad_norm': 0.0007875665905885398, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07110041379928589, 'eval_precision': 0.7357594936708861, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7422186751795691, 'eval_accuracy': 0.9896431781764177, 'eval_runtime': 4.5808, 'eval_samples_per_second': 330.294, 'eval_steps_per_second': 41.478, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0002, 'grad_norm': 0.003960217349231243, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.07275815308094025, 'eval_precision': 0.7311320754716981, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7398568019093079, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6483, 'eval_samples_per_second': 325.496, 'eval_steps_per_second': 40.875, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0001, 'grad_norm': 0.001516750198788941, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.07349283993244171, 'eval_precision': 0.7269890795631825, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7385103011093502, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.6826, 'eval_samples_per_second': 323.112, 'eval_steps_per_second': 40.576, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 3536.8248, 'train_samples_per_second': 29.077, 'train_steps_per_second': 0.909, 'total_flos': 1.1352380222341752e+16, 'train_loss': 0.01009275148210315, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9896
  predict_f1                 =     0.7028
  predict_loss               =     0.0344
  predict_precision          =       0.67
  predict_recall             =      0.739
  predict_runtime            = 0:00:04.03
  predict_samples_per_second =     310.17
  predict_steps_per_second   =     38.895
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_101.json completed. F1: 0.70281543274244
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_303.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3217.77 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4241.12 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5018.27 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5720.90 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6181.60 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6583.02 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6721.44 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6971.54 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5674.30 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6960.58 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2969.30 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7215.10 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 1849.77 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1191, 'grad_norm': 0.30356213450431824, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.0502110980451107, 'eval_precision': 0.5466101694915254, 'eval_recall': 0.6231884057971014, 'eval_f1': 0.582392776523702, 'eval_accuracy': 0.9822566146871459, 'eval_runtime': 2.4719, 'eval_samples_per_second': 612.068, 'eval_steps_per_second': 76.862, 'epoch': 1.0}
{'loss': 0.0399, 'grad_norm': 0.5014657378196716, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04391826316714287, 'eval_precision': 0.6336336336336337, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6557886557886559, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.3697, 'eval_samples_per_second': 638.481, 'eval_steps_per_second': 80.179, 'epoch': 2.0}
{'loss': 0.03, 'grad_norm': 0.3278054893016815, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.045526355504989624, 'eval_precision': 0.6474258970358814, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6576862123613312, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3886, 'eval_samples_per_second': 633.438, 'eval_steps_per_second': 79.546, 'epoch': 3.0}
{'loss': 0.0235, 'grad_norm': 0.743461549282074, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04708009213209152, 'eval_precision': 0.6692307692307692, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6845003933910306, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3309, 'eval_samples_per_second': 649.107, 'eval_steps_per_second': 81.514, 'epoch': 4.0}
{'loss': 0.0183, 'grad_norm': 0.168866366147995, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.05191882327198982, 'eval_precision': 0.6329866270430906, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6584234930448223, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 2.3894, 'eval_samples_per_second': 633.215, 'eval_steps_per_second': 79.518, 'epoch': 5.0}
{'loss': 0.0139, 'grad_norm': 0.9857322573661804, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05237557739019394, 'eval_precision': 0.6754658385093167, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6877470355731226, 'eval_accuracy': 0.9872200726931645, 'eval_runtime': 2.3482, 'eval_samples_per_second': 644.334, 'eval_steps_per_second': 80.914, 'epoch': 6.0}
{'loss': 0.0107, 'grad_norm': 0.2561468183994293, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05572215095162392, 'eval_precision': 0.665158371040724, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6869158878504674, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.3278, 'eval_samples_per_second': 649.961, 'eval_steps_per_second': 81.621, 'epoch': 7.0}
{'loss': 0.0084, 'grad_norm': 0.6141912937164307, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05812719836831093, 'eval_precision': 0.6521739130434783, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6754658385093167, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3871, 'eval_samples_per_second': 633.819, 'eval_steps_per_second': 79.594, 'epoch': 8.0}
{'loss': 0.007, 'grad_norm': 1.5843088626861572, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06190638989210129, 'eval_precision': 0.6626686656671664, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6863354037267081, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3495, 'eval_samples_per_second': 643.96, 'eval_steps_per_second': 80.867, 'epoch': 9.0}
{'loss': 0.0057, 'grad_norm': 0.10885612666606903, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06498496234416962, 'eval_precision': 0.6636500754147813, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6853582554517135, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.3019, 'eval_samples_per_second': 657.285, 'eval_steps_per_second': 82.541, 'epoch': 10.0}
{'loss': 0.0049, 'grad_norm': 0.6881728172302246, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06405391544103622, 'eval_precision': 0.6671732522796353, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6864738076622361, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.3094, 'eval_samples_per_second': 655.158, 'eval_steps_per_second': 82.274, 'epoch': 11.0}
{'loss': 0.0043, 'grad_norm': 0.6184513568878174, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06381762772798538, 'eval_precision': 0.6860643185298622, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7032967032967032, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3392, 'eval_samples_per_second': 646.794, 'eval_steps_per_second': 81.223, 'epoch': 12.0}
{'train_runtime': 1731.1472, 'train_samples_per_second': 59.406, 'train_steps_per_second': 1.858, 'train_loss': 0.023807146693047006, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0238
  train_runtime            = 0:28:51.14
  train_samples            =       8570
  train_samples_per_second =     59.406
  train_steps_per_second   =      1.858
[{'loss': 0.1191, 'grad_norm': 0.30356213450431824, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.0502110980451107, 'eval_precision': 0.5466101694915254, 'eval_recall': 0.6231884057971014, 'eval_f1': 0.582392776523702, 'eval_accuracy': 0.9822566146871459, 'eval_runtime': 2.4719, 'eval_samples_per_second': 612.068, 'eval_steps_per_second': 76.862, 'epoch': 1.0, 'step': 268}, {'loss': 0.0399, 'grad_norm': 0.5014657378196716, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04391826316714287, 'eval_precision': 0.6336336336336337, 'eval_recall': 0.679549114331723, 'eval_f1': 0.6557886557886559, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.3697, 'eval_samples_per_second': 638.481, 'eval_steps_per_second': 80.179, 'epoch': 2.0, 'step': 536}, {'loss': 0.03, 'grad_norm': 0.3278054893016815, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.045526355504989624, 'eval_precision': 0.6474258970358814, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.6576862123613312, 'eval_accuracy': 0.9860085199515379, 'eval_runtime': 2.3886, 'eval_samples_per_second': 633.438, 'eval_steps_per_second': 79.546, 'epoch': 3.0, 'step': 804}, {'loss': 0.0235, 'grad_norm': 0.743461549282074, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04708009213209152, 'eval_precision': 0.6692307692307692, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6845003933910306, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3309, 'eval_samples_per_second': 649.107, 'eval_steps_per_second': 81.514, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0183, 'grad_norm': 0.168866366147995, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05191882327198982, 'eval_precision': 0.6329866270430906, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.6584234930448223, 'eval_accuracy': 0.9857349435260093, 'eval_runtime': 2.3894, 'eval_samples_per_second': 633.215, 'eval_steps_per_second': 79.518, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0139, 'grad_norm': 0.9857322573661804, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05237557739019394, 'eval_precision': 0.6754658385093167, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6877470355731226, 'eval_accuracy': 0.9872200726931645, 'eval_runtime': 2.3482, 'eval_samples_per_second': 644.334, 'eval_steps_per_second': 80.914, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0107, 'grad_norm': 0.2561468183994293, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05572215095162392, 'eval_precision': 0.665158371040724, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6869158878504674, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.3278, 'eval_samples_per_second': 649.961, 'eval_steps_per_second': 81.621, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0084, 'grad_norm': 0.6141912937164307, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05812719836831093, 'eval_precision': 0.6521739130434783, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6754658385093167, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3871, 'eval_samples_per_second': 633.819, 'eval_steps_per_second': 79.594, 'epoch': 8.0, 'step': 2144}, {'loss': 0.007, 'grad_norm': 1.5843088626861572, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06190638989210129, 'eval_precision': 0.6626686656671664, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6863354037267081, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3495, 'eval_samples_per_second': 643.96, 'eval_steps_per_second': 80.867, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0057, 'grad_norm': 0.10885612666606903, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06498496234416962, 'eval_precision': 0.6636500754147813, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6853582554517135, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.3019, 'eval_samples_per_second': 657.285, 'eval_steps_per_second': 82.541, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0049, 'grad_norm': 0.6881728172302246, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06405391544103622, 'eval_precision': 0.6671732522796353, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6864738076622361, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.3094, 'eval_samples_per_second': 655.158, 'eval_steps_per_second': 82.274, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0043, 'grad_norm': 0.6184513568878174, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06381762772798538, 'eval_precision': 0.6860643185298622, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7032967032967032, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3392, 'eval_samples_per_second': 646.794, 'eval_steps_per_second': 81.223, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1731.1472, 'train_samples_per_second': 59.406, 'train_steps_per_second': 1.858, 'total_flos': 3923677790716332.0, 'train_loss': 0.023807146693047006, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9877
  predict_f1                 =     0.6477
  predict_loss               =     0.0402
  predict_precision          =     0.6222
  predict_recall             =     0.6754
  predict_runtime            = 0:00:02.18
  predict_samples_per_second =     573.99
  predict_steps_per_second   =     71.978
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_303.json completed. F1: 0.647739221871714
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_303.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3713.24 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5136.80 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6112.20 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6453.53 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6715.38 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7005.36 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7022.30 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7186.83 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6100.28 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 3385.77 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3344.59 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2705.62 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7227.35 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3040.88 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1223, 'grad_norm': 0.4419647753238678, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.0475144162774086, 'eval_precision': 0.5320121951219512, 'eval_recall': 0.5619967793880838, 'eval_f1': 0.5465935787000783, 'eval_accuracy': 0.9847578848634072, 'eval_runtime': 4.8001, 'eval_samples_per_second': 315.205, 'eval_steps_per_second': 39.583, 'epoch': 1.0}
{'loss': 0.0388, 'grad_norm': 0.6479407548904419, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03965913504362106, 'eval_precision': 0.6842105263157895, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6977111286503552, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.8736, 'eval_samples_per_second': 310.446, 'eval_steps_per_second': 38.985, 'epoch': 2.0}
{'loss': 0.028, 'grad_norm': 0.4510750472545624, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03513341397047043, 'eval_precision': 0.676829268292683, 'eval_recall': 0.714975845410628, 'eval_f1': 0.6953797963978073, 'eval_accuracy': 0.988822448899832, 'eval_runtime': 4.7457, 'eval_samples_per_second': 318.816, 'eval_steps_per_second': 40.036, 'epoch': 3.0}
{'loss': 0.0204, 'grad_norm': 0.5340815782546997, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.036520816385746, 'eval_precision': 0.6884735202492211, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6999208234362629, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.6246, 'eval_samples_per_second': 327.164, 'eval_steps_per_second': 41.085, 'epoch': 4.0}
{'loss': 0.0146, 'grad_norm': 0.20459766685962677, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.038542959839105606, 'eval_precision': 0.7017001545595054, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7160883280757098, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.7022, 'eval_samples_per_second': 321.765, 'eval_steps_per_second': 40.407, 'epoch': 5.0}
{'loss': 0.0101, 'grad_norm': 0.2806215286254883, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.040767885744571686, 'eval_precision': 0.71875, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7295796986518636, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.5972, 'eval_samples_per_second': 329.111, 'eval_steps_per_second': 41.329, 'epoch': 6.0}
{'loss': 0.0077, 'grad_norm': 0.41560912132263184, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.045503344386816025, 'eval_precision': 0.7001569858712716, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7090620031796503, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6738, 'eval_samples_per_second': 323.719, 'eval_steps_per_second': 40.652, 'epoch': 7.0}
{'loss': 0.0055, 'grad_norm': 0.1219831258058548, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.04745829477906227, 'eval_precision': 0.7242472266244057, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7300319488817891, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.5684, 'eval_samples_per_second': 331.19, 'eval_steps_per_second': 41.59, 'epoch': 8.0}
{'loss': 0.0041, 'grad_norm': 0.2300443798303604, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.048776231706142426, 'eval_precision': 0.7365930599369085, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7442231075697211, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 4.5662, 'eval_samples_per_second': 331.345, 'eval_steps_per_second': 41.61, 'epoch': 9.0}
{'loss': 0.0035, 'grad_norm': 0.13164958357810974, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.050364844501018524, 'eval_precision': 0.7357594936708861, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7422186751795691, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.5746, 'eval_samples_per_second': 330.738, 'eval_steps_per_second': 41.534, 'epoch': 10.0}
{'loss': 0.0029, 'grad_norm': 0.47008925676345825, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05234330892562866, 'eval_precision': 0.731437598736177, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7384370015948962, 'eval_accuracy': 0.9896431781764177, 'eval_runtime': 4.734, 'eval_samples_per_second': 319.605, 'eval_steps_per_second': 40.136, 'epoch': 11.0}
{'loss': 0.0028, 'grad_norm': 0.23809269070625305, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.052055515348911285, 'eval_precision': 0.740506329113924, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7470071827613727, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.5644, 'eval_samples_per_second': 331.479, 'eval_steps_per_second': 41.627, 'epoch': 12.0}
{'train_runtime': 1280.8883, 'train_samples_per_second': 80.288, 'train_steps_per_second': 1.255, 'train_loss': 0.021733722227871122, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0217
  train_runtime            = 0:21:20.88
  train_samples            =       8570
  train_samples_per_second =     80.288
  train_steps_per_second   =      1.255
[{'loss': 0.1223, 'grad_norm': 0.4419647753238678, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.0475144162774086, 'eval_precision': 0.5320121951219512, 'eval_recall': 0.5619967793880838, 'eval_f1': 0.5465935787000783, 'eval_accuracy': 0.9847578848634072, 'eval_runtime': 4.8001, 'eval_samples_per_second': 315.205, 'eval_steps_per_second': 39.583, 'epoch': 1.0, 'step': 134}, {'loss': 0.0388, 'grad_norm': 0.6479407548904419, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.03965913504362106, 'eval_precision': 0.6842105263157895, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6977111286503552, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.8736, 'eval_samples_per_second': 310.446, 'eval_steps_per_second': 38.985, 'epoch': 2.0, 'step': 268}, {'loss': 0.028, 'grad_norm': 0.4510750472545624, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.03513341397047043, 'eval_precision': 0.676829268292683, 'eval_recall': 0.714975845410628, 'eval_f1': 0.6953797963978073, 'eval_accuracy': 0.988822448899832, 'eval_runtime': 4.7457, 'eval_samples_per_second': 318.816, 'eval_steps_per_second': 40.036, 'epoch': 3.0, 'step': 402}, {'loss': 0.0204, 'grad_norm': 0.5340815782546997, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.036520816385746, 'eval_precision': 0.6884735202492211, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6999208234362629, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.6246, 'eval_samples_per_second': 327.164, 'eval_steps_per_second': 41.085, 'epoch': 4.0, 'step': 536}, {'loss': 0.0146, 'grad_norm': 0.20459766685962677, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.038542959839105606, 'eval_precision': 0.7017001545595054, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7160883280757098, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.7022, 'eval_samples_per_second': 321.765, 'eval_steps_per_second': 40.407, 'epoch': 5.0, 'step': 670}, {'loss': 0.0101, 'grad_norm': 0.2806215286254883, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.040767885744571686, 'eval_precision': 0.71875, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7295796986518636, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.5972, 'eval_samples_per_second': 329.111, 'eval_steps_per_second': 41.329, 'epoch': 6.0, 'step': 804}, {'loss': 0.0077, 'grad_norm': 0.41560912132263184, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.045503344386816025, 'eval_precision': 0.7001569858712716, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7090620031796503, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.6738, 'eval_samples_per_second': 323.719, 'eval_steps_per_second': 40.652, 'epoch': 7.0, 'step': 938}, {'loss': 0.0055, 'grad_norm': 0.1219831258058548, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.04745829477906227, 'eval_precision': 0.7242472266244057, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7300319488817891, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.5684, 'eval_samples_per_second': 331.19, 'eval_steps_per_second': 41.59, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0041, 'grad_norm': 0.2300443798303604, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.048776231706142426, 'eval_precision': 0.7365930599369085, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7442231075697211, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 4.5662, 'eval_samples_per_second': 331.345, 'eval_steps_per_second': 41.61, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0035, 'grad_norm': 0.13164958357810974, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.050364844501018524, 'eval_precision': 0.7357594936708861, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7422186751795691, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.5746, 'eval_samples_per_second': 330.738, 'eval_steps_per_second': 41.534, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0029, 'grad_norm': 0.47008925676345825, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05234330892562866, 'eval_precision': 0.731437598736177, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7384370015948962, 'eval_accuracy': 0.9896431781764177, 'eval_runtime': 4.734, 'eval_samples_per_second': 319.605, 'eval_steps_per_second': 40.136, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0028, 'grad_norm': 0.23809269070625305, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.052055515348911285, 'eval_precision': 0.740506329113924, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7470071827613727, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.5644, 'eval_samples_per_second': 331.479, 'eval_steps_per_second': 41.627, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1280.8883, 'train_samples_per_second': 80.288, 'train_steps_per_second': 1.255, 'total_flos': 1.290621679981722e+16, 'train_loss': 0.021733722227871122, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =       0.99
  predict_f1                 =     0.7149
  predict_loss               =      0.035
  predict_precision          =     0.6847
  predict_recall             =     0.7478
  predict_runtime            = 0:00:04.04
  predict_samples_per_second =     309.74
  predict_steps_per_second   =     38.841
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_303.json completed. F1: 0.7148846960167715
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_101.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3048.10 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2532.71 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 3577.96 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 4476.75 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5176.47 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5786.89 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6126.14 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6519.56 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 2945.02 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 3888.30 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3967.59 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3309.64 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 5602.00 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3757.16 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0893, 'grad_norm': 0.2918817698955536, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.042876794934272766, 'eval_precision': 0.6306729264475743, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6396825396825397, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.623, 'eval_samples_per_second': 576.822, 'eval_steps_per_second': 72.436, 'epoch': 1.0}
{'loss': 0.0328, 'grad_norm': 0.3880288302898407, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.043129391968250275, 'eval_precision': 0.6479289940828402, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6754047802621433, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3198, 'eval_samples_per_second': 652.203, 'eval_steps_per_second': 81.903, 'epoch': 2.0}
{'loss': 0.0207, 'grad_norm': 0.4584507346153259, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04244735836982727, 'eval_precision': 0.6777609682299546, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6989079563182526, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.2815, 'eval_samples_per_second': 663.169, 'eval_steps_per_second': 83.28, 'epoch': 3.0}
{'loss': 0.012, 'grad_norm': 0.1667652279138565, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04839551821351051, 'eval_precision': 0.6631892697466468, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6888544891640866, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3968, 'eval_samples_per_second': 631.252, 'eval_steps_per_second': 79.272, 'epoch': 4.0}
{'loss': 0.007, 'grad_norm': 0.1099034920334816, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05845153331756592, 'eval_precision': 0.6559633027522935, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6729411764705883, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.854, 'eval_samples_per_second': 530.125, 'eval_steps_per_second': 66.572, 'epoch': 5.0}
{'loss': 0.0039, 'grad_norm': 0.32399001717567444, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.059677448123693466, 'eval_precision': 0.7003205128205128, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.702008032128514, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 2.3119, 'eval_samples_per_second': 654.43, 'eval_steps_per_second': 82.182, 'epoch': 6.0}
{'loss': 0.0026, 'grad_norm': 0.20870019495487213, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06249275803565979, 'eval_precision': 0.6713178294573643, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.684044233807267, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2759, 'eval_samples_per_second': 664.803, 'eval_steps_per_second': 83.485, 'epoch': 7.0}
{'loss': 0.0016, 'grad_norm': 0.31628280878067017, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07125689834356308, 'eval_precision': 0.6591251885369532, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6806853582554517, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.4426, 'eval_samples_per_second': 619.424, 'eval_steps_per_second': 77.786, 'epoch': 8.0}
{'loss': 0.0011, 'grad_norm': 0.19143635034561157, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07103820145130157, 'eval_precision': 0.6783004552352049, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6984375, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.3517, 'eval_samples_per_second': 643.37, 'eval_steps_per_second': 80.793, 'epoch': 9.0}
{'loss': 0.0007, 'grad_norm': 0.05290522798895836, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07167210429906845, 'eval_precision': 0.6789554531490015, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6949685534591195, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.2935, 'eval_samples_per_second': 659.696, 'eval_steps_per_second': 82.844, 'epoch': 10.0}
{'loss': 0.0005, 'grad_norm': 0.15344181656837463, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07116083800792694, 'eval_precision': 0.6783004552352049, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6984375, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.4393, 'eval_samples_per_second': 620.267, 'eval_steps_per_second': 77.892, 'epoch': 11.0}
{'loss': 0.0004, 'grad_norm': 0.009963789954781532, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07159263640642166, 'eval_precision': 0.6876923076923077, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7033831628638867, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.3656, 'eval_samples_per_second': 639.585, 'eval_steps_per_second': 80.318, 'epoch': 12.0}
{'train_runtime': 1305.3141, 'train_samples_per_second': 78.786, 'train_steps_per_second': 1.232, 'train_loss': 0.014374599029861428, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0144
  train_runtime            = 0:21:45.31
  train_samples            =       8570
  train_samples_per_second =     78.786
  train_steps_per_second   =      1.232
[{'loss': 0.0893, 'grad_norm': 0.2918817698955536, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.042876794934272766, 'eval_precision': 0.6306729264475743, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6396825396825397, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.623, 'eval_samples_per_second': 576.822, 'eval_steps_per_second': 72.436, 'epoch': 1.0, 'step': 134}, {'loss': 0.0328, 'grad_norm': 0.3880288302898407, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.043129391968250275, 'eval_precision': 0.6479289940828402, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6754047802621433, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.3198, 'eval_samples_per_second': 652.203, 'eval_steps_per_second': 81.903, 'epoch': 2.0, 'step': 268}, {'loss': 0.0207, 'grad_norm': 0.4584507346153259, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04244735836982727, 'eval_precision': 0.6777609682299546, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.6989079563182526, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.2815, 'eval_samples_per_second': 663.169, 'eval_steps_per_second': 83.28, 'epoch': 3.0, 'step': 402}, {'loss': 0.012, 'grad_norm': 0.1667652279138565, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04839551821351051, 'eval_precision': 0.6631892697466468, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6888544891640866, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3968, 'eval_samples_per_second': 631.252, 'eval_steps_per_second': 79.272, 'epoch': 4.0, 'step': 536}, {'loss': 0.007, 'grad_norm': 0.1099034920334816, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05845153331756592, 'eval_precision': 0.6559633027522935, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6729411764705883, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.854, 'eval_samples_per_second': 530.125, 'eval_steps_per_second': 66.572, 'epoch': 5.0, 'step': 670}, {'loss': 0.0039, 'grad_norm': 0.32399001717567444, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.059677448123693466, 'eval_precision': 0.7003205128205128, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.702008032128514, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 2.3119, 'eval_samples_per_second': 654.43, 'eval_steps_per_second': 82.182, 'epoch': 6.0, 'step': 804}, {'loss': 0.0026, 'grad_norm': 0.20870019495487213, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06249275803565979, 'eval_precision': 0.6713178294573643, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.684044233807267, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2759, 'eval_samples_per_second': 664.803, 'eval_steps_per_second': 83.485, 'epoch': 7.0, 'step': 938}, {'loss': 0.0016, 'grad_norm': 0.31628280878067017, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07125689834356308, 'eval_precision': 0.6591251885369532, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6806853582554517, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.4426, 'eval_samples_per_second': 619.424, 'eval_steps_per_second': 77.786, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0011, 'grad_norm': 0.19143635034561157, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.07103820145130157, 'eval_precision': 0.6783004552352049, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6984375, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.3517, 'eval_samples_per_second': 643.37, 'eval_steps_per_second': 80.793, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0007, 'grad_norm': 0.05290522798895836, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07167210429906845, 'eval_precision': 0.6789554531490015, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6949685534591195, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.2935, 'eval_samples_per_second': 659.696, 'eval_steps_per_second': 82.844, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0005, 'grad_norm': 0.15344181656837463, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07116083800792694, 'eval_precision': 0.6783004552352049, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6984375, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.4393, 'eval_samples_per_second': 620.267, 'eval_steps_per_second': 77.892, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0004, 'grad_norm': 0.009963789954781532, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07159263640642166, 'eval_precision': 0.6876923076923077, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7033831628638867, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.3656, 'eval_samples_per_second': 639.585, 'eval_steps_per_second': 80.318, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1305.3141, 'train_samples_per_second': 78.786, 'train_steps_per_second': 1.232, 'total_flos': 4438602656880876.0, 'train_loss': 0.014374599029861428, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9891
  predict_f1                 =     0.7178
  predict_loss               =     0.0384
  predict_precision          =     0.6977
  predict_recall             =      0.739
  predict_runtime            = 0:00:01.99
  predict_samples_per_second =    628.535
  predict_steps_per_second   =     78.818
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_101.json completed. F1: 0.7177848775292865
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_202.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3108.40 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4381.02 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5359.86 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5600.33 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6093.62 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6509.74 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6632.34 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6883.27 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5477.38 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6813.58 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 6021.97 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 3035.47 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2880.74 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0633, 'grad_norm': 0.31192636489868164, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.034756146371364594, 'eval_precision': 0.6882911392405063, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6943335993615324, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 5.8325, 'eval_samples_per_second': 259.408, 'eval_steps_per_second': 32.576, 'epoch': 1.0}
{'loss': 0.0196, 'grad_norm': 0.8446531295776367, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.03473042696714401, 'eval_precision': 0.6862442040185471, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7003154574132492, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 5.66, 'eval_samples_per_second': 267.313, 'eval_steps_per_second': 33.569, 'epoch': 2.0}
{'loss': 0.0066, 'grad_norm': 0.09589999914169312, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04395517706871033, 'eval_precision': 0.7170418006430869, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7176186645213193, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.6423, 'eval_samples_per_second': 268.153, 'eval_steps_per_second': 33.674, 'epoch': 3.0}
{'loss': 0.0023, 'grad_norm': 0.05262536182999611, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04931402578949928, 'eval_precision': 0.728, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7303370786516855, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.6864, 'eval_samples_per_second': 266.076, 'eval_steps_per_second': 33.413, 'epoch': 4.0}
{'loss': 0.0009, 'grad_norm': 0.01983364298939705, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.053365860134363174, 'eval_precision': 0.7076923076923077, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7238394964594808, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.6245, 'eval_samples_per_second': 269.0, 'eval_steps_per_second': 33.781, 'epoch': 5.0}
{'loss': 0.0005, 'grad_norm': 0.028281452134251595, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.05840061977505684, 'eval_precision': 0.7031963470319634, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7230046948356808, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.501, 'eval_samples_per_second': 275.043, 'eval_steps_per_second': 34.539, 'epoch': 6.0}
{'loss': 0.0002, 'grad_norm': 0.011405461467802525, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06874554604291916, 'eval_precision': 0.7479406919275123, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.739413680781759, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 5.5166, 'eval_samples_per_second': 274.262, 'eval_steps_per_second': 34.441, 'epoch': 7.0}
{'loss': 0.0001, 'grad_norm': 0.0006119607714936137, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06359712779521942, 'eval_precision': 0.7230769230769231, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7395751376868608, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 5.4971, 'eval_samples_per_second': 275.236, 'eval_steps_per_second': 34.564, 'epoch': 8.0}
{'loss': 0.0, 'grad_norm': 0.005296874791383743, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06474607437849045, 'eval_precision': 0.7257318952234206, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7417322834645669, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.5575, 'eval_samples_per_second': 272.244, 'eval_steps_per_second': 34.188, 'epoch': 9.0}
{'loss': 0.0, 'grad_norm': 0.0013091694563627243, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06576928496360779, 'eval_precision': 0.7279752704791345, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7429022082018928, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.4721, 'eval_samples_per_second': 276.493, 'eval_steps_per_second': 34.722, 'epoch': 10.0}
{'loss': 0.0, 'grad_norm': 0.0025252678897231817, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.0664445012807846, 'eval_precision': 0.724191063174114, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7401574803149605, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.5435, 'eval_samples_per_second': 272.933, 'eval_steps_per_second': 34.274, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 0.0009214523597620428, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06668194383382797, 'eval_precision': 0.7253086419753086, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7407407407407407, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.4604, 'eval_samples_per_second': 277.088, 'eval_steps_per_second': 34.796, 'epoch': 12.0}
{'train_runtime': 2091.6834, 'train_samples_per_second': 49.166, 'train_steps_per_second': 1.538, 'train_loss': 0.007804701038154389, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0078
  train_runtime            = 0:34:51.68
  train_samples            =       8570
  train_samples_per_second =     49.166
  train_steps_per_second   =      1.538
[{'loss': 0.0633, 'grad_norm': 0.31192636489868164, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.034756146371364594, 'eval_precision': 0.6882911392405063, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6943335993615324, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 5.8325, 'eval_samples_per_second': 259.408, 'eval_steps_per_second': 32.576, 'epoch': 1.0, 'step': 268}, {'loss': 0.0196, 'grad_norm': 0.8446531295776367, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03473042696714401, 'eval_precision': 0.6862442040185471, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7003154574132492, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 5.66, 'eval_samples_per_second': 267.313, 'eval_steps_per_second': 33.569, 'epoch': 2.0, 'step': 536}, {'loss': 0.0066, 'grad_norm': 0.09589999914169312, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04395517706871033, 'eval_precision': 0.7170418006430869, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7176186645213193, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.6423, 'eval_samples_per_second': 268.153, 'eval_steps_per_second': 33.674, 'epoch': 3.0, 'step': 804}, {'loss': 0.0023, 'grad_norm': 0.05262536182999611, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04931402578949928, 'eval_precision': 0.728, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7303370786516855, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.6864, 'eval_samples_per_second': 266.076, 'eval_steps_per_second': 33.413, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0009, 'grad_norm': 0.01983364298939705, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.053365860134363174, 'eval_precision': 0.7076923076923077, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7238394964594808, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.6245, 'eval_samples_per_second': 269.0, 'eval_steps_per_second': 33.781, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0005, 'grad_norm': 0.028281452134251595, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05840061977505684, 'eval_precision': 0.7031963470319634, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7230046948356808, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.501, 'eval_samples_per_second': 275.043, 'eval_steps_per_second': 34.539, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0002, 'grad_norm': 0.011405461467802525, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.06874554604291916, 'eval_precision': 0.7479406919275123, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.739413680781759, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 5.5166, 'eval_samples_per_second': 274.262, 'eval_steps_per_second': 34.441, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0001, 'grad_norm': 0.0006119607714936137, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06359712779521942, 'eval_precision': 0.7230769230769231, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7395751376868608, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 5.4971, 'eval_samples_per_second': 275.236, 'eval_steps_per_second': 34.564, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0, 'grad_norm': 0.005296874791383743, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06474607437849045, 'eval_precision': 0.7257318952234206, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7417322834645669, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.5575, 'eval_samples_per_second': 272.244, 'eval_steps_per_second': 34.188, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0, 'grad_norm': 0.0013091694563627243, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06576928496360779, 'eval_precision': 0.7279752704791345, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7429022082018928, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.4721, 'eval_samples_per_second': 276.493, 'eval_steps_per_second': 34.722, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0, 'grad_norm': 0.0025252678897231817, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.0664445012807846, 'eval_precision': 0.724191063174114, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7401574803149605, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.5435, 'eval_samples_per_second': 272.933, 'eval_steps_per_second': 34.274, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0, 'grad_norm': 0.0009214523597620428, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06668194383382797, 'eval_precision': 0.7253086419753086, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7407407407407407, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.4604, 'eval_samples_per_second': 277.088, 'eval_steps_per_second': 34.796, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 2091.6834, 'train_samples_per_second': 49.166, 'train_steps_per_second': 1.538, 'total_flos': 1.177774843518018e+16, 'train_loss': 0.007804701038154389, 'epoch': 12.0, 'step': 3216}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9917
  predict_f1                 =     0.7481
  predict_loss               =     0.0294
  predict_precision          =     0.7258
  predict_recall             =     0.7719
  predict_runtime            = 0:00:04.61
  predict_samples_per_second =    271.367
  predict_steps_per_second   =     34.029
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_202.json completed. F1: 0.7481402763018066
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_303.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 5295.02 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 6247.38 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6282.86 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6718.77 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6901.23 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7166.95 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7195.91 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7339.41 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6267.53 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7273.97 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4273.30 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6895.84 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:01<00:00, 1089.36 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0693, 'grad_norm': 0.2614443898200989, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04343898966908455, 'eval_precision': 0.6402985074626866, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6646010844306739, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3559, 'eval_samples_per_second': 642.226, 'eval_steps_per_second': 80.65, 'epoch': 1.0}
{'loss': 0.0317, 'grad_norm': 0.32190969586372375, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04380437359213829, 'eval_precision': 0.6104815864022662, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6495855312735492, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.2858, 'eval_samples_per_second': 661.925, 'eval_steps_per_second': 83.123, 'epoch': 2.0}
{'loss': 0.0181, 'grad_norm': 0.7155570983886719, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05150783434510231, 'eval_precision': 0.6043478260869565, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6361556064073226, 'eval_accuracy': 0.9848751319029194, 'eval_runtime': 2.316, 'eval_samples_per_second': 653.292, 'eval_steps_per_second': 82.039, 'epoch': 3.0}
{'loss': 0.0101, 'grad_norm': 0.14767374098300934, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05598387122154236, 'eval_precision': 0.6765625, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6867565424266455, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.2762, 'eval_samples_per_second': 664.701, 'eval_steps_per_second': 83.472, 'epoch': 4.0}
{'loss': 0.0056, 'grad_norm': 1.1219513416290283, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06077811121940613, 'eval_precision': 0.6731927710843374, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6957198443579767, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.2605, 'eval_samples_per_second': 669.323, 'eval_steps_per_second': 84.052, 'epoch': 5.0}
{'loss': 0.0027, 'grad_norm': 0.9076226353645325, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07137414067983627, 'eval_precision': 0.6645962732919255, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6766798418972332, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3044, 'eval_samples_per_second': 656.56, 'eval_steps_per_second': 82.45, 'epoch': 6.0}
{'loss': 0.0015, 'grad_norm': 0.08851493149995804, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07400908321142197, 'eval_precision': 0.648936170212766, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6677091477716965, 'eval_accuracy': 0.985617696486497, 'eval_runtime': 2.2427, 'eval_samples_per_second': 674.632, 'eval_steps_per_second': 84.719, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.008009428158402443, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07445354759693146, 'eval_precision': 0.6452095808383234, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6687354538401862, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.5298, 'eval_samples_per_second': 598.065, 'eval_steps_per_second': 75.104, 'epoch': 8.0}
{'loss': 0.0006, 'grad_norm': 0.02263910137116909, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07805278897285461, 'eval_precision': 0.6565809379727685, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6770670826833073, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2771, 'eval_samples_per_second': 664.43, 'eval_steps_per_second': 83.438, 'epoch': 9.0}
{'loss': 0.0004, 'grad_norm': 0.002386966021731496, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.0815669447183609, 'eval_precision': 0.6727549467275494, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6917057902973396, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.2456, 'eval_samples_per_second': 673.748, 'eval_steps_per_second': 84.608, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.6427801847457886, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08152604848146439, 'eval_precision': 0.6585735963581184, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.678125, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.2503, 'eval_samples_per_second': 672.353, 'eval_steps_per_second': 84.433, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.004373444244265556, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0816865935921669, 'eval_precision': 0.6732824427480916, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6912225705329155, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 2.2589, 'eval_samples_per_second': 669.796, 'eval_steps_per_second': 84.112, 'epoch': 12.0}
{'train_runtime': 738.3749, 'train_samples_per_second': 139.279, 'train_steps_per_second': 4.356, 'train_loss': 0.011781317861843391, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0118
  train_runtime            = 0:12:18.37
  train_samples            =       8570
  train_samples_per_second =    139.279
  train_steps_per_second   =      4.356
[{'loss': 0.0693, 'grad_norm': 0.2614443898200989, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04343898966908455, 'eval_precision': 0.6402985074626866, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6646010844306739, 'eval_accuracy': 0.9859303552585297, 'eval_runtime': 2.3559, 'eval_samples_per_second': 642.226, 'eval_steps_per_second': 80.65, 'epoch': 1.0, 'step': 268}, {'loss': 0.0317, 'grad_norm': 0.32190969586372375, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04380437359213829, 'eval_precision': 0.6104815864022662, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6495855312735492, 'eval_accuracy': 0.986047602298042, 'eval_runtime': 2.2858, 'eval_samples_per_second': 661.925, 'eval_steps_per_second': 83.123, 'epoch': 2.0, 'step': 536}, {'loss': 0.0181, 'grad_norm': 0.7155570983886719, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05150783434510231, 'eval_precision': 0.6043478260869565, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6361556064073226, 'eval_accuracy': 0.9848751319029194, 'eval_runtime': 2.316, 'eval_samples_per_second': 653.292, 'eval_steps_per_second': 82.039, 'epoch': 3.0, 'step': 804}, {'loss': 0.0101, 'grad_norm': 0.14767374098300934, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05598387122154236, 'eval_precision': 0.6765625, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6867565424266455, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.2762, 'eval_samples_per_second': 664.701, 'eval_steps_per_second': 83.472, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0056, 'grad_norm': 1.1219513416290283, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06077811121940613, 'eval_precision': 0.6731927710843374, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6957198443579767, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.2605, 'eval_samples_per_second': 669.323, 'eval_steps_per_second': 84.052, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0027, 'grad_norm': 0.9076226353645325, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.07137414067983627, 'eval_precision': 0.6645962732919255, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6766798418972332, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3044, 'eval_samples_per_second': 656.56, 'eval_steps_per_second': 82.45, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0015, 'grad_norm': 0.08851493149995804, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07400908321142197, 'eval_precision': 0.648936170212766, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6677091477716965, 'eval_accuracy': 0.985617696486497, 'eval_runtime': 2.2427, 'eval_samples_per_second': 674.632, 'eval_steps_per_second': 84.719, 'epoch': 7.0, 'step': 1876}, {'loss': 0.001, 'grad_norm': 0.008009428158402443, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07445354759693146, 'eval_precision': 0.6452095808383234, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6687354538401862, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.5298, 'eval_samples_per_second': 598.065, 'eval_steps_per_second': 75.104, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0006, 'grad_norm': 0.02263910137116909, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07805278897285461, 'eval_precision': 0.6565809379727685, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6770670826833073, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.2771, 'eval_samples_per_second': 664.43, 'eval_steps_per_second': 83.438, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0004, 'grad_norm': 0.002386966021731496, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.0815669447183609, 'eval_precision': 0.6727549467275494, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6917057902973396, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.2456, 'eval_samples_per_second': 673.748, 'eval_steps_per_second': 84.608, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0002, 'grad_norm': 0.6427801847457886, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08152604848146439, 'eval_precision': 0.6585735963581184, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.678125, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.2503, 'eval_samples_per_second': 672.353, 'eval_steps_per_second': 84.433, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0002, 'grad_norm': 0.004373444244265556, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.0816865935921669, 'eval_precision': 0.6732824427480916, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.6912225705329155, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 2.2589, 'eval_samples_per_second': 669.796, 'eval_steps_per_second': 84.112, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 738.3749, 'train_samples_per_second': 139.279, 'train_steps_per_second': 4.356, 'total_flos': 3923677790716332.0, 'train_loss': 0.011781317861843391, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9871
  predict_f1                 =     0.6437
  predict_loss               =     0.0401
  predict_precision          =     0.6148
  predict_recall             =     0.6754
  predict_runtime            = 0:00:01.93
  predict_samples_per_second =     645.74
  predict_steps_per_second   =     80.975
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_303.json completed. F1: 0.6436781609195403
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_303.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4571.04 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5014.48 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4389.03 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:01, 4512.00 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5257.88 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5908.57 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6292.39 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6689.52 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5281.14 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7179.36 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4631.77 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7302.65 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5281.84 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1514, 'grad_norm': 0.3649711608886719, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04673390090465546, 'eval_precision': 0.5294985250737463, 'eval_recall': 0.5780998389694042, 'eval_f1': 0.5527328714395688, 'eval_accuracy': 0.9839371555868215, 'eval_runtime': 5.6042, 'eval_samples_per_second': 269.974, 'eval_steps_per_second': 33.903, 'epoch': 1.0}
{'loss': 0.0342, 'grad_norm': 0.6950766444206238, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03955797478556633, 'eval_precision': 0.6425339366515838, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.663551401869159, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 5.5409, 'eval_samples_per_second': 273.061, 'eval_steps_per_second': 34.291, 'epoch': 2.0}
{'loss': 0.0236, 'grad_norm': 0.3093683123588562, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04035152867436409, 'eval_precision': 0.661993769470405, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6730007917656374, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 5.5285, 'eval_samples_per_second': 273.674, 'eval_steps_per_second': 34.368, 'epoch': 3.0}
{'loss': 0.0155, 'grad_norm': 1.0175694227218628, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.03953313082456589, 'eval_precision': 0.7216174183514774, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7341772151898734, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5232, 'eval_samples_per_second': 273.935, 'eval_steps_per_second': 34.4, 'epoch': 4.0}
{'loss': 0.0099, 'grad_norm': 0.26707637310028076, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.043965332210063934, 'eval_precision': 0.7203791469194313, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7272727272727273, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.5603, 'eval_samples_per_second': 272.108, 'eval_steps_per_second': 34.171, 'epoch': 5.0}
{'loss': 0.0063, 'grad_norm': 0.4297124445438385, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04875367507338524, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 5.5747, 'eval_samples_per_second': 271.405, 'eval_steps_per_second': 34.083, 'epoch': 6.0}
{'loss': 0.0042, 'grad_norm': 0.29315999150276184, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05092230066657066, 'eval_precision': 0.7255520504731862, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7330677290836652, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5899, 'eval_samples_per_second': 270.666, 'eval_steps_per_second': 33.99, 'epoch': 7.0}
{'loss': 0.0031, 'grad_norm': 0.15432393550872803, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.052756138145923615, 'eval_precision': 0.7205651491365777, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7297297297297296, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.4909, 'eval_samples_per_second': 275.546, 'eval_steps_per_second': 34.603, 'epoch': 8.0}
{'loss': 0.0022, 'grad_norm': 0.16069947183132172, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.055508289486169815, 'eval_precision': 0.7298578199052133, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7368421052631579, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5174, 'eval_samples_per_second': 274.221, 'eval_steps_per_second': 34.436, 'epoch': 9.0}
{'loss': 0.0017, 'grad_norm': 0.07606880366802216, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.057650670409202576, 'eval_precision': 0.7355769230769231, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7373493975903614, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 5.4958, 'eval_samples_per_second': 275.303, 'eval_steps_per_second': 34.572, 'epoch': 10.0}
{'loss': 0.0013, 'grad_norm': 0.12075480073690414, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.058773189783096313, 'eval_precision': 0.7447833065810594, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7459807073954984, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.5691, 'eval_samples_per_second': 271.678, 'eval_steps_per_second': 34.117, 'epoch': 11.0}
{'loss': 0.0012, 'grad_norm': 0.08402767032384872, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.058403775095939636, 'eval_precision': 0.7408585055643879, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7456, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.5269, 'eval_samples_per_second': 273.753, 'eval_steps_per_second': 34.377, 'epoch': 12.0}
{'train_runtime': 2419.6981, 'train_samples_per_second': 42.501, 'train_steps_per_second': 0.665, 'train_loss': 0.021232993439284722, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0212
  train_runtime            = 0:40:19.69
  train_samples            =       8570
  train_samples_per_second =     42.501
  train_steps_per_second   =      0.665
[{'loss': 0.1514, 'grad_norm': 0.3649711608886719, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04673390090465546, 'eval_precision': 0.5294985250737463, 'eval_recall': 0.5780998389694042, 'eval_f1': 0.5527328714395688, 'eval_accuracy': 0.9839371555868215, 'eval_runtime': 5.6042, 'eval_samples_per_second': 269.974, 'eval_steps_per_second': 33.903, 'epoch': 1.0, 'step': 134}, {'loss': 0.0342, 'grad_norm': 0.6950766444206238, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.03955797478556633, 'eval_precision': 0.6425339366515838, 'eval_recall': 0.6859903381642513, 'eval_f1': 0.663551401869159, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 5.5409, 'eval_samples_per_second': 273.061, 'eval_steps_per_second': 34.291, 'epoch': 2.0, 'step': 268}, {'loss': 0.0236, 'grad_norm': 0.3093683123588562, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04035152867436409, 'eval_precision': 0.661993769470405, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6730007917656374, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 5.5285, 'eval_samples_per_second': 273.674, 'eval_steps_per_second': 34.368, 'epoch': 3.0, 'step': 402}, {'loss': 0.0155, 'grad_norm': 1.0175694227218628, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.03953313082456589, 'eval_precision': 0.7216174183514774, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7341772151898734, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5232, 'eval_samples_per_second': 273.935, 'eval_steps_per_second': 34.4, 'epoch': 4.0, 'step': 536}, {'loss': 0.0099, 'grad_norm': 0.26707637310028076, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.043965332210063934, 'eval_precision': 0.7203791469194313, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7272727272727273, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.5603, 'eval_samples_per_second': 272.108, 'eval_steps_per_second': 34.171, 'epoch': 5.0, 'step': 670}, {'loss': 0.0063, 'grad_norm': 0.4297124445438385, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04875367507338524, 'eval_precision': 0.7050473186119873, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7123505976095617, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 5.5747, 'eval_samples_per_second': 271.405, 'eval_steps_per_second': 34.083, 'epoch': 6.0, 'step': 804}, {'loss': 0.0042, 'grad_norm': 0.29315999150276184, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05092230066657066, 'eval_precision': 0.7255520504731862, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7330677290836652, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5899, 'eval_samples_per_second': 270.666, 'eval_steps_per_second': 33.99, 'epoch': 7.0, 'step': 938}, {'loss': 0.0031, 'grad_norm': 0.15432393550872803, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.052756138145923615, 'eval_precision': 0.7205651491365777, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7297297297297296, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.4909, 'eval_samples_per_second': 275.546, 'eval_steps_per_second': 34.603, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0022, 'grad_norm': 0.16069947183132172, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.055508289486169815, 'eval_precision': 0.7298578199052133, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7368421052631579, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5174, 'eval_samples_per_second': 274.221, 'eval_steps_per_second': 34.436, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0017, 'grad_norm': 0.07606880366802216, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.057650670409202576, 'eval_precision': 0.7355769230769231, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7373493975903614, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 5.4958, 'eval_samples_per_second': 275.303, 'eval_steps_per_second': 34.572, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0013, 'grad_norm': 0.12075480073690414, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.058773189783096313, 'eval_precision': 0.7447833065810594, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7459807073954984, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.5691, 'eval_samples_per_second': 271.678, 'eval_steps_per_second': 34.117, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0012, 'grad_norm': 0.08402767032384872, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.058403775095939636, 'eval_precision': 0.7408585055643879, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7456, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.5269, 'eval_samples_per_second': 273.753, 'eval_steps_per_second': 34.377, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 2419.6981, 'train_samples_per_second': 42.501, 'train_steps_per_second': 0.665, 'total_flos': 1.3424700060693864e+16, 'train_loss': 0.021232993439284722, 'epoch': 12.0, 'step': 1608}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =      0.991
  predict_f1                 =      0.734
  predict_loss               =     0.0325
  predict_precision          =     0.7128
  predict_recall             =     0.7566
  predict_runtime            = 0:00:04.62
  predict_samples_per_second =    270.533
  predict_steps_per_second   =     33.925
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_303.json completed. F1: 0.7340425531914894
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_303.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4722.74 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5121.13 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5843.27 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6122.07 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6539.34 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6937.85 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7026.32 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7255.58 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6362.09 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7463.19 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 6222.80 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7399.35 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4633.14 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0747, 'grad_norm': 0.2792517840862274, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.0363343246281147, 'eval_precision': 0.6886503067484663, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7054202670856246, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 5.5425, 'eval_samples_per_second': 272.98, 'eval_steps_per_second': 34.28, 'epoch': 1.0}
{'loss': 0.0205, 'grad_norm': 0.2589074969291687, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.03676971048116684, 'eval_precision': 0.7003058103975535, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7184313725490195, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 5.5308, 'eval_samples_per_second': 273.561, 'eval_steps_per_second': 34.353, 'epoch': 2.0}
{'loss': 0.0078, 'grad_norm': 0.3386368155479431, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04740257188677788, 'eval_precision': 0.6939078751857355, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7217928902627512, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 5.5878, 'eval_samples_per_second': 270.768, 'eval_steps_per_second': 34.003, 'epoch': 3.0}
{'loss': 0.0029, 'grad_norm': 0.26183220744132996, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05122038722038269, 'eval_precision': 0.7100313479623824, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7196187450357427, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 5.5447, 'eval_samples_per_second': 272.874, 'eval_steps_per_second': 34.267, 'epoch': 4.0}
{'loss': 0.0011, 'grad_norm': 0.03679297864437103, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.057713620364665985, 'eval_precision': 0.722662440570523, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7284345047923322, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 5.4852, 'eval_samples_per_second': 275.832, 'eval_steps_per_second': 34.638, 'epoch': 5.0}
{'loss': 0.0007, 'grad_norm': 0.05354846641421318, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.05810968950390816, 'eval_precision': 0.7045101088646968, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7167721518987341, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 5.5469, 'eval_samples_per_second': 272.766, 'eval_steps_per_second': 34.253, 'epoch': 6.0}
{'loss': 0.0007, 'grad_norm': 0.02100302465260029, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.061838556081056595, 'eval_precision': 0.7043343653250774, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7182320441988951, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 6.4388, 'eval_samples_per_second': 234.983, 'eval_steps_per_second': 29.509, 'epoch': 7.0}
{'loss': 0.0001, 'grad_norm': 0.024109240621328354, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06344250589609146, 'eval_precision': 0.7064220183486238, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7247058823529412, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 5.6958, 'eval_samples_per_second': 265.636, 'eval_steps_per_second': 33.358, 'epoch': 8.0}
{'loss': 0.0001, 'grad_norm': 0.037932928651571274, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.067562036216259, 'eval_precision': 0.6888217522658611, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.710833982852689, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 5.7481, 'eval_samples_per_second': 263.217, 'eval_steps_per_second': 33.054, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.0013871944975107908, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06722570955753326, 'eval_precision': 0.7149532710280374, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7268408551068883, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 5.7676, 'eval_samples_per_second': 262.326, 'eval_steps_per_second': 32.942, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.00378775573335588, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06815752387046814, 'eval_precision': 0.7134052388289677, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7291338582677164, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 5.5322, 'eval_samples_per_second': 273.49, 'eval_steps_per_second': 34.344, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.0019053606083616614, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06868942081928253, 'eval_precision': 0.7209302325581395, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7345971563981043, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 5.5564, 'eval_samples_per_second': 272.301, 'eval_steps_per_second': 34.195, 'epoch': 12.0}
{'train_runtime': 3349.368, 'train_samples_per_second': 30.704, 'train_steps_per_second': 0.48, 'train_loss': 0.009071662992213514, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0091
  train_runtime            = 0:55:49.36
  train_samples            =       8570
  train_samples_per_second =     30.704
  train_steps_per_second   =       0.48
[{'loss': 0.0747, 'grad_norm': 0.2792517840862274, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.0363343246281147, 'eval_precision': 0.6886503067484663, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7054202670856246, 'eval_accuracy': 0.9886270371673115, 'eval_runtime': 5.5425, 'eval_samples_per_second': 272.98, 'eval_steps_per_second': 34.28, 'epoch': 1.0, 'step': 134}, {'loss': 0.0205, 'grad_norm': 0.2589074969291687, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.03676971048116684, 'eval_precision': 0.7003058103975535, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7184313725490195, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 5.5308, 'eval_samples_per_second': 273.561, 'eval_steps_per_second': 34.353, 'epoch': 2.0, 'step': 268}, {'loss': 0.0078, 'grad_norm': 0.3386368155479431, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04740257188677788, 'eval_precision': 0.6939078751857355, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7217928902627512, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 5.5878, 'eval_samples_per_second': 270.768, 'eval_steps_per_second': 34.003, 'epoch': 3.0, 'step': 402}, {'loss': 0.0029, 'grad_norm': 0.26183220744132996, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05122038722038269, 'eval_precision': 0.7100313479623824, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7196187450357427, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 5.5447, 'eval_samples_per_second': 272.874, 'eval_steps_per_second': 34.267, 'epoch': 4.0, 'step': 536}, {'loss': 0.0011, 'grad_norm': 0.03679297864437103, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.057713620364665985, 'eval_precision': 0.722662440570523, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7284345047923322, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 5.4852, 'eval_samples_per_second': 275.832, 'eval_steps_per_second': 34.638, 'epoch': 5.0, 'step': 670}, {'loss': 0.0007, 'grad_norm': 0.05354846641421318, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05810968950390816, 'eval_precision': 0.7045101088646968, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7167721518987341, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 5.5469, 'eval_samples_per_second': 272.766, 'eval_steps_per_second': 34.253, 'epoch': 6.0, 'step': 804}, {'loss': 0.0007, 'grad_norm': 0.02100302465260029, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.061838556081056595, 'eval_precision': 0.7043343653250774, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7182320441988951, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 6.4388, 'eval_samples_per_second': 234.983, 'eval_steps_per_second': 29.509, 'epoch': 7.0, 'step': 938}, {'loss': 0.0001, 'grad_norm': 0.024109240621328354, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.06344250589609146, 'eval_precision': 0.7064220183486238, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7247058823529412, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 5.6958, 'eval_samples_per_second': 265.636, 'eval_steps_per_second': 33.358, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0001, 'grad_norm': 0.037932928651571274, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.067562036216259, 'eval_precision': 0.6888217522658611, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.710833982852689, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 5.7481, 'eval_samples_per_second': 263.217, 'eval_steps_per_second': 33.054, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0001, 'grad_norm': 0.0013871944975107908, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06722570955753326, 'eval_precision': 0.7149532710280374, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7268408551068883, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 5.7676, 'eval_samples_per_second': 262.326, 'eval_steps_per_second': 32.942, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0001, 'grad_norm': 0.00378775573335588, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06815752387046814, 'eval_precision': 0.7134052388289677, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7291338582677164, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 5.5322, 'eval_samples_per_second': 273.49, 'eval_steps_per_second': 34.344, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0001, 'grad_norm': 0.0019053606083616614, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06868942081928253, 'eval_precision': 0.7209302325581395, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7345971563981043, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 5.5564, 'eval_samples_per_second': 272.301, 'eval_steps_per_second': 34.195, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 3349.368, 'train_samples_per_second': 30.704, 'train_steps_per_second': 0.48, 'total_flos': 1.3424700060693864e+16, 'train_loss': 0.009071662992213514, 'epoch': 12.0, 'step': 1608}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9911
  predict_f1                 =     0.7331
  predict_loss               =     0.0306
  predict_precision          =      0.717
  predict_recall             =       0.75
  predict_runtime            = 0:00:04.79
  predict_samples_per_second =    261.062
  predict_steps_per_second   =     32.737
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_303.json completed. F1: 0.7331189710610934
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_303.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 3980.59 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5057.85 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 6070.41 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6232.71 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6594.14 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6971.27 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7058.50 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7306.72 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5923.36 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 3598.63 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3207.42 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2740.20 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7064.58 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4116.44 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0808, 'grad_norm': 0.32286587357521057, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.039792027324438095, 'eval_precision': 0.6536050156739812, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6624305003971406, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 4.771, 'eval_samples_per_second': 317.126, 'eval_steps_per_second': 39.824, 'epoch': 1.0}
{'loss': 0.031, 'grad_norm': 0.707647979259491, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04053482785820961, 'eval_precision': 0.674347158218126, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.690251572327044, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5222, 'eval_samples_per_second': 334.573, 'eval_steps_per_second': 42.015, 'epoch': 2.0}
{'loss': 0.0174, 'grad_norm': 0.27480050921440125, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04450104758143425, 'eval_precision': 0.6177777777777778, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6435185185185185, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 4.5991, 'eval_samples_per_second': 328.979, 'eval_steps_per_second': 41.313, 'epoch': 3.0}
{'loss': 0.0085, 'grad_norm': 0.17585688829421997, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.047934502363204956, 'eval_precision': 0.7466887417218543, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7363265306122448, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.582, 'eval_samples_per_second': 330.202, 'eval_steps_per_second': 41.466, 'epoch': 4.0}
{'loss': 0.0041, 'grad_norm': 0.2135692983865738, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05784064903855324, 'eval_precision': 0.6562962962962963, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6836419753086419, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 4.5998, 'eval_samples_per_second': 328.925, 'eval_steps_per_second': 41.306, 'epoch': 5.0}
{'loss': 0.0016, 'grad_norm': 0.0022959779016673565, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06261170655488968, 'eval_precision': 0.7069767441860465, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7203791469194313, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6872, 'eval_samples_per_second': 322.795, 'eval_steps_per_second': 40.536, 'epoch': 6.0}
{'loss': 0.0007, 'grad_norm': 0.004808797501027584, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06979500502347946, 'eval_precision': 0.7265372168284789, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7247780468119451, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.534, 'eval_samples_per_second': 333.7, 'eval_steps_per_second': 41.905, 'epoch': 7.0}
{'loss': 0.0006, 'grad_norm': 0.0026108697056770325, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07106488943099976, 'eval_precision': 0.7189542483660131, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.713706407137064, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.5628, 'eval_samples_per_second': 331.598, 'eval_steps_per_second': 41.641, 'epoch': 8.0}
{'loss': 0.0006, 'grad_norm': 0.006975056603550911, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06987760215997696, 'eval_precision': 0.7131537242472267, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7188498402555911, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.5237, 'eval_samples_per_second': 334.461, 'eval_steps_per_second': 42.001, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.022022852674126625, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07395923882722855, 'eval_precision': 0.717948717948718, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7196787148594378, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6172, 'eval_samples_per_second': 327.691, 'eval_steps_per_second': 41.151, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.027381887659430504, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.0753985047340393, 'eval_precision': 0.7333333333333333, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7386091127098321, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.652, 'eval_samples_per_second': 325.239, 'eval_steps_per_second': 40.843, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.005209135357290506, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07561573386192322, 'eval_precision': 0.7376, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.739967897271268, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 4.8778, 'eval_samples_per_second': 310.178, 'eval_steps_per_second': 38.952, 'epoch': 12.0}
{'train_runtime': 1287.2871, 'train_samples_per_second': 79.889, 'train_steps_per_second': 1.249, 'train_loss': 0.012145598208188862, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0121
  train_runtime            = 0:21:27.28
  train_samples            =       8570
  train_samples_per_second =     79.889
  train_steps_per_second   =      1.249
[{'loss': 0.0808, 'grad_norm': 0.32286587357521057, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.039792027324438095, 'eval_precision': 0.6536050156739812, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6624305003971406, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 4.771, 'eval_samples_per_second': 317.126, 'eval_steps_per_second': 39.824, 'epoch': 1.0, 'step': 134}, {'loss': 0.031, 'grad_norm': 0.707647979259491, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04053482785820961, 'eval_precision': 0.674347158218126, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.690251572327044, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.5222, 'eval_samples_per_second': 334.573, 'eval_steps_per_second': 42.015, 'epoch': 2.0, 'step': 268}, {'loss': 0.0174, 'grad_norm': 0.27480050921440125, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04450104758143425, 'eval_precision': 0.6177777777777778, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6435185185185185, 'eval_accuracy': 0.9862820963770664, 'eval_runtime': 4.5991, 'eval_samples_per_second': 328.979, 'eval_steps_per_second': 41.313, 'epoch': 3.0, 'step': 402}, {'loss': 0.0085, 'grad_norm': 0.17585688829421997, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.047934502363204956, 'eval_precision': 0.7466887417218543, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7363265306122448, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.582, 'eval_samples_per_second': 330.202, 'eval_steps_per_second': 41.466, 'epoch': 4.0, 'step': 536}, {'loss': 0.0041, 'grad_norm': 0.2135692983865738, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05784064903855324, 'eval_precision': 0.6562962962962963, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6836419753086419, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 4.5998, 'eval_samples_per_second': 328.925, 'eval_steps_per_second': 41.306, 'epoch': 5.0, 'step': 670}, {'loss': 0.0016, 'grad_norm': 0.0022959779016673565, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06261170655488968, 'eval_precision': 0.7069767441860465, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7203791469194313, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.6872, 'eval_samples_per_second': 322.795, 'eval_steps_per_second': 40.536, 'epoch': 6.0, 'step': 804}, {'loss': 0.0007, 'grad_norm': 0.004808797501027584, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06979500502347946, 'eval_precision': 0.7265372168284789, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7247780468119451, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.534, 'eval_samples_per_second': 333.7, 'eval_steps_per_second': 41.905, 'epoch': 7.0, 'step': 938}, {'loss': 0.0006, 'grad_norm': 0.0026108697056770325, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07106488943099976, 'eval_precision': 0.7189542483660131, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.713706407137064, 'eval_accuracy': 0.9887442842068238, 'eval_runtime': 4.5628, 'eval_samples_per_second': 331.598, 'eval_steps_per_second': 41.641, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0006, 'grad_norm': 0.006975056603550911, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06987760215997696, 'eval_precision': 0.7131537242472267, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7188498402555911, 'eval_accuracy': 0.9890960253253606, 'eval_runtime': 4.5237, 'eval_samples_per_second': 334.461, 'eval_steps_per_second': 42.001, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0002, 'grad_norm': 0.022022852674126625, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07395923882722855, 'eval_precision': 0.717948717948718, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7196787148594378, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6172, 'eval_samples_per_second': 327.691, 'eval_steps_per_second': 41.151, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0002, 'grad_norm': 0.027381887659430504, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.0753985047340393, 'eval_precision': 0.7333333333333333, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7386091127098321, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.652, 'eval_samples_per_second': 325.239, 'eval_steps_per_second': 40.843, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0001, 'grad_norm': 0.005209135357290506, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07561573386192322, 'eval_precision': 0.7376, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.739967897271268, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 4.8778, 'eval_samples_per_second': 310.178, 'eval_steps_per_second': 38.952, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1287.2871, 'train_samples_per_second': 79.889, 'train_steps_per_second': 1.249, 'total_flos': 1.290621679981722e+16, 'train_loss': 0.012145598208188862, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =      0.987
  predict_f1                 =     0.6451
  predict_loss               =     0.0404
  predict_precision          =     0.6348
  predict_recall             =     0.6557
  predict_runtime            = 0:00:04.00
  predict_samples_per_second =    312.975
  predict_steps_per_second   =     39.247
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_303.json completed. F1: 0.6450916936353829
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_202.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4416.47 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5352.00 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5834.04 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6360.29 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6643.66 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6960.78 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7010.96 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7223.07 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6091.48 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6167.48 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2130.95 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7269.51 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5455.30 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.181, 'grad_norm': 0.5790396928787231, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.055736150592565536, 'eval_precision': 0.5700787401574803, 'eval_recall': 0.5829307568438004, 'eval_f1': 0.5764331210191083, 'eval_accuracy': 0.9815531324500723, 'eval_runtime': 2.4495, 'eval_samples_per_second': 617.677, 'eval_steps_per_second': 77.567, 'epoch': 1.0}
{'loss': 0.0462, 'grad_norm': 0.39269647002220154, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04577194154262543, 'eval_precision': 0.6112759643916914, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6362934362934363, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.251, 'eval_samples_per_second': 672.154, 'eval_steps_per_second': 84.408, 'epoch': 2.0}
{'loss': 0.0357, 'grad_norm': 0.4128834009170532, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.043930284678936005, 'eval_precision': 0.6477794793261868, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6640502354788068, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.2571, 'eval_samples_per_second': 670.344, 'eval_steps_per_second': 84.181, 'epoch': 3.0}
{'loss': 0.0302, 'grad_norm': 0.40437355637550354, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04536787047982216, 'eval_precision': 0.6431852986217458, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6593406593406593, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3597, 'eval_samples_per_second': 641.17, 'eval_steps_per_second': 80.517, 'epoch': 4.0}
{'loss': 0.0254, 'grad_norm': 0.5609546303749084, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.044663019478321075, 'eval_precision': 0.684931506849315, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.704225352112676, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.4431, 'eval_samples_per_second': 619.292, 'eval_steps_per_second': 77.77, 'epoch': 5.0}
{'loss': 0.0217, 'grad_norm': 0.9003572463989258, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04608449712395668, 'eval_precision': 0.6606334841628959, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6822429906542057, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.5386, 'eval_samples_per_second': 596.009, 'eval_steps_per_second': 74.846, 'epoch': 6.0}
{'loss': 0.0183, 'grad_norm': 0.44940710067749023, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04719631373882294, 'eval_precision': 0.661608497723824, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6812499999999999, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.4788, 'eval_samples_per_second': 610.382, 'eval_steps_per_second': 76.651, 'epoch': 7.0}
{'loss': 0.0156, 'grad_norm': 0.4757234454154968, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.04849184677004814, 'eval_precision': 0.6640986132511556, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6787401574803149, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.4773, 'eval_samples_per_second': 610.751, 'eval_steps_per_second': 76.697, 'epoch': 8.0}
{'loss': 0.0136, 'grad_norm': 0.46063071489334106, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.049976713955402374, 'eval_precision': 0.6626323751891074, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6833073322932918, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.2961, 'eval_samples_per_second': 658.934, 'eval_steps_per_second': 82.748, 'epoch': 9.0}
{'loss': 0.0122, 'grad_norm': 0.31526973843574524, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05000622197985649, 'eval_precision': 0.7036474164133738, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7240031274433151, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 2.4121, 'eval_samples_per_second': 627.259, 'eval_steps_per_second': 78.77, 'epoch': 10.0}
{'loss': 0.0111, 'grad_norm': 0.25857582688331604, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05111982673406601, 'eval_precision': 0.6793893129770993, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6974921630094044, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.243, 'eval_samples_per_second': 674.55, 'eval_steps_per_second': 84.709, 'epoch': 11.0}
{'loss': 0.0105, 'grad_norm': 0.4133397936820984, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05132570490241051, 'eval_precision': 0.6809160305343511, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6990595611285266, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.5645, 'eval_samples_per_second': 589.973, 'eval_steps_per_second': 74.088, 'epoch': 12.0}
{'train_runtime': 589.4916, 'train_samples_per_second': 174.455, 'train_steps_per_second': 2.728, 'train_loss': 0.03514220947353401, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0351
  train_runtime            = 0:09:49.49
  train_samples            =       8570
  train_samples_per_second =    174.455
  train_steps_per_second   =      2.728
[{'loss': 0.181, 'grad_norm': 0.5790396928787231, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.055736150592565536, 'eval_precision': 0.5700787401574803, 'eval_recall': 0.5829307568438004, 'eval_f1': 0.5764331210191083, 'eval_accuracy': 0.9815531324500723, 'eval_runtime': 2.4495, 'eval_samples_per_second': 617.677, 'eval_steps_per_second': 77.567, 'epoch': 1.0, 'step': 134}, {'loss': 0.0462, 'grad_norm': 0.39269647002220154, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04577194154262543, 'eval_precision': 0.6112759643916914, 'eval_recall': 0.6634460547504025, 'eval_f1': 0.6362934362934363, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.251, 'eval_samples_per_second': 672.154, 'eval_steps_per_second': 84.408, 'epoch': 2.0, 'step': 268}, {'loss': 0.0357, 'grad_norm': 0.4128834009170532, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.043930284678936005, 'eval_precision': 0.6477794793261868, 'eval_recall': 0.6811594202898551, 'eval_f1': 0.6640502354788068, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.2571, 'eval_samples_per_second': 670.344, 'eval_steps_per_second': 84.181, 'epoch': 3.0, 'step': 402}, {'loss': 0.0302, 'grad_norm': 0.40437355637550354, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04536787047982216, 'eval_precision': 0.6431852986217458, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6593406593406593, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.3597, 'eval_samples_per_second': 641.17, 'eval_steps_per_second': 80.517, 'epoch': 4.0, 'step': 536}, {'loss': 0.0254, 'grad_norm': 0.5609546303749084, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.044663019478321075, 'eval_precision': 0.684931506849315, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.704225352112676, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.4431, 'eval_samples_per_second': 619.292, 'eval_steps_per_second': 77.77, 'epoch': 5.0, 'step': 670}, {'loss': 0.0217, 'grad_norm': 0.9003572463989258, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04608449712395668, 'eval_precision': 0.6606334841628959, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6822429906542057, 'eval_accuracy': 0.9864775081095869, 'eval_runtime': 2.5386, 'eval_samples_per_second': 596.009, 'eval_steps_per_second': 74.846, 'epoch': 6.0, 'step': 804}, {'loss': 0.0183, 'grad_norm': 0.44940710067749023, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.04719631373882294, 'eval_precision': 0.661608497723824, 'eval_recall': 0.7020933977455717, 'eval_f1': 0.6812499999999999, 'eval_accuracy': 0.9867901668816196, 'eval_runtime': 2.4788, 'eval_samples_per_second': 610.382, 'eval_steps_per_second': 76.651, 'epoch': 7.0, 'step': 938}, {'loss': 0.0156, 'grad_norm': 0.4757234454154968, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.04849184677004814, 'eval_precision': 0.6640986132511556, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6787401574803149, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.4773, 'eval_samples_per_second': 610.751, 'eval_steps_per_second': 76.697, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0136, 'grad_norm': 0.46063071489334106, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.049976713955402374, 'eval_precision': 0.6626323751891074, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6833073322932918, 'eval_accuracy': 0.986516590456091, 'eval_runtime': 2.2961, 'eval_samples_per_second': 658.934, 'eval_steps_per_second': 82.748, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0122, 'grad_norm': 0.31526973843574524, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05000622197985649, 'eval_precision': 0.7036474164133738, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7240031274433151, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 2.4121, 'eval_samples_per_second': 627.259, 'eval_steps_per_second': 78.77, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0111, 'grad_norm': 0.25857582688331604, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05111982673406601, 'eval_precision': 0.6793893129770993, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6974921630094044, 'eval_accuracy': 0.9871028256536523, 'eval_runtime': 2.243, 'eval_samples_per_second': 674.55, 'eval_steps_per_second': 84.709, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0105, 'grad_norm': 0.4133397936820984, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05132570490241051, 'eval_precision': 0.6809160305343511, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6990595611285266, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.5645, 'eval_samples_per_second': 589.973, 'eval_steps_per_second': 74.088, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 589.4916, 'train_samples_per_second': 174.455, 'train_steps_per_second': 2.728, 'total_flos': 4417493824075596.0, 'train_loss': 0.03514220947353401, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =      0.989
  predict_f1                 =     0.6817
  predict_loss               =     0.0373
  predict_precision          =     0.6667
  predict_recall             =     0.6974
  predict_runtime            = 0:00:02.00
  predict_samples_per_second =    623.898
  predict_steps_per_second   =     78.236
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_202.json completed. F1: 0.6816720257234726
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_101.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3760.50 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 3170.27 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4206.23 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5037.55 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5635.84 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6140.11 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6384.16 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6707.76 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5213.55 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7065.54 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4177.86 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6881.81 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5069.28 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0695, 'grad_norm': 0.5961449146270752, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.0398520790040493, 'eval_precision': 0.672782874617737, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6901960784313725, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.4513, 'eval_samples_per_second': 617.225, 'eval_steps_per_second': 77.51, 'epoch': 1.0}
{'loss': 0.03, 'grad_norm': 0.40605196356773376, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04245094582438469, 'eval_precision': 0.5888888888888889, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6323639075316928, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.4059, 'eval_samples_per_second': 628.868, 'eval_steps_per_second': 78.972, 'epoch': 2.0}
{'loss': 0.018, 'grad_norm': 0.12524297833442688, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04341592267155647, 'eval_precision': 0.6804281345565749, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6980392156862745, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.331, 'eval_samples_per_second': 649.068, 'eval_steps_per_second': 81.509, 'epoch': 3.0}
{'loss': 0.0093, 'grad_norm': 0.8248512148857117, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04652368649840355, 'eval_precision': 0.6945736434108527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7077409162717219, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 2.3746, 'eval_samples_per_second': 637.148, 'eval_steps_per_second': 80.012, 'epoch': 4.0}
{'loss': 0.0044, 'grad_norm': 0.6966655254364014, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05736149102449417, 'eval_precision': 0.6764705882352942, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7071483474250576, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.9693, 'eval_samples_per_second': 509.54, 'eval_steps_per_second': 63.987, 'epoch': 5.0}
{'loss': 0.003, 'grad_norm': 1.0776393413543701, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06331922858953476, 'eval_precision': 0.6626323751891074, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6833073322932918, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3743, 'eval_samples_per_second': 637.24, 'eval_steps_per_second': 80.023, 'epoch': 6.0}
{'loss': 0.0017, 'grad_norm': 0.0720701739192009, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.0738058015704155, 'eval_precision': 0.6822289156626506, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.705058365758755, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.4244, 'eval_samples_per_second': 624.082, 'eval_steps_per_second': 78.371, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.026005476713180542, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.0716615542769432, 'eval_precision': 0.7097288676236044, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7131410256410257, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.3329, 'eval_samples_per_second': 648.553, 'eval_steps_per_second': 81.444, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.004385600797832012, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07802033424377441, 'eval_precision': 0.7062600321027287, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.707395498392283, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3279, 'eval_samples_per_second': 649.955, 'eval_steps_per_second': 81.62, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.005868938751518726, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07893550395965576, 'eval_precision': 0.703125, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7137192704203014, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.3096, 'eval_samples_per_second': 655.092, 'eval_steps_per_second': 82.265, 'epoch': 10.0}
{'loss': 0.0003, 'grad_norm': 0.001366686075925827, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08082488924264908, 'eval_precision': 0.6915887850467289, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7030878859857481, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3133, 'eval_samples_per_second': 654.044, 'eval_steps_per_second': 82.134, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.0015152707928791642, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08113232254981995, 'eval_precision': 0.6962616822429907, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7078384798099763, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 2.3321, 'eval_samples_per_second': 648.768, 'eval_steps_per_second': 81.471, 'epoch': 12.0}
{'train_runtime': 518.7395, 'train_samples_per_second': 198.25, 'train_steps_per_second': 6.2, 'train_loss': 0.011508791436978723, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0115
  train_runtime            = 0:08:38.73
  train_samples            =       8570
  train_samples_per_second =     198.25
  train_steps_per_second   =        6.2
[{'loss': 0.0695, 'grad_norm': 0.5961449146270752, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.0398520790040493, 'eval_precision': 0.672782874617737, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6901960784313725, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.4513, 'eval_samples_per_second': 617.225, 'eval_steps_per_second': 77.51, 'epoch': 1.0, 'step': 268}, {'loss': 0.03, 'grad_norm': 0.40605196356773376, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04245094582438469, 'eval_precision': 0.5888888888888889, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6323639075316928, 'eval_accuracy': 0.9855786141399929, 'eval_runtime': 2.4059, 'eval_samples_per_second': 628.868, 'eval_steps_per_second': 78.972, 'epoch': 2.0, 'step': 536}, {'loss': 0.018, 'grad_norm': 0.12524297833442688, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04341592267155647, 'eval_precision': 0.6804281345565749, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6980392156862745, 'eval_accuracy': 0.9873373197326768, 'eval_runtime': 2.331, 'eval_samples_per_second': 649.068, 'eval_steps_per_second': 81.509, 'epoch': 3.0, 'step': 804}, {'loss': 0.0093, 'grad_norm': 0.8248512148857117, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04652368649840355, 'eval_precision': 0.6945736434108527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7077409162717219, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 2.3746, 'eval_samples_per_second': 637.148, 'eval_steps_per_second': 80.012, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0044, 'grad_norm': 0.6966655254364014, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.05736149102449417, 'eval_precision': 0.6764705882352942, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7071483474250576, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.9693, 'eval_samples_per_second': 509.54, 'eval_steps_per_second': 63.987, 'epoch': 5.0, 'step': 1340}, {'loss': 0.003, 'grad_norm': 1.0776393413543701, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.06331922858953476, 'eval_precision': 0.6626323751891074, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6833073322932918, 'eval_accuracy': 0.9866729198421074, 'eval_runtime': 2.3743, 'eval_samples_per_second': 637.24, 'eval_steps_per_second': 80.023, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0017, 'grad_norm': 0.0720701739192009, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.0738058015704155, 'eval_precision': 0.6822289156626506, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.705058365758755, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.4244, 'eval_samples_per_second': 624.082, 'eval_steps_per_second': 78.371, 'epoch': 7.0, 'step': 1876}, {'loss': 0.001, 'grad_norm': 0.026005476713180542, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.0716615542769432, 'eval_precision': 0.7097288676236044, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7131410256410257, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.3329, 'eval_samples_per_second': 648.553, 'eval_steps_per_second': 81.444, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0005, 'grad_norm': 0.004385600797832012, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07802033424377441, 'eval_precision': 0.7062600321027287, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.707395498392283, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3279, 'eval_samples_per_second': 649.955, 'eval_steps_per_second': 81.62, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.005868938751518726, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.07893550395965576, 'eval_precision': 0.703125, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7137192704203014, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.3096, 'eval_samples_per_second': 655.092, 'eval_steps_per_second': 82.265, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0003, 'grad_norm': 0.001366686075925827, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08082488924264908, 'eval_precision': 0.6915887850467289, 'eval_recall': 0.714975845410628, 'eval_f1': 0.7030878859857481, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.3133, 'eval_samples_per_second': 654.044, 'eval_steps_per_second': 82.134, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0001, 'grad_norm': 0.0015152707928791642, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08113232254981995, 'eval_precision': 0.6962616822429907, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7078384798099763, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 2.3321, 'eval_samples_per_second': 648.768, 'eval_steps_per_second': 81.471, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 518.7395, 'train_samples_per_second': 198.25, 'train_steps_per_second': 6.2, 'total_flos': 3923375652683916.0, 'train_loss': 0.011508791436978723, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9878
  predict_f1                 =     0.6852
  predict_loss               =     0.0385
  predict_precision          =     0.6674
  predict_recall             =     0.7039
  predict_runtime            = 0:00:02.03
  predict_samples_per_second =    616.137
  predict_steps_per_second   =     77.263
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_101.json completed. F1: 0.6851654215581644
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_101.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6267.18 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5447.21 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5861.32 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6179.02 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6510.68 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6809.95 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6861.90 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7066.79 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6340.59 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7137.58 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3936.14 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7081.65 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4658.24 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-134 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1539, 'grad_norm': 0.3296232223510742, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04979319125413895, 'eval_precision': 0.5184615384615384, 'eval_recall': 0.5426731078904992, 'eval_f1': 0.5302911093627064, 'eval_accuracy': 0.9837808262008051, 'eval_runtime': 5.8469, 'eval_samples_per_second': 258.772, 'eval_steps_per_second': 32.496, 'epoch': 1.0}
{'loss': 0.0346, 'grad_norm': 0.5587783455848694, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.042312636971473694, 'eval_precision': 0.6047548291233283, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6290571870170016, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 5.839, 'eval_samples_per_second': 259.121, 'eval_steps_per_second': 32.54, 'epoch': 2.0}
{'loss': 0.0244, 'grad_norm': 0.29749438166618347, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03996684402227402, 'eval_precision': 0.69826224328594, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7049441786283892, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 5.8143, 'eval_samples_per_second': 260.22, 'eval_steps_per_second': 32.678, 'epoch': 3.0}
{'loss': 0.0167, 'grad_norm': 0.6218884587287903, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04161093011498451, 'eval_precision': 0.6661585365853658, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6844166014095536, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 5.8615, 'eval_samples_per_second': 258.127, 'eval_steps_per_second': 32.415, 'epoch': 4.0}
{'loss': 0.0102, 'grad_norm': 0.4983583390712738, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.045037612318992615, 'eval_precision': 0.6864535768645358, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7057902973395931, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 5.8367, 'eval_samples_per_second': 259.221, 'eval_steps_per_second': 32.553, 'epoch': 5.0}
{'loss': 0.0066, 'grad_norm': 1.1108839511871338, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04979821667075157, 'eval_precision': 0.7023255813953488, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7156398104265402, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 5.8051, 'eval_samples_per_second': 260.634, 'eval_steps_per_second': 32.73, 'epoch': 6.0}
{'loss': 0.0043, 'grad_norm': 0.3678855299949646, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.0512876957654953, 'eval_precision': 0.6906584992343032, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7080062794348508, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 5.9041, 'eval_samples_per_second': 256.263, 'eval_steps_per_second': 32.181, 'epoch': 7.0}
{'loss': 0.003, 'grad_norm': 0.37565016746520996, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05381874740123749, 'eval_precision': 0.7353407290015848, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7412140575079872, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 5.863, 'eval_samples_per_second': 258.061, 'eval_steps_per_second': 32.407, 'epoch': 8.0}
{'loss': 0.0023, 'grad_norm': 0.3145071566104889, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.055756647139787674, 'eval_precision': 0.7091194968553459, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7175815433571997, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 5.8757, 'eval_samples_per_second': 257.503, 'eval_steps_per_second': 32.337, 'epoch': 9.0}
{'loss': 0.0017, 'grad_norm': 0.22504815459251404, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05793910473585129, 'eval_precision': 0.7193798449612403, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7330173775671407, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 5.8394, 'eval_samples_per_second': 259.102, 'eval_steps_per_second': 32.538, 'epoch': 10.0}
{'loss': 0.0014, 'grad_norm': 0.5821225643157959, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.058721307665109634, 'eval_precision': 0.7245696400625978, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.734920634920635, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.9014, 'eval_samples_per_second': 256.38, 'eval_steps_per_second': 32.196, 'epoch': 11.0}
{'loss': 0.0012, 'grad_norm': 0.1736297607421875, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05906357616186142, 'eval_precision': 0.7252747252747253, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7344992050874404, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.8865, 'eval_samples_per_second': 257.029, 'eval_steps_per_second': 32.277, 'epoch': 12.0}
{'train_runtime': 1508.1249, 'train_samples_per_second': 68.191, 'train_steps_per_second': 1.066, 'train_loss': 0.021691896707116076, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0217
  train_runtime            = 0:25:08.12
  train_samples            =       8570
  train_samples_per_second =     68.191
  train_steps_per_second   =      1.066
[{'loss': 0.1539, 'grad_norm': 0.3296232223510742, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04979319125413895, 'eval_precision': 0.5184615384615384, 'eval_recall': 0.5426731078904992, 'eval_f1': 0.5302911093627064, 'eval_accuracy': 0.9837808262008051, 'eval_runtime': 5.8469, 'eval_samples_per_second': 258.772, 'eval_steps_per_second': 32.496, 'epoch': 1.0, 'step': 134}, {'loss': 0.0346, 'grad_norm': 0.5587783455848694, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.042312636971473694, 'eval_precision': 0.6047548291233283, 'eval_recall': 0.6553945249597424, 'eval_f1': 0.6290571870170016, 'eval_accuracy': 0.9859694376050339, 'eval_runtime': 5.839, 'eval_samples_per_second': 259.121, 'eval_steps_per_second': 32.54, 'epoch': 2.0, 'step': 268}, {'loss': 0.0244, 'grad_norm': 0.29749438166618347, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.03996684402227402, 'eval_precision': 0.69826224328594, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7049441786283892, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 5.8143, 'eval_samples_per_second': 260.22, 'eval_steps_per_second': 32.678, 'epoch': 3.0, 'step': 402}, {'loss': 0.0167, 'grad_norm': 0.6218884587287903, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04161093011498451, 'eval_precision': 0.6661585365853658, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6844166014095536, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 5.8615, 'eval_samples_per_second': 258.127, 'eval_steps_per_second': 32.415, 'epoch': 4.0, 'step': 536}, {'loss': 0.0102, 'grad_norm': 0.4983583390712738, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.045037612318992615, 'eval_precision': 0.6864535768645358, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7057902973395931, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 5.8367, 'eval_samples_per_second': 259.221, 'eval_steps_per_second': 32.553, 'epoch': 5.0, 'step': 670}, {'loss': 0.0066, 'grad_norm': 1.1108839511871338, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04979821667075157, 'eval_precision': 0.7023255813953488, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.7156398104265402, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 5.8051, 'eval_samples_per_second': 260.634, 'eval_steps_per_second': 32.73, 'epoch': 6.0, 'step': 804}, {'loss': 0.0043, 'grad_norm': 0.3678855299949646, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.0512876957654953, 'eval_precision': 0.6906584992343032, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7080062794348508, 'eval_accuracy': 0.9885488724743033, 'eval_runtime': 5.9041, 'eval_samples_per_second': 256.263, 'eval_steps_per_second': 32.181, 'epoch': 7.0, 'step': 938}, {'loss': 0.003, 'grad_norm': 0.37565016746520996, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05381874740123749, 'eval_precision': 0.7353407290015848, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7412140575079872, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 5.863, 'eval_samples_per_second': 258.061, 'eval_steps_per_second': 32.407, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0023, 'grad_norm': 0.3145071566104889, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.055756647139787674, 'eval_precision': 0.7091194968553459, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7175815433571997, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 5.8757, 'eval_samples_per_second': 257.503, 'eval_steps_per_second': 32.337, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0017, 'grad_norm': 0.22504815459251404, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05793910473585129, 'eval_precision': 0.7193798449612403, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7330173775671407, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 5.8394, 'eval_samples_per_second': 259.102, 'eval_steps_per_second': 32.538, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0014, 'grad_norm': 0.5821225643157959, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.058721307665109634, 'eval_precision': 0.7245696400625978, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.734920634920635, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.9014, 'eval_samples_per_second': 256.38, 'eval_steps_per_second': 32.196, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0012, 'grad_norm': 0.1736297607421875, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05906357616186142, 'eval_precision': 0.7252747252747253, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7344992050874404, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.8865, 'eval_samples_per_second': 257.029, 'eval_steps_per_second': 32.277, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1508.1249, 'train_samples_per_second': 68.191, 'train_steps_per_second': 1.066, 'total_flos': 1.344041490771522e+16, 'train_loss': 0.021691896707116076, 'epoch': 12.0, 'step': 1608}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9904
  predict_f1                 =     0.7212
  predict_loss               =      0.035
  predict_precision          =     0.7082
  predict_recall             =     0.7346
  predict_runtime            = 0:00:04.88
  predict_samples_per_second =    256.326
  predict_steps_per_second   =     32.143
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_101.json completed. F1: 0.721205597416577
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_101.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2572.43 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4023.44 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4938.01 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5196.30 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5757.69 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6246.08 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6451.15 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6748.95 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5517.41 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6528.70 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5508.36 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7050.87 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4397.03 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-402 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0759, 'grad_norm': 0.31803134083747864, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04028506949543953, 'eval_precision': 0.676126878130217, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6639344262295083, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 5.7673, 'eval_samples_per_second': 262.343, 'eval_steps_per_second': 32.945, 'epoch': 1.0}
{'loss': 0.0195, 'grad_norm': 0.12624970078468323, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.037760283797979355, 'eval_precision': 0.6626506024096386, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6848249027237355, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 5.7597, 'eval_samples_per_second': 262.688, 'eval_steps_per_second': 32.988, 'epoch': 2.0}
{'loss': 0.0078, 'grad_norm': 0.16814793646335602, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04194682091474533, 'eval_precision': 0.7197452229299363, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7237790232185749, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 5.8829, 'eval_samples_per_second': 257.187, 'eval_steps_per_second': 32.297, 'epoch': 3.0}
{'loss': 0.0028, 'grad_norm': 0.17736853659152985, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05023667961359024, 'eval_precision': 0.657185628742515, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6811481768813032, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 5.7521, 'eval_samples_per_second': 263.034, 'eval_steps_per_second': 33.031, 'epoch': 4.0}
{'loss': 0.0015, 'grad_norm': 0.028764039278030396, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05060933530330658, 'eval_precision': 0.7461059190031153, 'eval_recall': 0.7713365539452496, 'eval_f1': 0.7585114806017419, 'eval_accuracy': 0.9905420721460116, 'eval_runtime': 5.7258, 'eval_samples_per_second': 264.242, 'eval_steps_per_second': 33.183, 'epoch': 5.0}
{'loss': 0.0007, 'grad_norm': 0.14860059320926666, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.05340912938117981, 'eval_precision': 0.7275541795665634, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7419100236779794, 'eval_accuracy': 0.9901512486809708, 'eval_runtime': 5.773, 'eval_samples_per_second': 262.083, 'eval_steps_per_second': 32.912, 'epoch': 6.0}
{'loss': 0.0002, 'grad_norm': 0.0024427117314189672, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.05704174563288689, 'eval_precision': 0.7428571428571429, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.748201438848921, 'eval_accuracy': 0.990268495720483, 'eval_runtime': 5.7601, 'eval_samples_per_second': 262.669, 'eval_steps_per_second': 32.986, 'epoch': 7.0}
{'loss': 0.0002, 'grad_norm': 0.005186848808079958, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.05955551937222481, 'eval_precision': 0.7519872813990461, 'eval_recall': 0.7616747181964574, 'eval_f1': 0.7567999999999999, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 5.8573, 'eval_samples_per_second': 258.309, 'eval_steps_per_second': 32.438, 'epoch': 8.0}
{'loss': 0.0001, 'grad_norm': 0.0027293432503938675, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06130870431661606, 'eval_precision': 0.7560192616372392, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.757234726688103, 'eval_accuracy': 0.9904248251064994, 'eval_runtime': 5.7214, 'eval_samples_per_second': 264.445, 'eval_steps_per_second': 33.209, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.008631795644760132, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06126251444220543, 'eval_precision': 0.7340590979782271, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7468354430379747, 'eval_accuracy': 0.9900730839879627, 'eval_runtime': 5.7767, 'eval_samples_per_second': 261.916, 'eval_steps_per_second': 32.891, 'epoch': 10.0}
{'loss': 0.0, 'grad_norm': 0.02612985670566559, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06213635951280594, 'eval_precision': 0.7329192546583851, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7462450592885376, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 5.7797, 'eval_samples_per_second': 261.778, 'eval_steps_per_second': 32.874, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 0.012099351733922958, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.062315259128808975, 'eval_precision': 0.735202492211838, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7474267616785433, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 5.7478, 'eval_samples_per_second': 263.232, 'eval_steps_per_second': 33.056, 'epoch': 12.0}
{'train_runtime': 1406.9568, 'train_samples_per_second': 73.094, 'train_steps_per_second': 1.143, 'train_loss': 0.009083204331057174, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0091
  train_runtime            = 0:23:26.95
  train_samples            =       8570
  train_samples_per_second =     73.094
  train_steps_per_second   =      1.143
[{'loss': 0.0759, 'grad_norm': 0.31803134083747864, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04028506949543953, 'eval_precision': 0.676126878130217, 'eval_recall': 0.6521739130434783, 'eval_f1': 0.6639344262295083, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 5.7673, 'eval_samples_per_second': 262.343, 'eval_steps_per_second': 32.945, 'epoch': 1.0, 'step': 134}, {'loss': 0.0195, 'grad_norm': 0.12624970078468323, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.037760283797979355, 'eval_precision': 0.6626506024096386, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6848249027237355, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 5.7597, 'eval_samples_per_second': 262.688, 'eval_steps_per_second': 32.988, 'epoch': 2.0, 'step': 268}, {'loss': 0.0078, 'grad_norm': 0.16814793646335602, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04194682091474533, 'eval_precision': 0.7197452229299363, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7237790232185749, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 5.8829, 'eval_samples_per_second': 257.187, 'eval_steps_per_second': 32.297, 'epoch': 3.0, 'step': 402}, {'loss': 0.0028, 'grad_norm': 0.17736853659152985, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05023667961359024, 'eval_precision': 0.657185628742515, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6811481768813032, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 5.7521, 'eval_samples_per_second': 263.034, 'eval_steps_per_second': 33.031, 'epoch': 4.0, 'step': 536}, {'loss': 0.0015, 'grad_norm': 0.028764039278030396, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05060933530330658, 'eval_precision': 0.7461059190031153, 'eval_recall': 0.7713365539452496, 'eval_f1': 0.7585114806017419, 'eval_accuracy': 0.9905420721460116, 'eval_runtime': 5.7258, 'eval_samples_per_second': 264.242, 'eval_steps_per_second': 33.183, 'epoch': 5.0, 'step': 670}, {'loss': 0.0007, 'grad_norm': 0.14860059320926666, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05340912938117981, 'eval_precision': 0.7275541795665634, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7419100236779794, 'eval_accuracy': 0.9901512486809708, 'eval_runtime': 5.773, 'eval_samples_per_second': 262.083, 'eval_steps_per_second': 32.912, 'epoch': 6.0, 'step': 804}, {'loss': 0.0002, 'grad_norm': 0.0024427117314189672, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05704174563288689, 'eval_precision': 0.7428571428571429, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.748201438848921, 'eval_accuracy': 0.990268495720483, 'eval_runtime': 5.7601, 'eval_samples_per_second': 262.669, 'eval_steps_per_second': 32.986, 'epoch': 7.0, 'step': 938}, {'loss': 0.0002, 'grad_norm': 0.005186848808079958, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05955551937222481, 'eval_precision': 0.7519872813990461, 'eval_recall': 0.7616747181964574, 'eval_f1': 0.7567999999999999, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 5.8573, 'eval_samples_per_second': 258.309, 'eval_steps_per_second': 32.438, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0001, 'grad_norm': 0.0027293432503938675, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06130870431661606, 'eval_precision': 0.7560192616372392, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.757234726688103, 'eval_accuracy': 0.9904248251064994, 'eval_runtime': 5.7214, 'eval_samples_per_second': 264.445, 'eval_steps_per_second': 33.209, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0001, 'grad_norm': 0.008631795644760132, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06126251444220543, 'eval_precision': 0.7340590979782271, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7468354430379747, 'eval_accuracy': 0.9900730839879627, 'eval_runtime': 5.7767, 'eval_samples_per_second': 261.916, 'eval_steps_per_second': 32.891, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0, 'grad_norm': 0.02612985670566559, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06213635951280594, 'eval_precision': 0.7329192546583851, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7462450592885376, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 5.7797, 'eval_samples_per_second': 261.778, 'eval_steps_per_second': 32.874, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0, 'grad_norm': 0.012099351733922958, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.062315259128808975, 'eval_precision': 0.735202492211838, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7474267616785433, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 5.7478, 'eval_samples_per_second': 263.232, 'eval_steps_per_second': 33.056, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1406.9568, 'train_samples_per_second': 73.094, 'train_steps_per_second': 1.143, 'total_flos': 1.344041490771522e+16, 'train_loss': 0.009083204331057174, 'epoch': 12.0, 'step': 1608}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9907
  predict_f1                 =     0.7158
  predict_loss               =     0.0337
  predict_precision          =     0.6979
  predict_recall             =     0.7346
  predict_runtime            = 0:00:04.78
  predict_samples_per_second =     261.85
  predict_steps_per_second   =     32.836
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_101.json completed. F1: 0.7158119658119659
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_202.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3237.28 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4969.75 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5955.80 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6520.83 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6813.54 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 7134.18 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 7176.23 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7391.69 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6209.74 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7154.01 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4550.29 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7339.28 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4364.34 examples/s]
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0949, 'grad_norm': 0.27305543422698975, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.03918952867388725, 'eval_precision': 0.6294573643410852, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.641390205371248, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.6354, 'eval_samples_per_second': 326.402, 'eval_steps_per_second': 40.989, 'epoch': 1.0}
{'loss': 0.0322, 'grad_norm': 1.3128284215927124, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03742515668272972, 'eval_precision': 0.7189542483660131, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.713706407137064, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.4893, 'eval_samples_per_second': 337.022, 'eval_steps_per_second': 42.323, 'epoch': 2.0}
{'loss': 0.0209, 'grad_norm': 0.36480534076690674, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03523214906454086, 'eval_precision': 0.6971608832807571, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7043824701195219, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5139, 'eval_samples_per_second': 335.184, 'eval_steps_per_second': 42.092, 'epoch': 3.0}
{'loss': 0.0128, 'grad_norm': 0.3892824947834015, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.040041834115982056, 'eval_precision': 0.7427184466019418, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7409200968523002, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 4.5954, 'eval_samples_per_second': 329.241, 'eval_steps_per_second': 41.346, 'epoch': 4.0}
{'loss': 0.0084, 'grad_norm': 0.3893069624900818, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04297507181763649, 'eval_precision': 0.7415730337078652, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7427652733118972, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 4.537, 'eval_samples_per_second': 333.482, 'eval_steps_per_second': 41.878, 'epoch': 5.0}
{'loss': 0.0055, 'grad_norm': 0.5687837600708008, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04325814172625542, 'eval_precision': 0.7479935794542536, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7491961414790996, 'eval_accuracy': 0.9901512486809708, 'eval_runtime': 4.5958, 'eval_samples_per_second': 329.212, 'eval_steps_per_second': 41.342, 'epoch': 6.0}
{'loss': 0.0034, 'grad_norm': 0.40291547775268555, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04864608868956566, 'eval_precision': 0.7216981132075472, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.730310262529833, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 4.6111, 'eval_samples_per_second': 328.122, 'eval_steps_per_second': 41.205, 'epoch': 7.0}
{'loss': 0.0027, 'grad_norm': 0.14476965367794037, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.053649913519620895, 'eval_precision': 0.7198697068403909, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7157894736842106, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.5377, 'eval_samples_per_second': 333.432, 'eval_steps_per_second': 41.872, 'epoch': 8.0}
{'loss': 0.0018, 'grad_norm': 1.7033016681671143, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05743002891540527, 'eval_precision': 0.7332268370607029, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7361668003207698, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.5461, 'eval_samples_per_second': 332.814, 'eval_steps_per_second': 41.794, 'epoch': 9.0}
{'loss': 0.0013, 'grad_norm': 0.11828450858592987, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.055787187069654465, 'eval_precision': 0.7279874213836478, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7366746221161495, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 4.5577, 'eval_samples_per_second': 331.967, 'eval_steps_per_second': 41.688, 'epoch': 10.0}
{'loss': 0.0013, 'grad_norm': 0.020176274701952934, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05774694308638573, 'eval_precision': 0.7354838709677419, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7348912167606769, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.6923, 'eval_samples_per_second': 322.445, 'eval_steps_per_second': 40.492, 'epoch': 11.0}
{'loss': 0.0009, 'grad_norm': 0.007715651299804449, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.056887779384851456, 'eval_precision': 0.7241379310344828, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7339158061953932, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 4.5621, 'eval_samples_per_second': 331.647, 'eval_steps_per_second': 41.648, 'epoch': 12.0}
{'train_runtime': 1181.5958, 'train_samples_per_second': 87.035, 'train_steps_per_second': 2.722, 'train_loss': 0.015492585140155323, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0155
  train_runtime            = 0:19:41.59
  train_samples            =       8570
  train_samples_per_second =     87.035
  train_steps_per_second   =      2.722
[{'loss': 0.0949, 'grad_norm': 0.27305543422698975, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.03918952867388725, 'eval_precision': 0.6294573643410852, 'eval_recall': 0.6537842190016103, 'eval_f1': 0.641390205371248, 'eval_accuracy': 0.9874936491186931, 'eval_runtime': 4.6354, 'eval_samples_per_second': 326.402, 'eval_steps_per_second': 40.989, 'epoch': 1.0, 'step': 268}, {'loss': 0.0322, 'grad_norm': 1.3128284215927124, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03742515668272972, 'eval_precision': 0.7189542483660131, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.713706407137064, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.4893, 'eval_samples_per_second': 337.022, 'eval_steps_per_second': 42.323, 'epoch': 2.0, 'step': 536}, {'loss': 0.0209, 'grad_norm': 0.36480534076690674, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.03523214906454086, 'eval_precision': 0.6971608832807571, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7043824701195219, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.5139, 'eval_samples_per_second': 335.184, 'eval_steps_per_second': 42.092, 'epoch': 3.0, 'step': 804}, {'loss': 0.0128, 'grad_norm': 0.3892824947834015, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.040041834115982056, 'eval_precision': 0.7427184466019418, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7409200968523002, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 4.5954, 'eval_samples_per_second': 329.241, 'eval_steps_per_second': 41.346, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0084, 'grad_norm': 0.3893069624900818, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.04297507181763649, 'eval_precision': 0.7415730337078652, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7427652733118972, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 4.537, 'eval_samples_per_second': 333.482, 'eval_steps_per_second': 41.878, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0055, 'grad_norm': 0.5687837600708008, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.04325814172625542, 'eval_precision': 0.7479935794542536, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7491961414790996, 'eval_accuracy': 0.9901512486809708, 'eval_runtime': 4.5958, 'eval_samples_per_second': 329.212, 'eval_steps_per_second': 41.342, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0034, 'grad_norm': 0.40291547775268555, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.04864608868956566, 'eval_precision': 0.7216981132075472, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.730310262529833, 'eval_accuracy': 0.9896040958299136, 'eval_runtime': 4.6111, 'eval_samples_per_second': 328.122, 'eval_steps_per_second': 41.205, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0027, 'grad_norm': 0.14476965367794037, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.053649913519620895, 'eval_precision': 0.7198697068403909, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7157894736842106, 'eval_accuracy': 0.9892914370578809, 'eval_runtime': 4.5377, 'eval_samples_per_second': 333.432, 'eval_steps_per_second': 41.872, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0018, 'grad_norm': 1.7033016681671143, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05743002891540527, 'eval_precision': 0.7332268370607029, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7361668003207698, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.5461, 'eval_samples_per_second': 332.814, 'eval_steps_per_second': 41.794, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0013, 'grad_norm': 0.11828450858592987, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.055787187069654465, 'eval_precision': 0.7279874213836478, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7366746221161495, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 4.5577, 'eval_samples_per_second': 331.967, 'eval_steps_per_second': 41.688, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0013, 'grad_norm': 0.020176274701952934, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.05774694308638573, 'eval_precision': 0.7354838709677419, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7348912167606769, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 4.6923, 'eval_samples_per_second': 322.445, 'eval_steps_per_second': 40.492, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0009, 'grad_norm': 0.007715651299804449, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.056887779384851456, 'eval_precision': 0.7241379310344828, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7339158061953932, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 4.5621, 'eval_samples_per_second': 331.647, 'eval_steps_per_second': 41.648, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1181.5958, 'train_samples_per_second': 87.035, 'train_steps_per_second': 2.722, 'total_flos': 1.1358239156790372e+16, 'train_loss': 0.015492585140155323, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9906
  predict_f1                 =     0.7302
  predict_loss               =     0.0345
  predict_precision          =     0.7055
  predict_recall             =     0.7566
  predict_runtime            = 0:00:03.82
  predict_samples_per_second =    327.136
  predict_steps_per_second   =     41.023
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_202.json completed. F1: 0.7301587301587301
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_101.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2555.19 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 3961.63 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5150.61 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5724.78 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6240.90 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6695.09 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6705.21 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7026.07 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4981.25 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 5227.31 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4168.48 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7212.20 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4488.57 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0813, 'grad_norm': 0.28722670674324036, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04087582603096962, 'eval_precision': 0.6741573033707865, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6752411575562701, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6089, 'eval_samples_per_second': 328.281, 'eval_steps_per_second': 41.225, 'epoch': 1.0}
{'loss': 0.0279, 'grad_norm': 0.14270462095737457, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04103695973753929, 'eval_precision': 0.6009463722397477, 'eval_recall': 0.6135265700483091, 'eval_f1': 0.6071713147410359, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 4.6504, 'eval_samples_per_second': 325.35, 'eval_steps_per_second': 40.857, 'epoch': 2.0}
{'loss': 0.0167, 'grad_norm': 0.245930477976799, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04421672970056534, 'eval_precision': 0.6854961832061068, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7037617554858934, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.5038, 'eval_samples_per_second': 335.936, 'eval_steps_per_second': 42.186, 'epoch': 3.0}
{'loss': 0.0062, 'grad_norm': 0.4861241579055786, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04890953749418259, 'eval_precision': 0.7083333333333334, 'eval_recall': 0.7665056360708534, 'eval_f1': 0.7362722351121422, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 4.4934, 'eval_samples_per_second': 336.713, 'eval_steps_per_second': 42.284, 'epoch': 4.0}
{'loss': 0.0036, 'grad_norm': 0.1385360211133957, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05019959434866905, 'eval_precision': 0.7090620031796503, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7136, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.4946, 'eval_samples_per_second': 336.627, 'eval_steps_per_second': 42.273, 'epoch': 5.0}
{'loss': 0.0021, 'grad_norm': 0.6234351992607117, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.05727587640285492, 'eval_precision': 0.7302100161550888, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7290322580645161, 'eval_accuracy': 0.9892523547113768, 'eval_runtime': 4.518, 'eval_samples_per_second': 334.879, 'eval_steps_per_second': 42.054, 'epoch': 6.0}
{'loss': 0.0013, 'grad_norm': 0.1046440601348877, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06211516261100769, 'eval_precision': 0.6821705426356589, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6951026856240128, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.5132, 'eval_samples_per_second': 335.239, 'eval_steps_per_second': 42.099, 'epoch': 7.0}
{'loss': 0.0006, 'grad_norm': 0.09790343046188354, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.0656609758734703, 'eval_precision': 0.7325949367088608, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7390263367916999, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5369, 'eval_samples_per_second': 333.487, 'eval_steps_per_second': 41.879, 'epoch': 8.0}
{'loss': 0.0004, 'grad_norm': 0.008606241084635258, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06452496349811554, 'eval_precision': 0.7310126582278481, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7374301675977654, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.5401, 'eval_samples_per_second': 333.249, 'eval_steps_per_second': 41.849, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.04645072668790817, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06791480630636215, 'eval_precision': 0.7436305732484076, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7477982385908728, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.7631, 'eval_samples_per_second': 317.647, 'eval_steps_per_second': 39.89, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.009299415163695812, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06930079311132431, 'eval_precision': 0.7222222222222222, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.727418065547562, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5789, 'eval_samples_per_second': 330.43, 'eval_steps_per_second': 41.495, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.03498278185725212, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06938286125659943, 'eval_precision': 0.7371794871794872, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7389558232931728, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.608, 'eval_samples_per_second': 328.339, 'eval_steps_per_second': 41.232, 'epoch': 12.0}
{'train_runtime': 1134.0933, 'train_samples_per_second': 90.68, 'train_steps_per_second': 1.418, 'train_loss': 0.011712835488878005, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0117
  train_runtime            = 0:18:54.09
  train_samples            =       8570
  train_samples_per_second =      90.68
  train_steps_per_second   =      1.418
[{'loss': 0.0813, 'grad_norm': 0.28722670674324036, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04087582603096962, 'eval_precision': 0.6741573033707865, 'eval_recall': 0.6763285024154589, 'eval_f1': 0.6752411575562701, 'eval_accuracy': 0.9882752960487747, 'eval_runtime': 4.6089, 'eval_samples_per_second': 328.281, 'eval_steps_per_second': 41.225, 'epoch': 1.0, 'step': 134}, {'loss': 0.0279, 'grad_norm': 0.14270462095737457, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04103695973753929, 'eval_precision': 0.6009463722397477, 'eval_recall': 0.6135265700483091, 'eval_f1': 0.6071713147410359, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 4.6504, 'eval_samples_per_second': 325.35, 'eval_steps_per_second': 40.857, 'epoch': 2.0, 'step': 268}, {'loss': 0.0167, 'grad_norm': 0.245930477976799, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04421672970056534, 'eval_precision': 0.6854961832061068, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7037617554858934, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.5038, 'eval_samples_per_second': 335.936, 'eval_steps_per_second': 42.186, 'epoch': 3.0, 'step': 402}, {'loss': 0.0062, 'grad_norm': 0.4861241579055786, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.04890953749418259, 'eval_precision': 0.7083333333333334, 'eval_recall': 0.7665056360708534, 'eval_f1': 0.7362722351121422, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 4.4934, 'eval_samples_per_second': 336.713, 'eval_steps_per_second': 42.284, 'epoch': 4.0, 'step': 536}, {'loss': 0.0036, 'grad_norm': 0.1385360211133957, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05019959434866905, 'eval_precision': 0.7090620031796503, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7136, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.4946, 'eval_samples_per_second': 336.627, 'eval_steps_per_second': 42.273, 'epoch': 5.0, 'step': 670}, {'loss': 0.0021, 'grad_norm': 0.6234351992607117, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.05727587640285492, 'eval_precision': 0.7302100161550888, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7290322580645161, 'eval_accuracy': 0.9892523547113768, 'eval_runtime': 4.518, 'eval_samples_per_second': 334.879, 'eval_steps_per_second': 42.054, 'epoch': 6.0, 'step': 804}, {'loss': 0.0013, 'grad_norm': 0.1046440601348877, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06211516261100769, 'eval_precision': 0.6821705426356589, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6951026856240128, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 4.5132, 'eval_samples_per_second': 335.239, 'eval_steps_per_second': 42.099, 'epoch': 7.0, 'step': 938}, {'loss': 0.0006, 'grad_norm': 0.09790343046188354, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.0656609758734703, 'eval_precision': 0.7325949367088608, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7390263367916999, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5369, 'eval_samples_per_second': 333.487, 'eval_steps_per_second': 41.879, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0004, 'grad_norm': 0.008606241084635258, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06452496349811554, 'eval_precision': 0.7310126582278481, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7374301675977654, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.5401, 'eval_samples_per_second': 333.249, 'eval_steps_per_second': 41.849, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0002, 'grad_norm': 0.04645072668790817, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06791480630636215, 'eval_precision': 0.7436305732484076, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7477982385908728, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 4.7631, 'eval_samples_per_second': 317.647, 'eval_steps_per_second': 39.89, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0001, 'grad_norm': 0.009299415163695812, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06930079311132431, 'eval_precision': 0.7222222222222222, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.727418065547562, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5789, 'eval_samples_per_second': 330.43, 'eval_steps_per_second': 41.495, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0002, 'grad_norm': 0.03498278185725212, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06938286125659943, 'eval_precision': 0.7371794871794872, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7389558232931728, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.608, 'eval_samples_per_second': 328.339, 'eval_steps_per_second': 41.232, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1134.0933, 'train_samples_per_second': 90.68, 'train_steps_per_second': 1.418, 'total_flos': 1.2912881111013576e+16, 'train_loss': 0.011712835488878005, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9895
  predict_f1                 =     0.6978
  predict_loss               =     0.0382
  predict_precision          =     0.6918
  predict_recall             =     0.7039
  predict_runtime            = 0:00:03.92
  predict_samples_per_second =    318.647
  predict_steps_per_second   =     39.958
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_101.json completed. F1: 0.6978260869565217
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_303.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2671.70 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 3358.75 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4404.31 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5091.73 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5749.82 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6338.05 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6384.44 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6797.18 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4749.53 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7276.64 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5983.19 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6464.93 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5278.63 examples/s]
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0638, 'grad_norm': 0.2849491238594055, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.041023481637239456, 'eval_precision': 0.6224489795918368, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6534047436878346, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 4.8239, 'eval_samples_per_second': 313.644, 'eval_steps_per_second': 39.387, 'epoch': 1.0}
{'loss': 0.0274, 'grad_norm': 0.47136127948760986, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.039748117327690125, 'eval_precision': 0.665680473372781, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.6939090208172706, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.4948, 'eval_samples_per_second': 336.613, 'eval_steps_per_second': 42.271, 'epoch': 2.0}
{'loss': 0.0149, 'grad_norm': 0.6562423706054688, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04127633199095726, 'eval_precision': 0.6367583212735166, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6707317073170732, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.4959, 'eval_samples_per_second': 336.527, 'eval_steps_per_second': 42.261, 'epoch': 3.0}
{'loss': 0.0071, 'grad_norm': 0.45729774236679077, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04908398538827896, 'eval_precision': 0.6945288753799392, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7146207974980453, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5532, 'eval_samples_per_second': 332.295, 'eval_steps_per_second': 41.729, 'epoch': 4.0}
{'loss': 0.0045, 'grad_norm': 0.047850899398326874, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.051830194890499115, 'eval_precision': 0.714968152866242, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7189751801441153, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.5292, 'eval_samples_per_second': 334.058, 'eval_steps_per_second': 41.95, 'epoch': 5.0}
{'loss': 0.0027, 'grad_norm': 0.3103863000869751, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.060327835381031036, 'eval_precision': 0.707667731629393, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.710505212510024, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.4894, 'eval_samples_per_second': 337.012, 'eval_steps_per_second': 42.321, 'epoch': 6.0}
{'loss': 0.0012, 'grad_norm': 0.02817491441965103, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.05831974372267723, 'eval_precision': 0.6897637795275591, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.697452229299363, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.5332, 'eval_samples_per_second': 333.758, 'eval_steps_per_second': 41.913, 'epoch': 7.0}
{'loss': 0.0006, 'grad_norm': 0.026143770664930344, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06084882840514183, 'eval_precision': 0.721875, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.732751784298176, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.4953, 'eval_samples_per_second': 336.573, 'eval_steps_per_second': 42.266, 'epoch': 8.0}
{'loss': 0.0003, 'grad_norm': 0.040057696402072906, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06565054506063461, 'eval_precision': 0.7229299363057324, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7269815852682145, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.5284, 'eval_samples_per_second': 334.112, 'eval_steps_per_second': 41.957, 'epoch': 9.0}
{'loss': 0.0002, 'grad_norm': 0.0004686684987973422, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06622108817100525, 'eval_precision': 0.7304625199362041, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7339743589743589, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.4977, 'eval_samples_per_second': 336.395, 'eval_steps_per_second': 42.244, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.26977163553237915, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06755930930376053, 'eval_precision': 0.721259842519685, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7292993630573248, 'eval_accuracy': 0.9887833665533279, 'eval_runtime': 4.4867, 'eval_samples_per_second': 337.221, 'eval_steps_per_second': 42.348, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.002446176251396537, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06784163415431976, 'eval_precision': 0.7176656151419558, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7250996015936255, 'eval_accuracy': 0.988822448899832, 'eval_runtime': 4.5176, 'eval_samples_per_second': 334.914, 'eval_steps_per_second': 42.058, 'epoch': 12.0}
{'train_runtime': 1127.589, 'train_samples_per_second': 91.203, 'train_steps_per_second': 2.852, 'train_loss': 0.010237491872643506, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0102
  train_runtime            = 0:18:47.58
  train_samples            =       8570
  train_samples_per_second =     91.203
  train_steps_per_second   =      2.852
[{'loss': 0.0638, 'grad_norm': 0.2849491238594055, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.041023481637239456, 'eval_precision': 0.6224489795918368, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6534047436878346, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 4.8239, 'eval_samples_per_second': 313.644, 'eval_steps_per_second': 39.387, 'epoch': 1.0, 'step': 268}, {'loss': 0.0274, 'grad_norm': 0.47136127948760986, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.039748117327690125, 'eval_precision': 0.665680473372781, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.6939090208172706, 'eval_accuracy': 0.9880798843162544, 'eval_runtime': 4.4948, 'eval_samples_per_second': 336.613, 'eval_steps_per_second': 42.271, 'epoch': 2.0, 'step': 536}, {'loss': 0.0149, 'grad_norm': 0.6562423706054688, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04127633199095726, 'eval_precision': 0.6367583212735166, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6707317073170732, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 4.4959, 'eval_samples_per_second': 336.527, 'eval_steps_per_second': 42.261, 'epoch': 3.0, 'step': 804}, {'loss': 0.0071, 'grad_norm': 0.45729774236679077, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04908398538827896, 'eval_precision': 0.6945288753799392, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7146207974980453, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5532, 'eval_samples_per_second': 332.295, 'eval_steps_per_second': 41.729, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0045, 'grad_norm': 0.047850899398326874, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.051830194890499115, 'eval_precision': 0.714968152866242, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7189751801441153, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.5292, 'eval_samples_per_second': 334.058, 'eval_steps_per_second': 41.95, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0027, 'grad_norm': 0.3103863000869751, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.060327835381031036, 'eval_precision': 0.707667731629393, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.710505212510024, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.4894, 'eval_samples_per_second': 337.012, 'eval_steps_per_second': 42.321, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0012, 'grad_norm': 0.02817491441965103, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05831974372267723, 'eval_precision': 0.6897637795275591, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.697452229299363, 'eval_accuracy': 0.9881971313557666, 'eval_runtime': 4.5332, 'eval_samples_per_second': 333.758, 'eval_steps_per_second': 41.913, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0006, 'grad_norm': 0.026143770664930344, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.06084882840514183, 'eval_precision': 0.721875, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.732751784298176, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 4.4953, 'eval_samples_per_second': 336.573, 'eval_steps_per_second': 42.266, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0003, 'grad_norm': 0.040057696402072906, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06565054506063461, 'eval_precision': 0.7229299363057324, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7269815852682145, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.5284, 'eval_samples_per_second': 334.112, 'eval_steps_per_second': 41.957, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0002, 'grad_norm': 0.0004686684987973422, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06622108817100525, 'eval_precision': 0.7304625199362041, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7339743589743589, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.4977, 'eval_samples_per_second': 336.395, 'eval_steps_per_second': 42.244, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0002, 'grad_norm': 0.26977163553237915, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06755930930376053, 'eval_precision': 0.721259842519685, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7292993630573248, 'eval_accuracy': 0.9887833665533279, 'eval_runtime': 4.4867, 'eval_samples_per_second': 337.221, 'eval_steps_per_second': 42.348, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0001, 'grad_norm': 0.002446176251396537, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06784163415431976, 'eval_precision': 0.7176656151419558, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7250996015936255, 'eval_accuracy': 0.988822448899832, 'eval_runtime': 4.5176, 'eval_samples_per_second': 334.914, 'eval_steps_per_second': 42.058, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1127.589, 'train_samples_per_second': 91.203, 'train_steps_per_second': 2.852, 'total_flos': 1.1349764561823204e+16, 'train_loss': 0.010237491872643506, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9898
  predict_f1                 =     0.7238
  predict_loss               =     0.0369
  predict_precision          =      0.692
  predict_recall             =     0.7588
  predict_runtime            = 0:00:03.84
  predict_samples_per_second =    325.853
  predict_steps_per_second   =     40.862
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_04_303.json completed. F1: 0.7238493723849372
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_101.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 6187.49 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 6433.12 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5286.31 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5962.60 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6369.77 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6785.85 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6911.64 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7147.80 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6184.97 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 4947.89 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4248.35 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3674.09 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6557.35 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4018.45 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1236, 'grad_norm': 0.49876028299331665, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04614397883415222, 'eval_precision': 0.5760233918128655, 'eval_recall': 0.6344605475040258, 'eval_f1': 0.6038314176245212, 'eval_accuracy': 0.9844452260913745, 'eval_runtime': 2.337, 'eval_samples_per_second': 647.408, 'eval_steps_per_second': 81.3, 'epoch': 1.0}
{'loss': 0.0379, 'grad_norm': 0.3783142864704132, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.04379190877079964, 'eval_precision': 0.6177777777777778, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6435185185185185, 'eval_accuracy': 0.9849532965959277, 'eval_runtime': 2.2499, 'eval_samples_per_second': 672.473, 'eval_steps_per_second': 84.448, 'epoch': 2.0}
{'loss': 0.0291, 'grad_norm': 0.2651391923427582, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.040810853242874146, 'eval_precision': 0.6785185185185185, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7067901234567902, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.3332, 'eval_samples_per_second': 648.471, 'eval_steps_per_second': 81.434, 'epoch': 3.0}
{'loss': 0.0219, 'grad_norm': 0.6229397058486938, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04283617064356804, 'eval_precision': 0.6946564885496184, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7131661442006271, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 2.2459, 'eval_samples_per_second': 673.675, 'eval_steps_per_second': 84.599, 'epoch': 4.0}
{'loss': 0.0164, 'grad_norm': 0.6456557512283325, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.0488956943154335, 'eval_precision': 0.6387665198237885, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6682027649769586, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.2429, 'eval_samples_per_second': 674.569, 'eval_steps_per_second': 84.711, 'epoch': 5.0}
{'loss': 0.0128, 'grad_norm': 1.621059775352478, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.052863918244838715, 'eval_precision': 0.6692546583850931, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6814229249011857, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.2631, 'eval_samples_per_second': 668.547, 'eval_steps_per_second': 83.955, 'epoch': 6.0}
{'loss': 0.0097, 'grad_norm': 0.27512913942337036, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.051147107034921646, 'eval_precision': 0.6934984520123839, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7071823204419889, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 2.6313, 'eval_samples_per_second': 574.992, 'eval_steps_per_second': 72.206, 'epoch': 7.0}
{'loss': 0.0075, 'grad_norm': 0.182829812169075, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05365139618515968, 'eval_precision': 0.6939721792890263, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7082018927444795, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.2936, 'eval_samples_per_second': 659.66, 'eval_steps_per_second': 82.839, 'epoch': 8.0}
{'loss': 0.0061, 'grad_norm': 0.7661235928535461, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.055804718285799026, 'eval_precision': 0.6846153846153846, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7002360346184108, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.4893, 'eval_samples_per_second': 607.793, 'eval_steps_per_second': 76.326, 'epoch': 9.0}
{'loss': 0.0048, 'grad_norm': 0.3249812722206116, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.057704515755176544, 'eval_precision': 0.6949152542372882, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.710236220472441, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.3539, 'eval_samples_per_second': 642.755, 'eval_steps_per_second': 80.716, 'epoch': 10.0}
{'loss': 0.0041, 'grad_norm': 0.07764046639204025, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05807819217443466, 'eval_precision': 0.7013996889580093, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7136075949367089, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.362, 'eval_samples_per_second': 640.565, 'eval_steps_per_second': 80.441, 'epoch': 11.0}
{'loss': 0.0035, 'grad_norm': 0.5402393937110901, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0590621754527092, 'eval_precision': 0.6953846153846154, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7112509834775768, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 2.367, 'eval_samples_per_second': 639.218, 'eval_steps_per_second': 80.272, 'epoch': 12.0}
{'train_runtime': 498.2903, 'train_samples_per_second': 206.386, 'train_steps_per_second': 6.454, 'train_loss': 0.02311162854456783, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0231
  train_runtime            = 0:08:18.29
  train_samples            =       8570
  train_samples_per_second =    206.386
  train_steps_per_second   =      6.454
[{'loss': 0.1236, 'grad_norm': 0.49876028299331665, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04614397883415222, 'eval_precision': 0.5760233918128655, 'eval_recall': 0.6344605475040258, 'eval_f1': 0.6038314176245212, 'eval_accuracy': 0.9844452260913745, 'eval_runtime': 2.337, 'eval_samples_per_second': 647.408, 'eval_steps_per_second': 81.3, 'epoch': 1.0, 'step': 268}, {'loss': 0.0379, 'grad_norm': 0.3783142864704132, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04379190877079964, 'eval_precision': 0.6177777777777778, 'eval_recall': 0.6714975845410628, 'eval_f1': 0.6435185185185185, 'eval_accuracy': 0.9849532965959277, 'eval_runtime': 2.2499, 'eval_samples_per_second': 672.473, 'eval_steps_per_second': 84.448, 'epoch': 2.0, 'step': 536}, {'loss': 0.0291, 'grad_norm': 0.2651391923427582, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.040810853242874146, 'eval_precision': 0.6785185185185185, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7067901234567902, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.3332, 'eval_samples_per_second': 648.471, 'eval_steps_per_second': 81.434, 'epoch': 3.0, 'step': 804}, {'loss': 0.0219, 'grad_norm': 0.6229397058486938, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04283617064356804, 'eval_precision': 0.6946564885496184, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7131661442006271, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 2.2459, 'eval_samples_per_second': 673.675, 'eval_steps_per_second': 84.599, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0164, 'grad_norm': 0.6456557512283325, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.0488956943154335, 'eval_precision': 0.6387665198237885, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6682027649769586, 'eval_accuracy': 0.9858521905655215, 'eval_runtime': 2.2429, 'eval_samples_per_second': 674.569, 'eval_steps_per_second': 84.711, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0128, 'grad_norm': 1.621059775352478, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.052863918244838715, 'eval_precision': 0.6692546583850931, 'eval_recall': 0.6940418679549114, 'eval_f1': 0.6814229249011857, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.2631, 'eval_samples_per_second': 668.547, 'eval_steps_per_second': 83.955, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0097, 'grad_norm': 0.27512913942337036, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.051147107034921646, 'eval_precision': 0.6934984520123839, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7071823204419889, 'eval_accuracy': 0.9878844725837339, 'eval_runtime': 2.6313, 'eval_samples_per_second': 574.992, 'eval_steps_per_second': 72.206, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0075, 'grad_norm': 0.182829812169075, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05365139618515968, 'eval_precision': 0.6939721792890263, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7082018927444795, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 2.2936, 'eval_samples_per_second': 659.66, 'eval_steps_per_second': 82.839, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0061, 'grad_norm': 0.7661235928535461, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.055804718285799026, 'eval_precision': 0.6846153846153846, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7002360346184108, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.4893, 'eval_samples_per_second': 607.793, 'eval_steps_per_second': 76.326, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0048, 'grad_norm': 0.3249812722206116, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.057704515755176544, 'eval_precision': 0.6949152542372882, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.710236220472441, 'eval_accuracy': 0.9876108961582053, 'eval_runtime': 2.3539, 'eval_samples_per_second': 642.755, 'eval_steps_per_second': 80.716, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0041, 'grad_norm': 0.07764046639204025, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.05807819217443466, 'eval_precision': 0.7013996889580093, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7136075949367089, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.362, 'eval_samples_per_second': 640.565, 'eval_steps_per_second': 80.441, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0035, 'grad_norm': 0.5402393937110901, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.0590621754527092, 'eval_precision': 0.6953846153846154, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7112509834775768, 'eval_accuracy': 0.9877281431977176, 'eval_runtime': 2.367, 'eval_samples_per_second': 639.218, 'eval_steps_per_second': 80.272, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 498.2903, 'train_samples_per_second': 206.386, 'train_steps_per_second': 6.454, 'total_flos': 3923375652683916.0, 'train_loss': 0.02311162854456783, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9896
  predict_f1                 =     0.7267
  predict_loss               =     0.0375
  predict_precision          =     0.7029
  predict_recall             =     0.7522
  predict_runtime            = 0:00:01.91
  predict_samples_per_second =    655.382
  predict_steps_per_second   =     82.185
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_101.json completed. F1: 0.7266949152542372
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_202.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3059.41 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4379.03 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5321.57 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5755.96 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6244.32 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6695.07 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6773.43 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7062.80 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5481.74 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7356.40 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3992.88 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7183.61 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3426.02 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0956, 'grad_norm': 0.4153391420841217, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04055071622133255, 'eval_precision': 0.5904334828101644, 'eval_recall': 0.6360708534621579, 'eval_f1': 0.6124031007751938, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 5.8218, 'eval_samples_per_second': 259.887, 'eval_steps_per_second': 32.636, 'epoch': 1.0}
{'loss': 0.0287, 'grad_norm': 1.169036626815796, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03663450479507446, 'eval_precision': 0.7043343653250774, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7182320441988951, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.4991, 'eval_samples_per_second': 275.134, 'eval_steps_per_second': 34.551, 'epoch': 2.0}
{'loss': 0.0169, 'grad_norm': 0.16839373111724854, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.039321642369031906, 'eval_precision': 0.6630602782071098, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6766561514195584, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 5.4962, 'eval_samples_per_second': 275.279, 'eval_steps_per_second': 34.569, 'epoch': 3.0}
{'loss': 0.0082, 'grad_norm': 0.48406702280044556, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.044716477394104004, 'eval_precision': 0.719626168224299, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7315914489311164, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.5602, 'eval_samples_per_second': 272.111, 'eval_steps_per_second': 34.171, 'epoch': 4.0}
{'loss': 0.0046, 'grad_norm': 0.3324970006942749, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.046404510736465454, 'eval_precision': 0.7317073170731707, 'eval_recall': 0.7729468599033816, 'eval_f1': 0.7517619420516836, 'eval_accuracy': 0.9903857427599954, 'eval_runtime': 5.5618, 'eval_samples_per_second': 272.033, 'eval_steps_per_second': 34.161, 'epoch': 5.0}
{'loss': 0.0025, 'grad_norm': 0.5688671469688416, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.0497339703142643, 'eval_precision': 0.7247278382581649, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7373417721518987, 'eval_accuracy': 0.9901121663344667, 'eval_runtime': 5.548, 'eval_samples_per_second': 272.709, 'eval_steps_per_second': 34.246, 'epoch': 6.0}
{'loss': 0.0013, 'grad_norm': 0.0884019136428833, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05614420026540756, 'eval_precision': 0.725, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7359238699444886, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.8368, 'eval_samples_per_second': 259.216, 'eval_steps_per_second': 32.552, 'epoch': 7.0}
{'loss': 0.0009, 'grad_norm': 0.1052107885479927, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.056348156183958054, 'eval_precision': 0.750788643533123, 'eval_recall': 0.7665056360708534, 'eval_f1': 0.7585657370517928, 'eval_accuracy': 0.9904639074530035, 'eval_runtime': 5.5394, 'eval_samples_per_second': 273.136, 'eval_steps_per_second': 34.3, 'epoch': 8.0}
{'loss': 0.0007, 'grad_norm': 0.07061365246772766, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05819448456168175, 'eval_precision': 0.7245696400625978, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.734920634920635, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.4848, 'eval_samples_per_second': 275.853, 'eval_steps_per_second': 34.641, 'epoch': 9.0}
{'loss': 0.0004, 'grad_norm': 0.13815636932849884, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06107586622238159, 'eval_precision': 0.7380952380952381, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7434052757793764, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 5.5182, 'eval_samples_per_second': 274.185, 'eval_steps_per_second': 34.432, 'epoch': 10.0}
{'loss': 0.0003, 'grad_norm': 0.010932552628219128, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.061371758580207825, 'eval_precision': 0.7351097178683386, 'eval_recall': 0.7552334943639292, 'eval_f1': 0.7450357426528992, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 5.6272, 'eval_samples_per_second': 268.872, 'eval_steps_per_second': 33.765, 'epoch': 11.0}
{'loss': 0.0003, 'grad_norm': 0.2877447009086609, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0616251565515995, 'eval_precision': 0.7398119122257053, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7498014297061159, 'eval_accuracy': 0.990268495720483, 'eval_runtime': 5.6026, 'eval_samples_per_second': 270.053, 'eval_steps_per_second': 33.913, 'epoch': 12.0}
{'train_runtime': 1355.3154, 'train_samples_per_second': 75.879, 'train_steps_per_second': 2.373, 'train_loss': 0.013371683344411762, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0134
  train_runtime            = 0:22:35.31
  train_samples            =       8570
  train_samples_per_second =     75.879
  train_steps_per_second   =      2.373
[{'loss': 0.0956, 'grad_norm': 0.4153391420841217, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04055071622133255, 'eval_precision': 0.5904334828101644, 'eval_recall': 0.6360708534621579, 'eval_f1': 0.6124031007751938, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 5.8218, 'eval_samples_per_second': 259.887, 'eval_steps_per_second': 32.636, 'epoch': 1.0, 'step': 268}, {'loss': 0.0287, 'grad_norm': 1.169036626815796, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03663450479507446, 'eval_precision': 0.7043343653250774, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7182320441988951, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 5.4991, 'eval_samples_per_second': 275.134, 'eval_steps_per_second': 34.551, 'epoch': 2.0, 'step': 536}, {'loss': 0.0169, 'grad_norm': 0.16839373111724854, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.039321642369031906, 'eval_precision': 0.6630602782071098, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6766561514195584, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 5.4962, 'eval_samples_per_second': 275.279, 'eval_steps_per_second': 34.569, 'epoch': 3.0, 'step': 804}, {'loss': 0.0082, 'grad_norm': 0.48406702280044556, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.044716477394104004, 'eval_precision': 0.719626168224299, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7315914489311164, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.5602, 'eval_samples_per_second': 272.111, 'eval_steps_per_second': 34.171, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0046, 'grad_norm': 0.3324970006942749, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.046404510736465454, 'eval_precision': 0.7317073170731707, 'eval_recall': 0.7729468599033816, 'eval_f1': 0.7517619420516836, 'eval_accuracy': 0.9903857427599954, 'eval_runtime': 5.5618, 'eval_samples_per_second': 272.033, 'eval_steps_per_second': 34.161, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0025, 'grad_norm': 0.5688671469688416, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.0497339703142643, 'eval_precision': 0.7247278382581649, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7373417721518987, 'eval_accuracy': 0.9901121663344667, 'eval_runtime': 5.548, 'eval_samples_per_second': 272.709, 'eval_steps_per_second': 34.246, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0013, 'grad_norm': 0.0884019136428833, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05614420026540756, 'eval_precision': 0.725, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7359238699444886, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.8368, 'eval_samples_per_second': 259.216, 'eval_steps_per_second': 32.552, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0009, 'grad_norm': 0.1052107885479927, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.056348156183958054, 'eval_precision': 0.750788643533123, 'eval_recall': 0.7665056360708534, 'eval_f1': 0.7585657370517928, 'eval_accuracy': 0.9904639074530035, 'eval_runtime': 5.5394, 'eval_samples_per_second': 273.136, 'eval_steps_per_second': 34.3, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0007, 'grad_norm': 0.07061365246772766, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05819448456168175, 'eval_precision': 0.7245696400625978, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.734920634920635, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.4848, 'eval_samples_per_second': 275.853, 'eval_steps_per_second': 34.641, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0004, 'grad_norm': 0.13815636932849884, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06107586622238159, 'eval_precision': 0.7380952380952381, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7434052757793764, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 5.5182, 'eval_samples_per_second': 274.185, 'eval_steps_per_second': 34.432, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0003, 'grad_norm': 0.010932552628219128, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.061371758580207825, 'eval_precision': 0.7351097178683386, 'eval_recall': 0.7552334943639292, 'eval_f1': 0.7450357426528992, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 5.6272, 'eval_samples_per_second': 268.872, 'eval_steps_per_second': 33.765, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0003, 'grad_norm': 0.2877447009086609, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.0616251565515995, 'eval_precision': 0.7398119122257053, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7498014297061159, 'eval_accuracy': 0.990268495720483, 'eval_runtime': 5.6026, 'eval_samples_per_second': 270.053, 'eval_steps_per_second': 33.913, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1355.3154, 'train_samples_per_second': 75.879, 'train_steps_per_second': 2.373, 'total_flos': 1.177774843518018e+16, 'train_loss': 0.013371683344411762, 'epoch': 12.0, 'step': 3216}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9903
  predict_f1                 =     0.7163
  predict_loss               =     0.0342
  predict_precision          =     0.6948
  predict_recall             =      0.739
  predict_runtime            = 0:00:04.63
  predict_samples_per_second =    270.382
  predict_steps_per_second   =     33.906
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_202.json completed. F1: 0.716259298618491
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_303.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:06, 1254.40 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:01<00:03, 2075.27 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:01, 3043.22 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 3975.49 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 4756.65 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5472.34 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 5850.30 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6334.43 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:02<00:00, 4231.67 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7052.68 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5049.67 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7284.32 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4204.81 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-804 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0908, 'grad_norm': 0.2996419370174408, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04462859034538269, 'eval_precision': 0.5628654970760234, 'eval_recall': 0.6199677938808373, 'eval_f1': 0.5900383141762452, 'eval_accuracy': 0.9849923789424317, 'eval_runtime': 2.3454, 'eval_samples_per_second': 645.094, 'eval_steps_per_second': 81.01, 'epoch': 1.0}
{'loss': 0.034, 'grad_norm': 0.2395641952753067, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04342686012387276, 'eval_precision': 0.6797488226059655, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6883942766295706, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3289, 'eval_samples_per_second': 649.652, 'eval_steps_per_second': 81.582, 'epoch': 2.0}
{'loss': 0.0215, 'grad_norm': 0.27200621366500854, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.043692197650671005, 'eval_precision': 0.6656346749226006, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6787687450670875, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 2.2693, 'eval_samples_per_second': 666.729, 'eval_steps_per_second': 83.727, 'epoch': 3.0}
{'loss': 0.0118, 'grad_norm': 1.531266212463379, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.047675274312496185, 'eval_precision': 0.6966824644549763, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7033492822966507, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 2.3259, 'eval_samples_per_second': 650.508, 'eval_steps_per_second': 81.69, 'epoch': 4.0}
{'loss': 0.0068, 'grad_norm': 0.19148950278759003, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.054996274411678314, 'eval_precision': 0.669218989280245, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6860282574568288, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 2.2359, 'eval_samples_per_second': 676.682, 'eval_steps_per_second': 84.977, 'epoch': 5.0}
{'loss': 0.0037, 'grad_norm': 0.09202093631029129, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.060659829527139664, 'eval_precision': 0.6982343499197432, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6993569131832797, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 2.2479, 'eval_samples_per_second': 673.058, 'eval_steps_per_second': 84.522, 'epoch': 6.0}
{'loss': 0.0019, 'grad_norm': 0.11630242317914963, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.06955768167972565, 'eval_precision': 0.6768759571209801, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6938775510204083, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.3147, 'eval_samples_per_second': 653.659, 'eval_steps_per_second': 82.085, 'epoch': 7.0}
{'loss': 0.0011, 'grad_norm': 0.040480393916368484, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07137227803468704, 'eval_precision': 0.7071197411003236, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.705407586763519, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 2.2466, 'eval_samples_per_second': 673.472, 'eval_steps_per_second': 84.574, 'epoch': 8.0}
{'loss': 0.0008, 'grad_norm': 0.00621469970792532, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07337939739227295, 'eval_precision': 0.7072784810126582, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.713487629688747, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 2.2566, 'eval_samples_per_second': 670.48, 'eval_steps_per_second': 84.198, 'epoch': 9.0}
{'loss': 0.0006, 'grad_norm': 0.04924822971224785, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.07588604092597961, 'eval_precision': 0.7011128775834659, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7056, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 2.2603, 'eval_samples_per_second': 669.372, 'eval_steps_per_second': 84.059, 'epoch': 10.0}
{'loss': 0.0004, 'grad_norm': 0.03942766785621643, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.07732532918453217, 'eval_precision': 0.69826224328594, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7049441786283892, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 2.3029, 'eval_samples_per_second': 657.006, 'eval_steps_per_second': 82.506, 'epoch': 11.0}
{'loss': 0.0004, 'grad_norm': 0.04579353705048561, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.07812540978193283, 'eval_precision': 0.7031746031746032, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7082334132693845, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.3243, 'eval_samples_per_second': 650.96, 'eval_steps_per_second': 81.746, 'epoch': 12.0}
{'train_runtime': 560.1185, 'train_samples_per_second': 183.604, 'train_steps_per_second': 2.871, 'train_loss': 0.014485419382670181, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0145
  train_runtime            = 0:09:20.11
  train_samples            =       8570
  train_samples_per_second =    183.604
  train_steps_per_second   =      2.871
[{'loss': 0.0908, 'grad_norm': 0.2996419370174408, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04462859034538269, 'eval_precision': 0.5628654970760234, 'eval_recall': 0.6199677938808373, 'eval_f1': 0.5900383141762452, 'eval_accuracy': 0.9849923789424317, 'eval_runtime': 2.3454, 'eval_samples_per_second': 645.094, 'eval_steps_per_second': 81.01, 'epoch': 1.0, 'step': 134}, {'loss': 0.034, 'grad_norm': 0.2395641952753067, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04342686012387276, 'eval_precision': 0.6797488226059655, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6883942766295706, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3289, 'eval_samples_per_second': 649.652, 'eval_steps_per_second': 81.582, 'epoch': 2.0, 'step': 268}, {'loss': 0.0215, 'grad_norm': 0.27200621366500854, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.043692197650671005, 'eval_precision': 0.6656346749226006, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6787687450670875, 'eval_accuracy': 0.98698557861414, 'eval_runtime': 2.2693, 'eval_samples_per_second': 666.729, 'eval_steps_per_second': 83.727, 'epoch': 3.0, 'step': 402}, {'loss': 0.0118, 'grad_norm': 1.531266212463379, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.047675274312496185, 'eval_precision': 0.6966824644549763, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7033492822966507, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 2.3259, 'eval_samples_per_second': 650.508, 'eval_steps_per_second': 81.69, 'epoch': 4.0, 'step': 536}, {'loss': 0.0068, 'grad_norm': 0.19148950278759003, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.054996274411678314, 'eval_precision': 0.669218989280245, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6860282574568288, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 2.2359, 'eval_samples_per_second': 676.682, 'eval_steps_per_second': 84.977, 'epoch': 5.0, 'step': 670}, {'loss': 0.0037, 'grad_norm': 0.09202093631029129, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.060659829527139664, 'eval_precision': 0.6982343499197432, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6993569131832797, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 2.2479, 'eval_samples_per_second': 673.058, 'eval_steps_per_second': 84.522, 'epoch': 6.0, 'step': 804}, {'loss': 0.0019, 'grad_norm': 0.11630242317914963, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.06955768167972565, 'eval_precision': 0.6768759571209801, 'eval_recall': 0.711755233494364, 'eval_f1': 0.6938775510204083, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.3147, 'eval_samples_per_second': 653.659, 'eval_steps_per_second': 82.085, 'epoch': 7.0, 'step': 938}, {'loss': 0.0011, 'grad_norm': 0.040480393916368484, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.07137227803468704, 'eval_precision': 0.7071197411003236, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.705407586763519, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 2.2466, 'eval_samples_per_second': 673.472, 'eval_steps_per_second': 84.574, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0008, 'grad_norm': 0.00621469970792532, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.07337939739227295, 'eval_precision': 0.7072784810126582, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.713487629688747, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 2.2566, 'eval_samples_per_second': 670.48, 'eval_steps_per_second': 84.198, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0006, 'grad_norm': 0.04924822971224785, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.07588604092597961, 'eval_precision': 0.7011128775834659, 'eval_recall': 0.7101449275362319, 'eval_f1': 0.7056, 'eval_accuracy': 0.9881189666627584, 'eval_runtime': 2.2603, 'eval_samples_per_second': 669.372, 'eval_steps_per_second': 84.059, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0004, 'grad_norm': 0.03942766785621643, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.07732532918453217, 'eval_precision': 0.69826224328594, 'eval_recall': 0.711755233494364, 'eval_f1': 0.7049441786283892, 'eval_accuracy': 0.9880408019697503, 'eval_runtime': 2.3029, 'eval_samples_per_second': 657.006, 'eval_steps_per_second': 82.506, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0004, 'grad_norm': 0.04579353705048561, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.07812540978193283, 'eval_precision': 0.7031746031746032, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7082334132693845, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.3243, 'eval_samples_per_second': 650.96, 'eval_steps_per_second': 81.746, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 560.1185, 'train_samples_per_second': 183.604, 'train_steps_per_second': 2.871, 'total_flos': 4434671800249680.0, 'train_loss': 0.014485419382670181, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9889
  predict_f1                 =     0.6932
  predict_loss               =     0.0409
  predict_precision          =     0.6808
  predict_recall             =     0.7061
  predict_runtime            = 0:00:01.90
  predict_samples_per_second =    655.782
  predict_steps_per_second   =     82.235
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_11_303.json completed. F1: 0.6932185145317545
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_101.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4399.87 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 5083.70 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4791.88 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:01, 4076.88 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 4866.89 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5591.69 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6049.08 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6542.47 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5314.24 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7036.63 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3643.92 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7145.81 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5034.41 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1261, 'grad_norm': 0.4551803767681122, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.048570360988378525, 'eval_precision': 0.5768621236133122, 'eval_recall': 0.5861513687600645, 'eval_f1': 0.5814696485623004, 'eval_accuracy': 0.983702661507797, 'eval_runtime': 4.7114, 'eval_samples_per_second': 321.137, 'eval_steps_per_second': 40.328, 'epoch': 1.0}
{'loss': 0.0389, 'grad_norm': 0.4604284167289734, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.041580554097890854, 'eval_precision': 0.6397608370702541, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6635658914728682, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 4.4794, 'eval_samples_per_second': 337.769, 'eval_steps_per_second': 42.416, 'epoch': 2.0}
{'loss': 0.0273, 'grad_norm': 0.3501981496810913, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.039027344435453415, 'eval_precision': 0.6636636636636637, 'eval_recall': 0.711755233494364, 'eval_f1': 0.686868686868687, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.5338, 'eval_samples_per_second': 333.714, 'eval_steps_per_second': 41.907, 'epoch': 3.0}
{'loss': 0.0198, 'grad_norm': 0.4481860101222992, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.03843408077955246, 'eval_precision': 0.7125, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7232355273592387, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 5.1666, 'eval_samples_per_second': 292.841, 'eval_steps_per_second': 36.774, 'epoch': 4.0}
{'loss': 0.014, 'grad_norm': 0.5463836193084717, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04221655800938606, 'eval_precision': 0.6923076923076923, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7081038552321007, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.5184, 'eval_samples_per_second': 334.853, 'eval_steps_per_second': 42.05, 'epoch': 5.0}
{'loss': 0.0103, 'grad_norm': 0.3030833303928375, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.043801892548799515, 'eval_precision': 0.7153965785381027, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7278481012658228, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5425, 'eval_samples_per_second': 333.075, 'eval_steps_per_second': 41.827, 'epoch': 6.0}
{'loss': 0.0073, 'grad_norm': 0.08968845754861832, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04861016198992729, 'eval_precision': 0.6975683890577508, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7177482408131353, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.579, 'eval_samples_per_second': 330.419, 'eval_steps_per_second': 41.493, 'epoch': 7.0}
{'loss': 0.0057, 'grad_norm': 0.3106764256954193, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05004796013236046, 'eval_precision': 0.717219589257504, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7240829346092504, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.5294, 'eval_samples_per_second': 334.043, 'eval_steps_per_second': 41.949, 'epoch': 8.0}
{'loss': 0.0046, 'grad_norm': 0.4365415871143341, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05173059180378914, 'eval_precision': 0.7085889570552147, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7258444619010213, 'eval_accuracy': 0.9892523547113768, 'eval_runtime': 4.5257, 'eval_samples_per_second': 334.315, 'eval_steps_per_second': 41.983, 'epoch': 9.0}
{'loss': 0.0036, 'grad_norm': 0.24143801629543304, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.054699450731277466, 'eval_precision': 0.7054263565891473, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7187993680884676, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.4925, 'eval_samples_per_second': 336.782, 'eval_steps_per_second': 42.292, 'epoch': 10.0}
{'loss': 0.003, 'grad_norm': 0.23538991808891296, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05618777871131897, 'eval_precision': 0.732484076433121, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7365892714171337, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.5746, 'eval_samples_per_second': 330.736, 'eval_steps_per_second': 41.533, 'epoch': 11.0}
{'loss': 0.0026, 'grad_norm': 0.12258603423833847, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.0562477707862854, 'eval_precision': 0.718944099378882, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7320158102766798, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.4847, 'eval_samples_per_second': 337.367, 'eval_steps_per_second': 42.366, 'epoch': 12.0}
{'train_runtime': 1198.9444, 'train_samples_per_second': 85.775, 'train_steps_per_second': 1.341, 'train_loss': 0.02192588020764773, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0219
  train_runtime            = 0:19:58.94
  train_samples            =       8570
  train_samples_per_second =     85.775
  train_steps_per_second   =      1.341
[{'loss': 0.1261, 'grad_norm': 0.4551803767681122, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.048570360988378525, 'eval_precision': 0.5768621236133122, 'eval_recall': 0.5861513687600645, 'eval_f1': 0.5814696485623004, 'eval_accuracy': 0.983702661507797, 'eval_runtime': 4.7114, 'eval_samples_per_second': 321.137, 'eval_steps_per_second': 40.328, 'epoch': 1.0, 'step': 134}, {'loss': 0.0389, 'grad_norm': 0.4604284167289734, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.041580554097890854, 'eval_precision': 0.6397608370702541, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6635658914728682, 'eval_accuracy': 0.9871809903466604, 'eval_runtime': 4.4794, 'eval_samples_per_second': 337.769, 'eval_steps_per_second': 42.416, 'epoch': 2.0, 'step': 268}, {'loss': 0.0273, 'grad_norm': 0.3501981496810913, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.039027344435453415, 'eval_precision': 0.6636636636636637, 'eval_recall': 0.711755233494364, 'eval_f1': 0.686868686868687, 'eval_accuracy': 0.9881580490092625, 'eval_runtime': 4.5338, 'eval_samples_per_second': 333.714, 'eval_steps_per_second': 41.907, 'epoch': 3.0, 'step': 402}, {'loss': 0.0198, 'grad_norm': 0.4481860101222992, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.03843408077955246, 'eval_precision': 0.7125, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7232355273592387, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 5.1666, 'eval_samples_per_second': 292.841, 'eval_steps_per_second': 36.774, 'epoch': 4.0, 'step': 536}, {'loss': 0.014, 'grad_norm': 0.5463836193084717, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04221655800938606, 'eval_precision': 0.6923076923076923, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7081038552321007, 'eval_accuracy': 0.9889006135928401, 'eval_runtime': 4.5184, 'eval_samples_per_second': 334.853, 'eval_steps_per_second': 42.05, 'epoch': 5.0, 'step': 670}, {'loss': 0.0103, 'grad_norm': 0.3030833303928375, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.043801892548799515, 'eval_precision': 0.7153965785381027, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7278481012658228, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5425, 'eval_samples_per_second': 333.075, 'eval_steps_per_second': 41.827, 'epoch': 6.0, 'step': 804}, {'loss': 0.0073, 'grad_norm': 0.08968845754861832, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.04861016198992729, 'eval_precision': 0.6975683890577508, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.7177482408131353, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 4.579, 'eval_samples_per_second': 330.419, 'eval_steps_per_second': 41.493, 'epoch': 7.0, 'step': 938}, {'loss': 0.0057, 'grad_norm': 0.3106764256954193, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.05004796013236046, 'eval_precision': 0.717219589257504, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7240829346092504, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.5294, 'eval_samples_per_second': 334.043, 'eval_steps_per_second': 41.949, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0046, 'grad_norm': 0.4365415871143341, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.05173059180378914, 'eval_precision': 0.7085889570552147, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7258444619010213, 'eval_accuracy': 0.9892523547113768, 'eval_runtime': 4.5257, 'eval_samples_per_second': 334.315, 'eval_steps_per_second': 41.983, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0036, 'grad_norm': 0.24143801629543304, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.054699450731277466, 'eval_precision': 0.7054263565891473, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7187993680884676, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 4.4925, 'eval_samples_per_second': 336.782, 'eval_steps_per_second': 42.292, 'epoch': 10.0, 'step': 1340}, {'loss': 0.003, 'grad_norm': 0.23538991808891296, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05618777871131897, 'eval_precision': 0.732484076433121, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7365892714171337, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.5746, 'eval_samples_per_second': 330.736, 'eval_steps_per_second': 41.533, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0026, 'grad_norm': 0.12258603423833847, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.0562477707862854, 'eval_precision': 0.718944099378882, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7320158102766798, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.4847, 'eval_samples_per_second': 337.367, 'eval_steps_per_second': 42.366, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1198.9444, 'train_samples_per_second': 85.775, 'train_steps_per_second': 1.341, 'total_flos': 1.2912881111013576e+16, 'train_loss': 0.02192588020764773, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9906
  predict_f1                 =     0.7414
  predict_loss               =     0.0345
  predict_precision          =     0.7094
  predict_recall             =     0.7763
  predict_runtime            = 0:00:03.85
  predict_samples_per_second =    324.895
  predict_steps_per_second   =     40.742
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_101.json completed. F1: 0.7413612565445027
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_303.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:01, 4138.48 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4902.37 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5590.83 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6200.08 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6454.47 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6823.26 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6920.65 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7149.11 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6083.68 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 5355.03 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4015.93 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 4577.25 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2731.31 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1719, 'grad_norm': 0.32947462797164917, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.055203117430210114, 'eval_precision': 0.5681114551083591, 'eval_recall': 0.5909822866344605, 'eval_f1': 0.579321231254933, 'eval_accuracy': 0.9813186383710478, 'eval_runtime': 2.3032, 'eval_samples_per_second': 656.912, 'eval_steps_per_second': 82.494, 'epoch': 1.0}
{'loss': 0.0464, 'grad_norm': 0.5447425246238708, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.0446234829723835, 'eval_precision': 0.5806451612903226, 'eval_recall': 0.6376811594202898, 'eval_f1': 0.6078280890253261, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3261, 'eval_samples_per_second': 650.452, 'eval_steps_per_second': 81.683, 'epoch': 2.0}
{'loss': 0.0356, 'grad_norm': 0.3473833501338959, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.04251671954989433, 'eval_precision': 0.6355283307810107, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.65149136577708, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.327, 'eval_samples_per_second': 650.191, 'eval_steps_per_second': 81.65, 'epoch': 3.0}
{'loss': 0.0297, 'grad_norm': 0.36845454573631287, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.042224545031785965, 'eval_precision': 0.6718995290423861, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6804451510333863, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.2455, 'eval_samples_per_second': 673.787, 'eval_steps_per_second': 84.613, 'epoch': 4.0}
{'loss': 0.0248, 'grad_norm': 0.24285583198070526, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04268808290362358, 'eval_precision': 0.6759259259259259, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6903073286052009, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.2925, 'eval_samples_per_second': 659.975, 'eval_steps_per_second': 82.879, 'epoch': 5.0}
{'loss': 0.0218, 'grad_norm': 0.28170642256736755, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04341146722435951, 'eval_precision': 0.6853582554517134, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6967537608867774, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.2912, 'eval_samples_per_second': 660.349, 'eval_steps_per_second': 82.926, 'epoch': 6.0}
{'loss': 0.0186, 'grad_norm': 0.5537117123603821, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04529891908168793, 'eval_precision': 0.6682027649769585, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6839622641509434, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.3551, 'eval_samples_per_second': 642.427, 'eval_steps_per_second': 80.675, 'epoch': 7.0}
{'loss': 0.0163, 'grad_norm': 0.4297034442424774, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.04698743298649788, 'eval_precision': 0.6579754601226994, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.673998428908091, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2541, 'eval_samples_per_second': 671.233, 'eval_steps_per_second': 84.292, 'epoch': 8.0}
{'loss': 0.0136, 'grad_norm': 0.2855331301689148, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.047963280230760574, 'eval_precision': 0.6854460093896714, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6952380952380952, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.2832, 'eval_samples_per_second': 662.67, 'eval_steps_per_second': 83.217, 'epoch': 9.0}
{'loss': 0.0126, 'grad_norm': 0.4104574918746948, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.04751168563961983, 'eval_precision': 0.6945736434108527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7077409162717219, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 2.2876, 'eval_samples_per_second': 661.389, 'eval_steps_per_second': 83.056, 'epoch': 10.0}
{'loss': 0.0108, 'grad_norm': 0.26645833253860474, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.04889790341258049, 'eval_precision': 0.6827371695178849, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6946202531645569, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.2401, 'eval_samples_per_second': 675.406, 'eval_steps_per_second': 84.816, 'epoch': 11.0}
{'loss': 0.0108, 'grad_norm': 0.6065248847007751, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.04856988415122032, 'eval_precision': 0.6857585139318886, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6992896606156275, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 2.5319, 'eval_samples_per_second': 597.563, 'eval_steps_per_second': 75.041, 'epoch': 12.0}
{'train_runtime': 491.2608, 'train_samples_per_second': 209.339, 'train_steps_per_second': 3.273, 'train_loss': 0.03440693754758408, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0344
  train_runtime            = 0:08:11.26
  train_samples            =       8570
  train_samples_per_second =    209.339
  train_steps_per_second   =      3.273
[{'loss': 0.1719, 'grad_norm': 0.32947462797164917, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.055203117430210114, 'eval_precision': 0.5681114551083591, 'eval_recall': 0.5909822866344605, 'eval_f1': 0.579321231254933, 'eval_accuracy': 0.9813186383710478, 'eval_runtime': 2.3032, 'eval_samples_per_second': 656.912, 'eval_steps_per_second': 82.494, 'epoch': 1.0, 'step': 134}, {'loss': 0.0464, 'grad_norm': 0.5447425246238708, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.0446234829723835, 'eval_precision': 0.5806451612903226, 'eval_recall': 0.6376811594202898, 'eval_f1': 0.6078280890253261, 'eval_accuracy': 0.9855004494469848, 'eval_runtime': 2.3261, 'eval_samples_per_second': 650.452, 'eval_steps_per_second': 81.683, 'epoch': 2.0, 'step': 268}, {'loss': 0.0356, 'grad_norm': 0.3473833501338959, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04251671954989433, 'eval_precision': 0.6355283307810107, 'eval_recall': 0.6682769726247987, 'eval_f1': 0.65149136577708, 'eval_accuracy': 0.9858131082190175, 'eval_runtime': 2.327, 'eval_samples_per_second': 650.191, 'eval_steps_per_second': 81.65, 'epoch': 3.0, 'step': 402}, {'loss': 0.0297, 'grad_norm': 0.36845454573631287, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.042224545031785965, 'eval_precision': 0.6718995290423861, 'eval_recall': 0.6892109500805152, 'eval_f1': 0.6804451510333863, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.2455, 'eval_samples_per_second': 673.787, 'eval_steps_per_second': 84.613, 'epoch': 4.0, 'step': 536}, {'loss': 0.0248, 'grad_norm': 0.24285583198070526, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04268808290362358, 'eval_precision': 0.6759259259259259, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6903073286052009, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.2925, 'eval_samples_per_second': 659.975, 'eval_steps_per_second': 82.879, 'epoch': 5.0, 'step': 670}, {'loss': 0.0218, 'grad_norm': 0.28170642256736755, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04341146722435951, 'eval_precision': 0.6853582554517134, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.6967537608867774, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.2912, 'eval_samples_per_second': 660.349, 'eval_steps_per_second': 82.926, 'epoch': 6.0, 'step': 804}, {'loss': 0.0186, 'grad_norm': 0.5537117123603821, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.04529891908168793, 'eval_precision': 0.6682027649769585, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6839622641509434, 'eval_accuracy': 0.9870637433071482, 'eval_runtime': 2.3551, 'eval_samples_per_second': 642.427, 'eval_steps_per_second': 80.675, 'epoch': 7.0, 'step': 938}, {'loss': 0.0163, 'grad_norm': 0.4297034442424774, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.04698743298649788, 'eval_precision': 0.6579754601226994, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.673998428908091, 'eval_accuracy': 0.9869074139211318, 'eval_runtime': 2.2541, 'eval_samples_per_second': 671.233, 'eval_steps_per_second': 84.292, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0136, 'grad_norm': 0.2855331301689148, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.047963280230760574, 'eval_precision': 0.6854460093896714, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.6952380952380952, 'eval_accuracy': 0.9873764020791809, 'eval_runtime': 2.2832, 'eval_samples_per_second': 662.67, 'eval_steps_per_second': 83.217, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0126, 'grad_norm': 0.4104574918746948, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.04751168563961983, 'eval_precision': 0.6945736434108527, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7077409162717219, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 2.2876, 'eval_samples_per_second': 661.389, 'eval_steps_per_second': 83.056, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0108, 'grad_norm': 0.26645833253860474, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.04889790341258049, 'eval_precision': 0.6827371695178849, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.6946202531645569, 'eval_accuracy': 0.9872591550396685, 'eval_runtime': 2.2401, 'eval_samples_per_second': 675.406, 'eval_steps_per_second': 84.816, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0108, 'grad_norm': 0.6065248847007751, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.04856988415122032, 'eval_precision': 0.6857585139318886, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.6992896606156275, 'eval_accuracy': 0.9876890608512136, 'eval_runtime': 2.5319, 'eval_samples_per_second': 597.563, 'eval_steps_per_second': 75.041, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 491.2608, 'train_samples_per_second': 209.339, 'train_steps_per_second': 3.273, 'total_flos': 4434671800249680.0, 'train_loss': 0.03440693754758408, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9901
  predict_f1                 =     0.7146
  predict_loss               =     0.0389
  predict_precision          =     0.6996
  predict_recall             =     0.7303
  predict_runtime            = 0:00:02.12
  predict_samples_per_second =    590.037
  predict_steps_per_second   =      73.99
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_08_303.json completed. F1: 0.7145922746781115
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_202.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:03, 2268.91 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 3760.59 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4781.32 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 5236.54 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5853.27 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6403.12 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6667.25 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7013.93 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5587.38 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6158.60 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 5366.50 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6058.94 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 1594.56 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0784, 'grad_norm': 0.7261815667152405, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.03933222219347954, 'eval_precision': 0.6916932907348243, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6944667201283079, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5998, 'eval_samples_per_second': 328.929, 'eval_steps_per_second': 41.306, 'epoch': 1.0}
{'loss': 0.0282, 'grad_norm': 0.567962646484375, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04043347015976906, 'eval_precision': 0.674565560821485, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6810207336523125, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.5021, 'eval_samples_per_second': 336.066, 'eval_steps_per_second': 42.203, 'epoch': 2.0}
{'loss': 0.0163, 'grad_norm': 0.347727507352829, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.039652206003665924, 'eval_precision': 0.7017828200972447, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6995153473344103, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6095, 'eval_samples_per_second': 328.233, 'eval_steps_per_second': 41.219, 'epoch': 3.0}
{'loss': 0.0077, 'grad_norm': 0.404636412858963, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.043192069977521896, 'eval_precision': 0.72, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7223113964686999, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.5033, 'eval_samples_per_second': 335.979, 'eval_steps_per_second': 42.192, 'epoch': 4.0}
{'loss': 0.0035, 'grad_norm': 0.13119111955165863, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05024687945842743, 'eval_precision': 0.7238095238095238, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7290167865707433, 'eval_accuracy': 0.9891351076718646, 'eval_runtime': 4.4997, 'eval_samples_per_second': 336.242, 'eval_steps_per_second': 42.225, 'epoch': 5.0}
{'loss': 0.0022, 'grad_norm': 0.06229700148105621, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.04814498871564865, 'eval_precision': 0.726984126984127, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7322142286171064, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.4955, 'eval_samples_per_second': 336.562, 'eval_steps_per_second': 42.265, 'epoch': 6.0}
{'loss': 0.0018, 'grad_norm': 0.21267962455749512, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.055063992738723755, 'eval_precision': 0.725, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7359238699444886, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5866, 'eval_samples_per_second': 329.872, 'eval_steps_per_second': 41.425, 'epoch': 7.0}
{'loss': 0.0009, 'grad_norm': 0.03157887980341911, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.061188966035842896, 'eval_precision': 0.734375, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7454401268834259, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.6142, 'eval_samples_per_second': 327.9, 'eval_steps_per_second': 41.177, 'epoch': 8.0}
{'loss': 0.0004, 'grad_norm': 0.010551352985203266, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06063912436366081, 'eval_precision': 0.7436708860759493, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7501995211492418, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 4.6005, 'eval_samples_per_second': 328.879, 'eval_steps_per_second': 41.3, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.005342273041605949, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06142694130539894, 'eval_precision': 0.7412140575079872, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7441860465116279, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 4.5691, 'eval_samples_per_second': 331.139, 'eval_steps_per_second': 41.584, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.005445199087262154, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.0625421553850174, 'eval_precision': 0.7479806138933764, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7467741935483871, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 4.6843, 'eval_samples_per_second': 322.995, 'eval_steps_per_second': 40.561, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.0025139604695141315, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06279060989618301, 'eval_precision': 0.7467948717948718, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7485943775100402, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 4.5687, 'eval_samples_per_second': 331.165, 'eval_steps_per_second': 41.587, 'epoch': 12.0}
{'train_runtime': 1398.9298, 'train_samples_per_second': 73.513, 'train_steps_per_second': 1.149, 'train_loss': 0.011648170926274537, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0116
  train_runtime            = 0:23:18.92
  train_samples            =       8570
  train_samples_per_second =     73.513
  train_steps_per_second   =      1.149
[{'loss': 0.0784, 'grad_norm': 0.7261815667152405, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.03933222219347954, 'eval_precision': 0.6916932907348243, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6944667201283079, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5998, 'eval_samples_per_second': 328.929, 'eval_steps_per_second': 41.306, 'epoch': 1.0, 'step': 134}, {'loss': 0.0282, 'grad_norm': 0.567962646484375, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.04043347015976906, 'eval_precision': 0.674565560821485, 'eval_recall': 0.6876006441223832, 'eval_f1': 0.6810207336523125, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 4.5021, 'eval_samples_per_second': 336.066, 'eval_steps_per_second': 42.203, 'epoch': 2.0, 'step': 268}, {'loss': 0.0163, 'grad_norm': 0.347727507352829, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.039652206003665924, 'eval_precision': 0.7017828200972447, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6995153473344103, 'eval_accuracy': 0.9883143783952788, 'eval_runtime': 4.6095, 'eval_samples_per_second': 328.233, 'eval_steps_per_second': 41.219, 'epoch': 3.0, 'step': 402}, {'loss': 0.0077, 'grad_norm': 0.404636412858963, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.043192069977521896, 'eval_precision': 0.72, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7223113964686999, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.5033, 'eval_samples_per_second': 335.979, 'eval_steps_per_second': 42.192, 'epoch': 4.0, 'step': 536}, {'loss': 0.0035, 'grad_norm': 0.13119111955165863, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05024687945842743, 'eval_precision': 0.7238095238095238, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7290167865707433, 'eval_accuracy': 0.9891351076718646, 'eval_runtime': 4.4997, 'eval_samples_per_second': 336.242, 'eval_steps_per_second': 42.225, 'epoch': 5.0, 'step': 670}, {'loss': 0.0022, 'grad_norm': 0.06229700148105621, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04814498871564865, 'eval_precision': 0.726984126984127, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7322142286171064, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.4955, 'eval_samples_per_second': 336.562, 'eval_steps_per_second': 42.265, 'epoch': 6.0, 'step': 804}, {'loss': 0.0018, 'grad_norm': 0.21267962455749512, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.055063992738723755, 'eval_precision': 0.725, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7359238699444886, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5866, 'eval_samples_per_second': 329.872, 'eval_steps_per_second': 41.425, 'epoch': 7.0, 'step': 938}, {'loss': 0.0009, 'grad_norm': 0.03157887980341911, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.061188966035842896, 'eval_precision': 0.734375, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7454401268834259, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.6142, 'eval_samples_per_second': 327.9, 'eval_steps_per_second': 41.177, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0004, 'grad_norm': 0.010551352985203266, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06063912436366081, 'eval_precision': 0.7436708860759493, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7501995211492418, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 4.6005, 'eval_samples_per_second': 328.879, 'eval_steps_per_second': 41.3, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0003, 'grad_norm': 0.005342273041605949, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.06142694130539894, 'eval_precision': 0.7412140575079872, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7441860465116279, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 4.5691, 'eval_samples_per_second': 331.139, 'eval_steps_per_second': 41.584, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0001, 'grad_norm': 0.005445199087262154, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.0625421553850174, 'eval_precision': 0.7479806138933764, 'eval_recall': 0.7455716586151369, 'eval_f1': 0.7467741935483871, 'eval_accuracy': 0.9899167546019463, 'eval_runtime': 4.6843, 'eval_samples_per_second': 322.995, 'eval_steps_per_second': 40.561, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0001, 'grad_norm': 0.0025139604695141315, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06279060989618301, 'eval_precision': 0.7467948717948718, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7485943775100402, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 4.5687, 'eval_samples_per_second': 331.165, 'eval_steps_per_second': 41.587, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1398.9298, 'train_samples_per_second': 73.513, 'train_steps_per_second': 1.149, 'total_flos': 1.2881569369077948e+16, 'train_loss': 0.011648170926274537, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9891
  predict_f1                 =     0.7059
  predict_loss               =      0.039
  predict_precision          =     0.7013
  predict_recall             =     0.7105
  predict_runtime            = 0:00:03.88
  predict_samples_per_second =     321.99
  predict_steps_per_second   =     40.377
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_10_202.json completed. F1: 0.7058823529411765
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_101.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:05, 1356.69 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2636.32 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:01, 3710.42 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 4441.65 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5194.79 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5868.95 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6283.58 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6722.36 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4386.08 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 5867.60 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4539.76 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7399.29 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4691.94 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1046, 'grad_norm': 0.8132388591766357, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04234103113412857, 'eval_precision': 0.6492063492063492, 'eval_recall': 0.6586151368760065, 'eval_f1': 0.653876898481215, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 5.1947, 'eval_samples_per_second': 291.261, 'eval_steps_per_second': 36.576, 'epoch': 1.0}
{'loss': 0.0359, 'grad_norm': 0.5037636160850525, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03731439635157585, 'eval_precision': 0.6605783866057838, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.679186228482003, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6441, 'eval_samples_per_second': 325.787, 'eval_steps_per_second': 40.912, 'epoch': 2.0}
{'loss': 0.0248, 'grad_norm': 0.20059441030025482, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.037921637296676636, 'eval_precision': 0.7031484257871065, 'eval_recall': 0.7552334943639292, 'eval_f1': 0.7282608695652175, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.5987, 'eval_samples_per_second': 329.005, 'eval_steps_per_second': 41.316, 'epoch': 3.0}
{'loss': 0.0165, 'grad_norm': 0.6718701720237732, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04101860150694847, 'eval_precision': 0.694006309148265, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.701195219123506, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6209, 'eval_samples_per_second': 327.423, 'eval_steps_per_second': 41.117, 'epoch': 4.0}
{'loss': 0.0106, 'grad_norm': 0.29207903146743774, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.046367790549993515, 'eval_precision': 0.7089201877934272, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.719047619047619, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.6478, 'eval_samples_per_second': 325.529, 'eval_steps_per_second': 40.879, 'epoch': 5.0}
{'loss': 0.0068, 'grad_norm': 0.7704616785049438, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04779212176799774, 'eval_precision': 0.7320574162679426, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.735576923076923, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.5837, 'eval_samples_per_second': 330.08, 'eval_steps_per_second': 41.451, 'epoch': 6.0}
{'loss': 0.0049, 'grad_norm': 0.13208261132240295, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05015813186764717, 'eval_precision': 0.7075471698113207, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7159904534606206, 'eval_accuracy': 0.9887833665533279, 'eval_runtime': 4.5713, 'eval_samples_per_second': 330.982, 'eval_steps_per_second': 41.564, 'epoch': 7.0}
{'loss': 0.003, 'grad_norm': 0.7671714425086975, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.056651681661605835, 'eval_precision': 0.7260940032414911, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7237479806138934, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6054, 'eval_samples_per_second': 328.53, 'eval_steps_per_second': 41.256, 'epoch': 8.0}
{'loss': 0.0024, 'grad_norm': 0.0860476940870285, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05999492481350899, 'eval_precision': 0.7158908507223114, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7170418006430868, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.6435, 'eval_samples_per_second': 325.829, 'eval_steps_per_second': 40.917, 'epoch': 9.0}
{'loss': 0.0018, 'grad_norm': 0.45563414692878723, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06099776178598404, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7233704292527822, 'eval_accuracy': 0.9891351076718646, 'eval_runtime': 4.5719, 'eval_samples_per_second': 330.936, 'eval_steps_per_second': 41.558, 'epoch': 10.0}
{'loss': 0.0015, 'grad_norm': 0.08677088469266891, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06247923523187637, 'eval_precision': 0.7253968253968254, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7306155075939249, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.6643, 'eval_samples_per_second': 324.376, 'eval_steps_per_second': 40.735, 'epoch': 11.0}
{'loss': 0.0012, 'grad_norm': 0.4307444393634796, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06267718225717545, 'eval_precision': 0.7420382165605095, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7461969575660529, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 4.6306, 'eval_samples_per_second': 326.739, 'eval_steps_per_second': 41.031, 'epoch': 12.0}
{'train_runtime': 1135.7224, 'train_samples_per_second': 90.55, 'train_steps_per_second': 2.832, 'train_loss': 0.01782766657311525, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0178
  train_runtime            = 0:18:55.72
  train_samples            =       8570
  train_samples_per_second =      90.55
  train_steps_per_second   =      2.832
[{'loss': 0.1046, 'grad_norm': 0.8132388591766357, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04234103113412857, 'eval_precision': 0.6492063492063492, 'eval_recall': 0.6586151368760065, 'eval_f1': 0.653876898481215, 'eval_accuracy': 0.9872982373861726, 'eval_runtime': 5.1947, 'eval_samples_per_second': 291.261, 'eval_steps_per_second': 36.576, 'epoch': 1.0, 'step': 268}, {'loss': 0.0359, 'grad_norm': 0.5037636160850525, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.03731439635157585, 'eval_precision': 0.6605783866057838, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.679186228482003, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 4.6441, 'eval_samples_per_second': 325.787, 'eval_steps_per_second': 40.912, 'epoch': 2.0, 'step': 536}, {'loss': 0.0248, 'grad_norm': 0.20059441030025482, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.037921637296676636, 'eval_precision': 0.7031484257871065, 'eval_recall': 0.7552334943639292, 'eval_f1': 0.7282608695652175, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 4.5987, 'eval_samples_per_second': 329.005, 'eval_steps_per_second': 41.316, 'epoch': 3.0, 'step': 804}, {'loss': 0.0165, 'grad_norm': 0.6718701720237732, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04101860150694847, 'eval_precision': 0.694006309148265, 'eval_recall': 0.7085346215780999, 'eval_f1': 0.701195219123506, 'eval_accuracy': 0.9884707077812952, 'eval_runtime': 4.6209, 'eval_samples_per_second': 327.423, 'eval_steps_per_second': 41.117, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0106, 'grad_norm': 0.29207903146743774, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.046367790549993515, 'eval_precision': 0.7089201877934272, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.719047619047619, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.6478, 'eval_samples_per_second': 325.529, 'eval_steps_per_second': 40.879, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0068, 'grad_norm': 0.7704616785049438, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.04779212176799774, 'eval_precision': 0.7320574162679426, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.735576923076923, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.5837, 'eval_samples_per_second': 330.08, 'eval_steps_per_second': 41.451, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0049, 'grad_norm': 0.13208261132240295, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05015813186764717, 'eval_precision': 0.7075471698113207, 'eval_recall': 0.7246376811594203, 'eval_f1': 0.7159904534606206, 'eval_accuracy': 0.9887833665533279, 'eval_runtime': 4.5713, 'eval_samples_per_second': 330.982, 'eval_steps_per_second': 41.564, 'epoch': 7.0, 'step': 1876}, {'loss': 0.003, 'grad_norm': 0.7671714425086975, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.056651681661605835, 'eval_precision': 0.7260940032414911, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.7237479806138934, 'eval_accuracy': 0.9891741900183687, 'eval_runtime': 4.6054, 'eval_samples_per_second': 328.53, 'eval_steps_per_second': 41.256, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0024, 'grad_norm': 0.0860476940870285, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05999492481350899, 'eval_precision': 0.7158908507223114, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.7170418006430868, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 4.6435, 'eval_samples_per_second': 325.829, 'eval_steps_per_second': 40.917, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0018, 'grad_norm': 0.45563414692878723, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06099776178598404, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7233704292527822, 'eval_accuracy': 0.9891351076718646, 'eval_runtime': 4.5719, 'eval_samples_per_second': 330.936, 'eval_steps_per_second': 41.558, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0015, 'grad_norm': 0.08677088469266891, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06247923523187637, 'eval_precision': 0.7253968253968254, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7306155075939249, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.6643, 'eval_samples_per_second': 324.376, 'eval_steps_per_second': 40.735, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0012, 'grad_norm': 0.4307444393634796, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06267718225717545, 'eval_precision': 0.7420382165605095, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7461969575660529, 'eval_accuracy': 0.9897213428694259, 'eval_runtime': 4.6306, 'eval_samples_per_second': 326.739, 'eval_steps_per_second': 41.031, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1135.7224, 'train_samples_per_second': 90.55, 'train_steps_per_second': 2.832, 'total_flos': 1.1352380222341752e+16, 'train_loss': 0.01782766657311525, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9882
  predict_f1                 =     0.6324
  predict_loss               =     0.0371
  predict_precision          =     0.5946
  predict_recall             =     0.6754
  predict_runtime            = 0:00:04.03
  predict_samples_per_second =    310.471
  predict_steps_per_second   =     38.933
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_01_101.json completed. F1: 0.6324435318275154
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_202.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 2822.11 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 3235.38 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 3926.29 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 4448.15 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5188.17 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5858.83 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6247.96 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6651.67 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5101.76 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7170.62 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2344.15 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 5649.96 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3670.09 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0801, 'grad_norm': 1.6437292098999023, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04192431643605232, 'eval_precision': 0.6671899529042387, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6756756756756758, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 5.5728, 'eval_samples_per_second': 271.496, 'eval_steps_per_second': 34.094, 'epoch': 1.0}
{'loss': 0.0215, 'grad_norm': 0.564292311668396, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.03921796754002571, 'eval_precision': 0.7140600315955766, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7208931419457736, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 5.5258, 'eval_samples_per_second': 273.805, 'eval_steps_per_second': 34.384, 'epoch': 2.0}
{'loss': 0.0074, 'grad_norm': 0.22457872331142426, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04541058838367462, 'eval_precision': 0.7098765432098766, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7249802994483846, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.5691, 'eval_samples_per_second': 271.677, 'eval_steps_per_second': 34.117, 'epoch': 3.0}
{'loss': 0.0026, 'grad_norm': 0.056682128459215164, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05127054452896118, 'eval_precision': 0.710236220472441, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7181528662420382, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 5.5993, 'eval_samples_per_second': 270.212, 'eval_steps_per_second': 33.933, 'epoch': 4.0}
{'loss': 0.0011, 'grad_norm': 0.26870524883270264, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.05705101042985916, 'eval_precision': 0.703875968992248, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7172195892575041, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 5.6256, 'eval_samples_per_second': 268.948, 'eval_steps_per_second': 33.774, 'epoch': 5.0}
{'loss': 0.0008, 'grad_norm': 0.17127197980880737, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.06098199263215065, 'eval_precision': 0.6972049689440993, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7098814229249012, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 5.6559, 'eval_samples_per_second': 267.507, 'eval_steps_per_second': 33.593, 'epoch': 6.0}
{'loss': 0.0006, 'grad_norm': 0.016679059714078903, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.05953802913427353, 'eval_precision': 0.7134146341463414, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7329678935003915, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.5297, 'eval_samples_per_second': 273.612, 'eval_steps_per_second': 34.36, 'epoch': 7.0}
{'loss': 0.0002, 'grad_norm': 0.0028207642026245594, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.06073851138353348, 'eval_precision': 0.7321711568938193, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7380191693290735, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.7111, 'eval_samples_per_second': 264.925, 'eval_steps_per_second': 33.269, 'epoch': 8.0}
{'loss': 0.0001, 'grad_norm': 0.003313051536679268, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06337746232748032, 'eval_precision': 0.7304075235109718, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7402700555996824, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5247, 'eval_samples_per_second': 273.861, 'eval_steps_per_second': 34.391, 'epoch': 9.0}
{'loss': 0.0001, 'grad_norm': 0.0028081978671252728, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.0645255520939827, 'eval_precision': 0.736, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7383627608346709, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 5.6018, 'eval_samples_per_second': 270.09, 'eval_steps_per_second': 33.917, 'epoch': 10.0}
{'loss': 0.0001, 'grad_norm': 0.0006880777655169368, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06439272314310074, 'eval_precision': 0.7385103011093502, 'eval_recall': 0.750402576489533, 'eval_f1': 0.744408945686901, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 5.6505, 'eval_samples_per_second': 267.764, 'eval_steps_per_second': 33.625, 'epoch': 11.0}
{'loss': 0.0001, 'grad_norm': 0.000953547889366746, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06450434029102325, 'eval_precision': 0.740506329113924, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7470071827613727, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.8215, 'eval_samples_per_second': 259.9, 'eval_steps_per_second': 32.638, 'epoch': 12.0}
{'train_runtime': 1413.6971, 'train_samples_per_second': 72.745, 'train_steps_per_second': 1.137, 'train_loss': 0.009550553051215036, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0096
  train_runtime            = 0:23:33.69
  train_samples            =       8570
  train_samples_per_second =     72.745
  train_steps_per_second   =      1.137
[{'loss': 0.0801, 'grad_norm': 1.6437292098999023, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04192431643605232, 'eval_precision': 0.6671899529042387, 'eval_recall': 0.6843800322061192, 'eval_f1': 0.6756756756756758, 'eval_accuracy': 0.987454566772189, 'eval_runtime': 5.5728, 'eval_samples_per_second': 271.496, 'eval_steps_per_second': 34.094, 'epoch': 1.0, 'step': 134}, {'loss': 0.0215, 'grad_norm': 0.564292311668396, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.03921796754002571, 'eval_precision': 0.7140600315955766, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.7208931419457736, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 5.5258, 'eval_samples_per_second': 273.805, 'eval_steps_per_second': 34.384, 'epoch': 2.0, 'step': 268}, {'loss': 0.0074, 'grad_norm': 0.22457872331142426, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.04541058838367462, 'eval_precision': 0.7098765432098766, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7249802994483846, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.5691, 'eval_samples_per_second': 271.677, 'eval_steps_per_second': 34.117, 'epoch': 3.0, 'step': 402}, {'loss': 0.0026, 'grad_norm': 0.056682128459215164, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.05127054452896118, 'eval_precision': 0.710236220472441, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7181528662420382, 'eval_accuracy': 0.9890178606323524, 'eval_runtime': 5.5993, 'eval_samples_per_second': 270.212, 'eval_steps_per_second': 33.933, 'epoch': 4.0, 'step': 536}, {'loss': 0.0011, 'grad_norm': 0.26870524883270264, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.05705101042985916, 'eval_precision': 0.703875968992248, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.7172195892575041, 'eval_accuracy': 0.9889396959393442, 'eval_runtime': 5.6256, 'eval_samples_per_second': 268.948, 'eval_steps_per_second': 33.774, 'epoch': 5.0, 'step': 670}, {'loss': 0.0008, 'grad_norm': 0.17127197980880737, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.06098199263215065, 'eval_precision': 0.6972049689440993, 'eval_recall': 0.7230273752012882, 'eval_f1': 0.7098814229249012, 'eval_accuracy': 0.9886661195138156, 'eval_runtime': 5.6559, 'eval_samples_per_second': 267.507, 'eval_steps_per_second': 33.593, 'epoch': 6.0, 'step': 804}, {'loss': 0.0006, 'grad_norm': 0.016679059714078903, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.05953802913427353, 'eval_precision': 0.7134146341463414, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7329678935003915, 'eval_accuracy': 0.989330519404385, 'eval_runtime': 5.5297, 'eval_samples_per_second': 273.612, 'eval_steps_per_second': 34.36, 'epoch': 7.0, 'step': 938}, {'loss': 0.0002, 'grad_norm': 0.0028207642026245594, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.06073851138353348, 'eval_precision': 0.7321711568938193, 'eval_recall': 0.7439613526570048, 'eval_f1': 0.7380191693290735, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.7111, 'eval_samples_per_second': 264.925, 'eval_steps_per_second': 33.269, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0001, 'grad_norm': 0.003313051536679268, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.06337746232748032, 'eval_precision': 0.7304075235109718, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7402700555996824, 'eval_accuracy': 0.9895650134834095, 'eval_runtime': 5.5247, 'eval_samples_per_second': 273.861, 'eval_steps_per_second': 34.391, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0001, 'grad_norm': 0.0028081978671252728, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.0645255520939827, 'eval_precision': 0.736, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7383627608346709, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 5.6018, 'eval_samples_per_second': 270.09, 'eval_steps_per_second': 33.917, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0001, 'grad_norm': 0.0006880777655169368, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.06439272314310074, 'eval_precision': 0.7385103011093502, 'eval_recall': 0.750402576489533, 'eval_f1': 0.744408945686901, 'eval_accuracy': 0.9899558369484504, 'eval_runtime': 5.6505, 'eval_samples_per_second': 267.764, 'eval_steps_per_second': 33.625, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0001, 'grad_norm': 0.000953547889366746, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.06450434029102325, 'eval_precision': 0.740506329113924, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7470071827613727, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.8215, 'eval_samples_per_second': 259.9, 'eval_steps_per_second': 32.638, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1413.6971, 'train_samples_per_second': 72.745, 'train_steps_per_second': 1.137, 'total_flos': 1.3389246143264808e+16, 'train_loss': 0.009550553051215036, 'epoch': 12.0, 'step': 1608}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9916
  predict_f1                 =     0.7575
  predict_loss               =     0.0313
  predict_precision          =     0.7416
  predict_recall             =     0.7741
  predict_runtime            = 0:00:04.73
  predict_samples_per_second =    264.287
  predict_steps_per_second   =     33.141
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_09_202.json completed. F1: 0.7575107296137339
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_202.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3172.11 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4625.75 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 5163.53 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 4932.63 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 5604.74 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6191.55 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6499.62 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6831.34 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 5408.61 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7276.22 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2816.76 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7382.78 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 5276.43 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1422, 'grad_norm': 1.5282396078109741, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04870511591434479, 'eval_precision': 0.5287009063444109, 'eval_recall': 0.5636070853462157, 'eval_f1': 0.5455962587685114, 'eval_accuracy': 0.98421073201235, 'eval_runtime': 5.6992, 'eval_samples_per_second': 265.477, 'eval_steps_per_second': 33.338, 'epoch': 1.0}
{'loss': 0.0351, 'grad_norm': 0.48395517468452454, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.038115326315164566, 'eval_precision': 0.671850699844479, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6835443037974684, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 5.7603, 'eval_samples_per_second': 262.658, 'eval_steps_per_second': 32.984, 'epoch': 2.0}
{'loss': 0.0243, 'grad_norm': 0.5630380511283875, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03956840932369232, 'eval_precision': 0.6888888888888889, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6938449240607514, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 5.5832, 'eval_samples_per_second': 270.993, 'eval_steps_per_second': 34.031, 'epoch': 3.0}
{'loss': 0.0164, 'grad_norm': 0.8870699405670166, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.0409337654709816, 'eval_precision': 0.7024, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7046548956661317, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 5.5918, 'eval_samples_per_second': 270.577, 'eval_steps_per_second': 33.979, 'epoch': 4.0}
{'loss': 0.0101, 'grad_norm': 0.3455486297607422, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04226674884557724, 'eval_precision': 0.7076205287713841, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7199367088607596, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 5.5341, 'eval_samples_per_second': 273.396, 'eval_steps_per_second': 34.333, 'epoch': 5.0}
{'loss': 0.0069, 'grad_norm': 1.5989006757736206, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04318810999393463, 'eval_precision': 0.7222222222222222, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7375886524822696, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.6551, 'eval_samples_per_second': 267.546, 'eval_steps_per_second': 33.598, 'epoch': 6.0}
{'loss': 0.0048, 'grad_norm': 0.4358207583427429, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04682059958577156, 'eval_precision': 0.7237048665620094, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7329093799682035, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.5919, 'eval_samples_per_second': 270.569, 'eval_steps_per_second': 33.978, 'epoch': 7.0}
{'loss': 0.0032, 'grad_norm': 0.09622112661600113, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.049089107662439346, 'eval_precision': 0.7373417721518988, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7438148443735035, 'eval_accuracy': 0.9901903310274749, 'eval_runtime': 5.8774, 'eval_samples_per_second': 257.427, 'eval_steps_per_second': 32.327, 'epoch': 8.0}
{'loss': 0.0023, 'grad_norm': 0.7014458179473877, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.050114184617996216, 'eval_precision': 0.7375, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7486122125297383, 'eval_accuracy': 0.9903466604134912, 'eval_runtime': 5.5544, 'eval_samples_per_second': 272.396, 'eval_steps_per_second': 34.207, 'epoch': 9.0}
{'loss': 0.0019, 'grad_norm': 0.5984876751899719, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.05159623548388481, 'eval_precision': 0.7394034536891679, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7488076311605723, 'eval_accuracy': 0.9901903310274749, 'eval_runtime': 5.5852, 'eval_samples_per_second': 270.896, 'eval_steps_per_second': 34.019, 'epoch': 10.0}
{'loss': 0.0015, 'grad_norm': 0.022623106837272644, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05362487956881523, 'eval_precision': 0.7435897435897436, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7453815261044177, 'eval_accuracy': 0.9901903310274749, 'eval_runtime': 5.6256, 'eval_samples_per_second': 268.947, 'eval_steps_per_second': 33.774, 'epoch': 11.0}
{'loss': 0.0013, 'grad_norm': 0.10615912079811096, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05356736108660698, 'eval_precision': 0.747588424437299, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7481898632341111, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 5.8552, 'eval_samples_per_second': 258.402, 'eval_steps_per_second': 32.45, 'epoch': 12.0}
{'train_runtime': 1475.8218, 'train_samples_per_second': 69.683, 'train_steps_per_second': 1.09, 'train_loss': 0.02082696638593626, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0208
  train_runtime            = 0:24:35.82
  train_samples            =       8570
  train_samples_per_second =     69.683
  train_steps_per_second   =       1.09
[{'loss': 0.1422, 'grad_norm': 1.5282396078109741, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.04870511591434479, 'eval_precision': 0.5287009063444109, 'eval_recall': 0.5636070853462157, 'eval_f1': 0.5455962587685114, 'eval_accuracy': 0.98421073201235, 'eval_runtime': 5.6992, 'eval_samples_per_second': 265.477, 'eval_steps_per_second': 33.338, 'epoch': 1.0, 'step': 134}, {'loss': 0.0351, 'grad_norm': 0.48395517468452454, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.038115326315164566, 'eval_precision': 0.671850699844479, 'eval_recall': 0.6956521739130435, 'eval_f1': 0.6835443037974684, 'eval_accuracy': 0.9887052018603197, 'eval_runtime': 5.7603, 'eval_samples_per_second': 262.658, 'eval_steps_per_second': 32.984, 'epoch': 2.0, 'step': 268}, {'loss': 0.0243, 'grad_norm': 0.5630380511283875, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.03956840932369232, 'eval_precision': 0.6888888888888889, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6938449240607514, 'eval_accuracy': 0.9885879548208074, 'eval_runtime': 5.5832, 'eval_samples_per_second': 270.993, 'eval_steps_per_second': 34.031, 'epoch': 3.0, 'step': 402}, {'loss': 0.0164, 'grad_norm': 0.8870699405670166, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.0409337654709816, 'eval_precision': 0.7024, 'eval_recall': 0.7069243156199678, 'eval_f1': 0.7046548956661317, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 5.5918, 'eval_samples_per_second': 270.577, 'eval_steps_per_second': 33.979, 'epoch': 4.0, 'step': 536}, {'loss': 0.0101, 'grad_norm': 0.3455486297607422, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.04226674884557724, 'eval_precision': 0.7076205287713841, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7199367088607596, 'eval_accuracy': 0.9892132723648728, 'eval_runtime': 5.5341, 'eval_samples_per_second': 273.396, 'eval_steps_per_second': 34.333, 'epoch': 5.0, 'step': 670}, {'loss': 0.0069, 'grad_norm': 1.5989006757736206, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04318810999393463, 'eval_precision': 0.7222222222222222, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7375886524822696, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.6551, 'eval_samples_per_second': 267.546, 'eval_steps_per_second': 33.598, 'epoch': 6.0, 'step': 804}, {'loss': 0.0048, 'grad_norm': 0.4358207583427429, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.04682059958577156, 'eval_precision': 0.7237048665620094, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7329093799682035, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 5.5919, 'eval_samples_per_second': 270.569, 'eval_steps_per_second': 33.978, 'epoch': 7.0, 'step': 938}, {'loss': 0.0032, 'grad_norm': 0.09622112661600113, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.049089107662439346, 'eval_precision': 0.7373417721518988, 'eval_recall': 0.750402576489533, 'eval_f1': 0.7438148443735035, 'eval_accuracy': 0.9901903310274749, 'eval_runtime': 5.8774, 'eval_samples_per_second': 257.427, 'eval_steps_per_second': 32.327, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0023, 'grad_norm': 0.7014458179473877, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.050114184617996216, 'eval_precision': 0.7375, 'eval_recall': 0.7600644122383253, 'eval_f1': 0.7486122125297383, 'eval_accuracy': 0.9903466604134912, 'eval_runtime': 5.5544, 'eval_samples_per_second': 272.396, 'eval_steps_per_second': 34.207, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0019, 'grad_norm': 0.5984876751899719, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.05159623548388481, 'eval_precision': 0.7394034536891679, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7488076311605723, 'eval_accuracy': 0.9901903310274749, 'eval_runtime': 5.5852, 'eval_samples_per_second': 270.896, 'eval_steps_per_second': 34.019, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0015, 'grad_norm': 0.022623106837272644, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05362487956881523, 'eval_precision': 0.7435897435897436, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7453815261044177, 'eval_accuracy': 0.9901903310274749, 'eval_runtime': 5.6256, 'eval_samples_per_second': 268.947, 'eval_steps_per_second': 33.774, 'epoch': 11.0, 'step': 1474}, {'loss': 0.0013, 'grad_norm': 0.10615912079811096, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05356736108660698, 'eval_precision': 0.747588424437299, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7481898632341111, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 5.8552, 'eval_samples_per_second': 258.402, 'eval_steps_per_second': 32.45, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1475.8218, 'train_samples_per_second': 69.683, 'train_steps_per_second': 1.09, 'total_flos': 1.3389246143264808e+16, 'train_loss': 0.02082696638593626, 'epoch': 12.0, 'step': 1608}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9902
  predict_f1                 =     0.7054
  predict_loss               =     0.0348
  predict_precision          =     0.6802
  predict_recall             =     0.7325
  predict_runtime            = 0:00:04.71
  predict_samples_per_second =    265.752
  predict_steps_per_second   =     33.325
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_06_202.json completed. F1: 0.7053854276663147
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_202.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:04, 1659.56 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2992.63 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 3823.20 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:00, 4710.21 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5299.10 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5880.36 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6163.45 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6535.28 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4794.43 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 3113.41 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3407.07 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 2732.93 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 4940.32 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3466.22 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0696, 'grad_norm': 0.7651100158691406, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.04340098425745964, 'eval_precision': 0.6359649122807017, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6666666666666667, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.4545, 'eval_samples_per_second': 616.419, 'eval_steps_per_second': 77.409, 'epoch': 1.0}
{'loss': 0.03, 'grad_norm': 0.8264626860618591, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04246736317873001, 'eval_precision': 0.6591639871382636, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.659694288012872, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 2.2988, 'eval_samples_per_second': 658.166, 'eval_steps_per_second': 82.651, 'epoch': 2.0}
{'loss': 0.0171, 'grad_norm': 0.27869993448257446, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.05245828628540039, 'eval_precision': 0.6711915535444947, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6931464174454828, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3618, 'eval_samples_per_second': 640.6, 'eval_steps_per_second': 80.445, 'epoch': 3.0}
{'loss': 0.0086, 'grad_norm': 0.18741796910762787, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.05648287385702133, 'eval_precision': 0.6661538461538462, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6813532651455547, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.8241, 'eval_samples_per_second': 535.74, 'eval_steps_per_second': 67.277, 'epoch': 4.0}
{'loss': 0.0055, 'grad_norm': 1.1469744443893433, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.06107942387461662, 'eval_precision': 0.6798780487804879, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6985121378230228, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3299, 'eval_samples_per_second': 649.38, 'eval_steps_per_second': 81.548, 'epoch': 5.0}
{'loss': 0.0025, 'grad_norm': 0.019136961549520493, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.07129501551389694, 'eval_precision': 0.6677018633540373, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6798418972332015, 'eval_accuracy': 0.9863993434165788, 'eval_runtime': 2.3709, 'eval_samples_per_second': 638.166, 'eval_steps_per_second': 80.14, 'epoch': 6.0}
{'loss': 0.0016, 'grad_norm': 0.015328873880207539, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.07849839329719543, 'eval_precision': 0.6777251184834123, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6842105263157895, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.3002, 'eval_samples_per_second': 657.759, 'eval_steps_per_second': 82.6, 'epoch': 7.0}
{'loss': 0.0008, 'grad_norm': 0.07717571407556534, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.07927478104829788, 'eval_precision': 0.6920684292379471, 'eval_recall': 0.71658615136876, 'eval_f1': 0.704113924050633, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.2922, 'eval_samples_per_second': 660.061, 'eval_steps_per_second': 82.889, 'epoch': 8.0}
{'loss': 0.0005, 'grad_norm': 0.055571120232343674, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.07745037227869034, 'eval_precision': 0.6943573667711599, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7037331215250199, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 2.2448, 'eval_samples_per_second': 673.992, 'eval_steps_per_second': 84.639, 'epoch': 9.0}
{'loss': 0.0003, 'grad_norm': 0.007439196575433016, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.08329717814922333, 'eval_precision': 0.6818897637795276, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6894904458598726, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3069, 'eval_samples_per_second': 655.863, 'eval_steps_per_second': 82.362, 'epoch': 10.0}
{'loss': 0.0002, 'grad_norm': 0.0010539265349507332, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.08363232016563416, 'eval_precision': 0.7087378640776699, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7070217917675544, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 2.2718, 'eval_samples_per_second': 666.004, 'eval_steps_per_second': 83.636, 'epoch': 11.0}
{'loss': 0.0002, 'grad_norm': 0.0012913144892081618, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.08462478220462799, 'eval_precision': 0.6971153846153846, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6987951807228916, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.2894, 'eval_samples_per_second': 660.863, 'eval_steps_per_second': 82.99, 'epoch': 12.0}
{'train_runtime': 504.4093, 'train_samples_per_second': 203.882, 'train_steps_per_second': 6.376, 'train_loss': 0.011410097659922284, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0114
  train_runtime            = 0:08:24.40
  train_samples            =       8570
  train_samples_per_second =    203.882
  train_steps_per_second   =      6.376
[{'loss': 0.0696, 'grad_norm': 0.7651100158691406, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04340098425745964, 'eval_precision': 0.6359649122807017, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6666666666666667, 'eval_accuracy': 0.9861648493375542, 'eval_runtime': 2.4545, 'eval_samples_per_second': 616.419, 'eval_steps_per_second': 77.409, 'epoch': 1.0, 'step': 268}, {'loss': 0.03, 'grad_norm': 0.8264626860618591, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04246736317873001, 'eval_precision': 0.6591639871382636, 'eval_recall': 0.6602254428341385, 'eval_f1': 0.659694288012872, 'eval_accuracy': 0.9878063078907258, 'eval_runtime': 2.2988, 'eval_samples_per_second': 658.166, 'eval_steps_per_second': 82.651, 'epoch': 2.0, 'step': 536}, {'loss': 0.0171, 'grad_norm': 0.27869993448257446, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.05245828628540039, 'eval_precision': 0.6711915535444947, 'eval_recall': 0.71658615136876, 'eval_f1': 0.6931464174454828, 'eval_accuracy': 0.9867120021886114, 'eval_runtime': 2.3618, 'eval_samples_per_second': 640.6, 'eval_steps_per_second': 80.445, 'epoch': 3.0, 'step': 804}, {'loss': 0.0086, 'grad_norm': 0.18741796910762787, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.05648287385702133, 'eval_precision': 0.6661538461538462, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6813532651455547, 'eval_accuracy': 0.9868683315746277, 'eval_runtime': 2.8241, 'eval_samples_per_second': 535.74, 'eval_steps_per_second': 67.277, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0055, 'grad_norm': 1.1469744443893433, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.06107942387461662, 'eval_precision': 0.6798780487804879, 'eval_recall': 0.7181964573268921, 'eval_f1': 0.6985121378230228, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3299, 'eval_samples_per_second': 649.38, 'eval_steps_per_second': 81.548, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0025, 'grad_norm': 0.019136961549520493, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.07129501551389694, 'eval_precision': 0.6677018633540373, 'eval_recall': 0.6924315619967794, 'eval_f1': 0.6798418972332015, 'eval_accuracy': 0.9863993434165788, 'eval_runtime': 2.3709, 'eval_samples_per_second': 638.166, 'eval_steps_per_second': 80.14, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0016, 'grad_norm': 0.015328873880207539, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.07849839329719543, 'eval_precision': 0.6777251184834123, 'eval_recall': 0.6908212560386473, 'eval_f1': 0.6842105263157895, 'eval_accuracy': 0.9869464962676359, 'eval_runtime': 2.3002, 'eval_samples_per_second': 657.759, 'eval_steps_per_second': 82.6, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0008, 'grad_norm': 0.07717571407556534, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.07927478104829788, 'eval_precision': 0.6920684292379471, 'eval_recall': 0.71658615136876, 'eval_f1': 0.704113924050633, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.2922, 'eval_samples_per_second': 660.061, 'eval_steps_per_second': 82.889, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0005, 'grad_norm': 0.055571120232343674, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.07745037227869034, 'eval_precision': 0.6943573667711599, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7037331215250199, 'eval_accuracy': 0.9876499785047094, 'eval_runtime': 2.2448, 'eval_samples_per_second': 673.992, 'eval_steps_per_second': 84.639, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0003, 'grad_norm': 0.007439196575433016, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.08329717814922333, 'eval_precision': 0.6818897637795276, 'eval_recall': 0.6972624798711755, 'eval_f1': 0.6894904458598726, 'eval_accuracy': 0.9868292492281237, 'eval_runtime': 2.3069, 'eval_samples_per_second': 655.863, 'eval_steps_per_second': 82.362, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0002, 'grad_norm': 0.0010539265349507332, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.08363232016563416, 'eval_precision': 0.7087378640776699, 'eval_recall': 0.7053140096618358, 'eval_f1': 0.7070217917675544, 'eval_accuracy': 0.9875327314651972, 'eval_runtime': 2.2718, 'eval_samples_per_second': 666.004, 'eval_steps_per_second': 83.636, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0002, 'grad_norm': 0.0012913144892081618, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.08462478220462799, 'eval_precision': 0.6971153846153846, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6987951807228916, 'eval_accuracy': 0.9874154844256849, 'eval_runtime': 2.2894, 'eval_samples_per_second': 660.863, 'eval_steps_per_second': 82.99, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 504.4093, 'train_samples_per_second': 203.882, 'train_steps_per_second': 6.376, 'total_flos': 3922873450278684.0, 'train_loss': 0.011410097659922284, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9895
  predict_f1                 =     0.6953
  predict_loss               =     0.0375
  predict_precision          =     0.7163
  predict_recall             =     0.6754
  predict_runtime            = 0:00:01.99
  predict_samples_per_second =    629.065
  predict_steps_per_second   =     78.884
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_05_202.json completed. F1: 0.6952595936794582
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_303.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_303.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:03, 2086.99 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 3031.57 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4100.68 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:00, 4714.20 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5424.53 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 6048.33 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6398.88 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6777.10 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4289.12 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7260.05 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3030.67 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 4932.36 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3274.61 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.061, 'grad_norm': 0.20747144520282745, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0}
{'eval_loss': 0.03776528686285019, 'eval_precision': 0.6656891495601173, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.6968534151957022, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 5.6307, 'eval_samples_per_second': 268.707, 'eval_steps_per_second': 33.744, 'epoch': 1.0}
{'loss': 0.0205, 'grad_norm': 0.686365008354187, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.04042442515492439, 'eval_precision': 0.664756446991404, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7035633055344958, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 5.775, 'eval_samples_per_second': 261.992, 'eval_steps_per_second': 32.901, 'epoch': 2.0}
{'loss': 0.0067, 'grad_norm': 0.4662937521934509, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0}
{'eval_loss': 0.04434962943196297, 'eval_precision': 0.7300319488817891, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7329591018444266, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.6065, 'eval_samples_per_second': 269.864, 'eval_steps_per_second': 33.889, 'epoch': 3.0}
{'loss': 0.0023, 'grad_norm': 0.18658103048801422, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}
{'eval_loss': 0.04803882911801338, 'eval_precision': 0.6966966966966966, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.721056721056721, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 5.4955, 'eval_samples_per_second': 275.315, 'eval_steps_per_second': 34.574, 'epoch': 4.0}
{'loss': 0.001, 'grad_norm': 0.10347001254558563, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.0604187548160553, 'eval_precision': 0.7337461300309598, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7482241515390686, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.575, 'eval_samples_per_second': 271.389, 'eval_steps_per_second': 34.081, 'epoch': 5.0}
{'loss': 0.0006, 'grad_norm': 0.011040977202355862, 'learning_rate': 2.5e-05, 'epoch': 6.0}
{'eval_loss': 0.055883947759866714, 'eval_precision': 0.7563694267515924, 'eval_recall': 0.7648953301127214, 'eval_f1': 0.7606084867894314, 'eval_accuracy': 0.9907374838785321, 'eval_runtime': 5.5155, 'eval_samples_per_second': 274.319, 'eval_steps_per_second': 34.448, 'epoch': 6.0}
{'loss': 0.0005, 'grad_norm': 0.0018092615064233541, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0}
{'eval_loss': 0.058639027178287506, 'eval_precision': 0.7210031347962382, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7307386814932486, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 5.5447, 'eval_samples_per_second': 272.875, 'eval_steps_per_second': 34.267, 'epoch': 7.0}
{'loss': 0.0001, 'grad_norm': 0.005002149846404791, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0}
{'eval_loss': 0.060337428003549576, 'eval_precision': 0.7416798732171157, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7476038338658147, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 5.5597, 'eval_samples_per_second': 272.135, 'eval_steps_per_second': 34.174, 'epoch': 8.0}
{'loss': 0.0, 'grad_norm': 0.0010814908891916275, 'learning_rate': 1.25e-05, 'epoch': 9.0}
{'eval_loss': 0.06262879073619843, 'eval_precision': 0.744, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7463884430176565, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 5.563, 'eval_samples_per_second': 271.974, 'eval_steps_per_second': 34.154, 'epoch': 9.0}
{'loss': 0.0, 'grad_norm': 0.00017891092284116894, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0}
{'eval_loss': 0.06414178013801575, 'eval_precision': 0.7483974358974359, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7502008032128514, 'eval_accuracy': 0.9903466604134912, 'eval_runtime': 5.5276, 'eval_samples_per_second': 273.719, 'eval_steps_per_second': 34.373, 'epoch': 10.0}
{'loss': 0.0, 'grad_norm': 0.0017886959249153733, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.06496305763721466, 'eval_precision': 0.7508038585209004, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.751407884151247, 'eval_accuracy': 0.9905420721460116, 'eval_runtime': 5.5908, 'eval_samples_per_second': 270.624, 'eval_steps_per_second': 33.985, 'epoch': 11.0}
{'loss': 0.0, 'grad_norm': 0.0013613927876576781, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06499428302049637, 'eval_precision': 0.7520128824476651, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7520128824476651, 'eval_accuracy': 0.9903857427599954, 'eval_runtime': 5.5005, 'eval_samples_per_second': 275.065, 'eval_steps_per_second': 34.542, 'epoch': 12.0}
{'train_runtime': 1387.1761, 'train_samples_per_second': 74.136, 'train_steps_per_second': 2.318, 'train_loss': 0.007735707400516092, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0077
  train_runtime            = 0:23:07.17
  train_samples            =       8570
  train_samples_per_second =     74.136
  train_steps_per_second   =      2.318
[{'loss': 0.061, 'grad_norm': 0.20747144520282745, 'learning_rate': 4.5833333333333334e-05, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.03776528686285019, 'eval_precision': 0.6656891495601173, 'eval_recall': 0.7310789049919485, 'eval_f1': 0.6968534151957022, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 5.6307, 'eval_samples_per_second': 268.707, 'eval_steps_per_second': 33.744, 'epoch': 1.0, 'step': 268}, {'loss': 0.0205, 'grad_norm': 0.686365008354187, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.04042442515492439, 'eval_precision': 0.664756446991404, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7035633055344958, 'eval_accuracy': 0.9878453902372298, 'eval_runtime': 5.775, 'eval_samples_per_second': 261.992, 'eval_steps_per_second': 32.901, 'epoch': 2.0, 'step': 536}, {'loss': 0.0067, 'grad_norm': 0.4662937521934509, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.04434962943196297, 'eval_precision': 0.7300319488817891, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7329591018444266, 'eval_accuracy': 0.9898385899089381, 'eval_runtime': 5.6065, 'eval_samples_per_second': 269.864, 'eval_steps_per_second': 33.889, 'epoch': 3.0, 'step': 804}, {'loss': 0.0023, 'grad_norm': 0.18658103048801422, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04803882911801338, 'eval_precision': 0.6966966966966966, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.721056721056721, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 5.4955, 'eval_samples_per_second': 275.315, 'eval_steps_per_second': 34.574, 'epoch': 4.0, 'step': 1072}, {'loss': 0.001, 'grad_norm': 0.10347001254558563, 'learning_rate': 2.916666666666667e-05, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.0604187548160553, 'eval_precision': 0.7337461300309598, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7482241515390686, 'eval_accuracy': 0.98976042521593, 'eval_runtime': 5.575, 'eval_samples_per_second': 271.389, 'eval_steps_per_second': 34.081, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0006, 'grad_norm': 0.011040977202355862, 'learning_rate': 2.5e-05, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.055883947759866714, 'eval_precision': 0.7563694267515924, 'eval_recall': 0.7648953301127214, 'eval_f1': 0.7606084867894314, 'eval_accuracy': 0.9907374838785321, 'eval_runtime': 5.5155, 'eval_samples_per_second': 274.319, 'eval_steps_per_second': 34.448, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0005, 'grad_norm': 0.0018092615064233541, 'learning_rate': 2.0833333333333336e-05, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.058639027178287506, 'eval_precision': 0.7210031347962382, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7307386814932486, 'eval_accuracy': 0.9898776722554422, 'eval_runtime': 5.5447, 'eval_samples_per_second': 272.875, 'eval_steps_per_second': 34.267, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0001, 'grad_norm': 0.005002149846404791, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.060337428003549576, 'eval_precision': 0.7416798732171157, 'eval_recall': 0.7536231884057971, 'eval_f1': 0.7476038338658147, 'eval_accuracy': 0.990229413373979, 'eval_runtime': 5.5597, 'eval_samples_per_second': 272.135, 'eval_steps_per_second': 34.174, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0, 'grad_norm': 0.0010814908891916275, 'learning_rate': 1.25e-05, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06262879073619843, 'eval_precision': 0.744, 'eval_recall': 0.748792270531401, 'eval_f1': 0.7463884430176565, 'eval_accuracy': 0.9903075780669871, 'eval_runtime': 5.563, 'eval_samples_per_second': 271.974, 'eval_steps_per_second': 34.154, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0, 'grad_norm': 0.00017891092284116894, 'learning_rate': 8.333333333333334e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06414178013801575, 'eval_precision': 0.7483974358974359, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7502008032128514, 'eval_accuracy': 0.9903466604134912, 'eval_runtime': 5.5276, 'eval_samples_per_second': 273.719, 'eval_steps_per_second': 34.373, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0, 'grad_norm': 0.0017886959249153733, 'learning_rate': 4.166666666666667e-06, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06496305763721466, 'eval_precision': 0.7508038585209004, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.751407884151247, 'eval_accuracy': 0.9905420721460116, 'eval_runtime': 5.5908, 'eval_samples_per_second': 270.624, 'eval_steps_per_second': 33.985, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0, 'grad_norm': 0.0013613927876576781, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06499428302049637, 'eval_precision': 0.7520128824476651, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7520128824476651, 'eval_accuracy': 0.9903857427599954, 'eval_runtime': 5.5005, 'eval_samples_per_second': 275.065, 'eval_steps_per_second': 34.542, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1387.1761, 'train_samples_per_second': 74.136, 'train_steps_per_second': 2.318, 'total_flos': 1.1778916405982184e+16, 'train_loss': 0.007735707400516092, 'epoch': 12.0, 'step': 3216}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9896
  predict_f1                 =     0.7097
  predict_loss               =     0.0339
  predict_precision          =     0.6865
  predict_recall             =     0.7346
  predict_runtime            = 0:00:04.66
  predict_samples_per_second =     268.34
  predict_steps_per_second   =      33.65
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_03_303.json completed. F1: 0.7097457627118644
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_202.json
03121434_elsa-polarity_nb-bert-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:04, 1827.21 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2940.43 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:01, 4050.78 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 3954.76 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 4744.32 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5484.79 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 5955.78 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6455.56 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4769.38 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7267.63 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 4181.42 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 4293.07 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 2783.56 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert-large/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1351, 'grad_norm': 0.6055886745452881, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.047161296010017395, 'eval_precision': 0.537890044576523, 'eval_recall': 0.5829307568438004, 'eval_f1': 0.5595054095826894, 'eval_accuracy': 0.9844452260913745, 'eval_runtime': 4.7163, 'eval_samples_per_second': 320.799, 'eval_steps_per_second': 40.285, 'epoch': 1.0}
{'loss': 0.0403, 'grad_norm': 0.4173235297203064, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.03975672274827957, 'eval_precision': 0.6791862284820032, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6888888888888889, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.5286, 'eval_samples_per_second': 334.101, 'eval_steps_per_second': 41.956, 'epoch': 2.0}
{'loss': 0.029, 'grad_norm': 0.5067043900489807, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.03891206905245781, 'eval_precision': 0.6838810641627543, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6936507936507937, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5175, 'eval_samples_per_second': 334.916, 'eval_steps_per_second': 42.058, 'epoch': 3.0}
{'loss': 0.0218, 'grad_norm': 0.5554532408714294, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.03980373218655586, 'eval_precision': 0.7191558441558441, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7162489894907031, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.7445, 'eval_samples_per_second': 318.894, 'eval_steps_per_second': 40.046, 'epoch': 4.0}
{'loss': 0.0157, 'grad_norm': 0.6873116493225098, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.041522178798913956, 'eval_precision': 0.7127496159754224, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7295597484276729, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5129, 'eval_samples_per_second': 335.264, 'eval_steps_per_second': 42.102, 'epoch': 5.0}
{'loss': 0.0111, 'grad_norm': 0.4326377213001251, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04328930377960205, 'eval_precision': 0.7232, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.725521669341894, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6042, 'eval_samples_per_second': 328.613, 'eval_steps_per_second': 41.267, 'epoch': 6.0}
{'loss': 0.0076, 'grad_norm': 0.48744192719459534, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.04641089215874672, 'eval_precision': 0.7354330708661417, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7436305732484078, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 4.5385, 'eval_samples_per_second': 333.371, 'eval_steps_per_second': 41.864, 'epoch': 7.0}
{'loss': 0.0062, 'grad_norm': 0.3202025294303894, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.049159809947013855, 'eval_precision': 0.7108626198083067, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7137129109863672, 'eval_accuracy': 0.9892523547113768, 'eval_runtime': 4.5236, 'eval_samples_per_second': 334.471, 'eval_steps_per_second': 42.002, 'epoch': 8.0}
{'loss': 0.0047, 'grad_norm': 0.2741537392139435, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.051046982407569885, 'eval_precision': 0.7245222929936306, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7285828662930345, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.8228, 'eval_samples_per_second': 313.719, 'eval_steps_per_second': 39.396, 'epoch': 9.0}
{'loss': 0.0038, 'grad_norm': 0.4823770225048065, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.052474189549684525, 'eval_precision': 0.7199367088607594, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7262569832402234, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.5709, 'eval_samples_per_second': 331.01, 'eval_steps_per_second': 41.568, 'epoch': 10.0}
{'loss': 0.0033, 'grad_norm': 0.2230423539876938, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05409684032201767, 'eval_precision': 0.7196850393700788, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7277070063694268, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.6303, 'eval_samples_per_second': 326.763, 'eval_steps_per_second': 41.034, 'epoch': 11.0}
{'loss': 0.003, 'grad_norm': 0.23804856836795807, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05447864904999733, 'eval_precision': 0.728, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7303370786516855, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.6096, 'eval_samples_per_second': 328.23, 'eval_steps_per_second': 41.219, 'epoch': 12.0}
{'train_runtime': 1190.9489, 'train_samples_per_second': 86.351, 'train_steps_per_second': 1.35, 'train_loss': 0.02345779328722859, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0235
  train_runtime            = 0:19:50.94
  train_samples            =       8570
  train_samples_per_second =     86.351
  train_steps_per_second   =       1.35
[{'loss': 0.1351, 'grad_norm': 0.6055886745452881, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 134}, {'eval_loss': 0.047161296010017395, 'eval_precision': 0.537890044576523, 'eval_recall': 0.5829307568438004, 'eval_f1': 0.5595054095826894, 'eval_accuracy': 0.9844452260913745, 'eval_runtime': 4.7163, 'eval_samples_per_second': 320.799, 'eval_steps_per_second': 40.285, 'epoch': 1.0, 'step': 134}, {'loss': 0.0403, 'grad_norm': 0.4173235297203064, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 268}, {'eval_loss': 0.03975672274827957, 'eval_precision': 0.6791862284820032, 'eval_recall': 0.6988727858293076, 'eval_f1': 0.6888888888888889, 'eval_accuracy': 0.9883925430882871, 'eval_runtime': 4.5286, 'eval_samples_per_second': 334.101, 'eval_steps_per_second': 41.956, 'epoch': 2.0, 'step': 268}, {'loss': 0.029, 'grad_norm': 0.5067043900489807, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 402}, {'eval_loss': 0.03891206905245781, 'eval_precision': 0.6838810641627543, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6936507936507937, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 4.5175, 'eval_samples_per_second': 334.916, 'eval_steps_per_second': 42.058, 'epoch': 3.0, 'step': 402}, {'loss': 0.0218, 'grad_norm': 0.5554532408714294, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 536}, {'eval_loss': 0.03980373218655586, 'eval_precision': 0.7191558441558441, 'eval_recall': 0.7133655394524959, 'eval_f1': 0.7162489894907031, 'eval_accuracy': 0.9890569429788565, 'eval_runtime': 4.7445, 'eval_samples_per_second': 318.894, 'eval_steps_per_second': 40.046, 'epoch': 4.0, 'step': 536}, {'loss': 0.0157, 'grad_norm': 0.6873116493225098, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 670}, {'eval_loss': 0.041522178798913956, 'eval_precision': 0.7127496159754224, 'eval_recall': 0.7471819645732689, 'eval_f1': 0.7295597484276729, 'eval_accuracy': 0.9894086840973932, 'eval_runtime': 4.5129, 'eval_samples_per_second': 335.264, 'eval_steps_per_second': 42.102, 'epoch': 5.0, 'step': 670}, {'loss': 0.0111, 'grad_norm': 0.4326377213001251, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 804}, {'eval_loss': 0.04328930377960205, 'eval_precision': 0.7232, 'eval_recall': 0.7278582930756844, 'eval_f1': 0.725521669341894, 'eval_accuracy': 0.9895259311369055, 'eval_runtime': 4.6042, 'eval_samples_per_second': 328.613, 'eval_steps_per_second': 41.267, 'epoch': 6.0, 'step': 804}, {'loss': 0.0076, 'grad_norm': 0.48744192719459534, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 938}, {'eval_loss': 0.04641089215874672, 'eval_precision': 0.7354330708661417, 'eval_recall': 0.7520128824476651, 'eval_f1': 0.7436305732484078, 'eval_accuracy': 0.9897995075624341, 'eval_runtime': 4.5385, 'eval_samples_per_second': 333.371, 'eval_steps_per_second': 41.864, 'epoch': 7.0, 'step': 938}, {'loss': 0.0062, 'grad_norm': 0.3202025294303894, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 1072}, {'eval_loss': 0.049159809947013855, 'eval_precision': 0.7108626198083067, 'eval_recall': 0.71658615136876, 'eval_f1': 0.7137129109863672, 'eval_accuracy': 0.9892523547113768, 'eval_runtime': 4.5236, 'eval_samples_per_second': 334.471, 'eval_steps_per_second': 42.002, 'epoch': 8.0, 'step': 1072}, {'loss': 0.0047, 'grad_norm': 0.2741537392139435, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 1206}, {'eval_loss': 0.051046982407569885, 'eval_precision': 0.7245222929936306, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7285828662930345, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.8228, 'eval_samples_per_second': 313.719, 'eval_steps_per_second': 39.396, 'epoch': 9.0, 'step': 1206}, {'loss': 0.0038, 'grad_norm': 0.4823770225048065, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 1340}, {'eval_loss': 0.052474189549684525, 'eval_precision': 0.7199367088607594, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7262569832402234, 'eval_accuracy': 0.9893696017508892, 'eval_runtime': 4.5709, 'eval_samples_per_second': 331.01, 'eval_steps_per_second': 41.568, 'epoch': 10.0, 'step': 1340}, {'loss': 0.0033, 'grad_norm': 0.2230423539876938, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 1474}, {'eval_loss': 0.05409684032201767, 'eval_precision': 0.7196850393700788, 'eval_recall': 0.7359098228663447, 'eval_f1': 0.7277070063694268, 'eval_accuracy': 0.9894477664438973, 'eval_runtime': 4.6303, 'eval_samples_per_second': 326.763, 'eval_steps_per_second': 41.034, 'epoch': 11.0, 'step': 1474}, {'loss': 0.003, 'grad_norm': 0.23804856836795807, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 1608}, {'eval_loss': 0.05447864904999733, 'eval_precision': 0.728, 'eval_recall': 0.7326892109500805, 'eval_f1': 0.7303370786516855, 'eval_accuracy': 0.9894868487904014, 'eval_runtime': 4.6096, 'eval_samples_per_second': 328.23, 'eval_steps_per_second': 41.219, 'epoch': 12.0, 'step': 1608}, {'train_runtime': 1190.9489, 'train_samples_per_second': 86.351, 'train_steps_per_second': 1.35, 'total_flos': 1.2881569369077948e+16, 'train_loss': 0.02345779328722859, 'epoch': 12.0, 'step': 1608}]

Evaluation, NbAiLab/nb-bert-large
***** predict metrics *****
  predict_accuracy           =     0.9896
  predict_f1                 =     0.6975
  predict_loss               =     0.0363
  predict_precision          =     0.6694
  predict_recall             =     0.7281
  predict_runtime            = 0:00:05.42
  predict_samples_per_second =    230.812
  predict_steps_per_second   =     28.944
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert-large_07_202.json completed. F1: 0.6974789915966387
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_101.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/cluster/shared/nlpl/software/eb/packages/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_101.json
03121434_elsa-polarity_norbert3-large Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:04, 1599.75 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:02, 2372.91 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:01<00:01, 3427.28 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:01<00:01, 4348.40 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:01<00:00, 5084.15 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:01<00:00, 5734.20 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6145.44 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 6536.03 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 4470.14 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 6909.72 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3065.85 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 6762.53 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 3698.89 examples/s]
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_norbert3-large/checkpoint-268 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_norbert3-large Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.0945, 'grad_norm': 0.5977490544319153, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.04377477243542671, 'eval_precision': 0.6219135802469136, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6351457840819543, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 6.8516, 'eval_samples_per_second': 220.825, 'eval_steps_per_second': 27.731, 'epoch': 1.0}
{'loss': 0.0285, 'grad_norm': 0.46560075879096985, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.038012273609638214, 'eval_precision': 0.6762481089258698, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6973478939157566, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 5.6465, 'eval_samples_per_second': 267.953, 'eval_steps_per_second': 33.649, 'epoch': 2.0}
{'loss': 0.0175, 'grad_norm': 0.17928403615951538, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.039003465324640274, 'eval_precision': 0.7317460317460317, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7370103916866507, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.6352, 'eval_samples_per_second': 268.492, 'eval_steps_per_second': 33.717, 'epoch': 3.0}
{'loss': 0.0091, 'grad_norm': 0.5957249402999878, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04450978711247444, 'eval_precision': 0.7035881435257411, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7147385103011094, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 5.6024, 'eval_samples_per_second': 270.061, 'eval_steps_per_second': 33.914, 'epoch': 4.0}
{'loss': 0.0048, 'grad_norm': 0.3217027187347412, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.048474106937646866, 'eval_precision': 0.6845329249617151, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7017268445839874, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 5.672, 'eval_samples_per_second': 266.748, 'eval_steps_per_second': 33.498, 'epoch': 5.0}
{'loss': 0.0027, 'grad_norm': 0.9969228506088257, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.05267878621816635, 'eval_precision': 0.7216981132075472, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.730310262529833, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.6034, 'eval_samples_per_second': 270.017, 'eval_steps_per_second': 33.908, 'epoch': 6.0}
{'loss': 0.0015, 'grad_norm': 1.5033581256866455, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.0553615503013134, 'eval_precision': 0.7405660377358491, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7494033412887827, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.5881, 'eval_samples_per_second': 270.752, 'eval_steps_per_second': 34.001, 'epoch': 7.0}
{'loss': 0.001, 'grad_norm': 0.15463946759700775, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05786925554275513, 'eval_precision': 0.7394695787831513, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7511885895404121, 'eval_accuracy': 0.9901121663344667, 'eval_runtime': 5.6265, 'eval_samples_per_second': 268.905, 'eval_steps_per_second': 33.769, 'epoch': 8.0}
{'loss': 0.0007, 'grad_norm': 0.10291828215122223, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.06037045642733574, 'eval_precision': 0.7429467084639498, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7529785544082604, 'eval_accuracy': 0.9901121663344667, 'eval_runtime': 5.6973, 'eval_samples_per_second': 265.564, 'eval_steps_per_second': 33.349, 'epoch': 9.0}
{'loss': 0.0005, 'grad_norm': 0.3936459720134735, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.06402979046106339, 'eval_precision': 0.7286821705426356, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7424960505529226, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 5.7401, 'eval_samples_per_second': 263.585, 'eval_steps_per_second': 33.101, 'epoch': 10.0}
{'loss': 0.0006, 'grad_norm': 0.10213807970285416, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.06336501240730286, 'eval_precision': 0.7330246913580247, 'eval_recall': 0.7648953301127214, 'eval_f1': 0.7486209613869188, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 5.6267, 'eval_samples_per_second': 268.897, 'eval_steps_per_second': 33.768, 'epoch': 11.0}
{'loss': 0.0003, 'grad_norm': 0.04900967329740524, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.06346708536148071, 'eval_precision': 0.7476489028213166, 'eval_recall': 0.7681159420289855, 'eval_f1': 0.7577442414614773, 'eval_accuracy': 0.9901512486809708, 'eval_runtime': 5.644, 'eval_samples_per_second': 268.073, 'eval_steps_per_second': 33.664, 'epoch': 12.0}
{'train_runtime': 1215.7152, 'train_samples_per_second': 84.592, 'train_steps_per_second': 2.645, 'train_loss': 0.013463174572112548, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0135
  train_runtime            = 0:20:15.71
  train_samples            =       8570
  train_samples_per_second =     84.592
  train_steps_per_second   =      2.645
[{'loss': 0.0945, 'grad_norm': 0.5977490544319153, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.04377477243542671, 'eval_precision': 0.6219135802469136, 'eval_recall': 0.6489533011272142, 'eval_f1': 0.6351457840819543, 'eval_accuracy': 0.9867510845351155, 'eval_runtime': 6.8516, 'eval_samples_per_second': 220.825, 'eval_steps_per_second': 27.731, 'epoch': 1.0, 'step': 268}, {'loss': 0.0285, 'grad_norm': 0.46560075879096985, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.038012273609638214, 'eval_precision': 0.6762481089258698, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.6973478939157566, 'eval_accuracy': 0.9885097901277993, 'eval_runtime': 5.6465, 'eval_samples_per_second': 267.953, 'eval_steps_per_second': 33.649, 'epoch': 2.0, 'step': 536}, {'loss': 0.0175, 'grad_norm': 0.17928403615951538, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.039003465324640274, 'eval_precision': 0.7317460317460317, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7370103916866507, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.6352, 'eval_samples_per_second': 268.492, 'eval_steps_per_second': 33.717, 'epoch': 3.0, 'step': 804}, {'loss': 0.0091, 'grad_norm': 0.5957249402999878, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04450978711247444, 'eval_precision': 0.7035881435257411, 'eval_recall': 0.7262479871175523, 'eval_f1': 0.7147385103011094, 'eval_accuracy': 0.9889787782858482, 'eval_runtime': 5.6024, 'eval_samples_per_second': 270.061, 'eval_steps_per_second': 33.914, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0048, 'grad_norm': 0.3217027187347412, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.048474106937646866, 'eval_precision': 0.6845329249617151, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7017268445839874, 'eval_accuracy': 0.988861531246336, 'eval_runtime': 5.672, 'eval_samples_per_second': 266.748, 'eval_steps_per_second': 33.498, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0027, 'grad_norm': 0.9969228506088257, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.05267878621816635, 'eval_precision': 0.7216981132075472, 'eval_recall': 0.7391304347826086, 'eval_f1': 0.730310262529833, 'eval_accuracy': 0.9896822605229217, 'eval_runtime': 5.6034, 'eval_samples_per_second': 270.017, 'eval_steps_per_second': 33.908, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0015, 'grad_norm': 1.5033581256866455, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.0553615503013134, 'eval_precision': 0.7405660377358491, 'eval_recall': 0.7584541062801933, 'eval_f1': 0.7494033412887827, 'eval_accuracy': 0.9899949192949544, 'eval_runtime': 5.5881, 'eval_samples_per_second': 270.752, 'eval_steps_per_second': 34.001, 'epoch': 7.0, 'step': 1876}, {'loss': 0.001, 'grad_norm': 0.15463946759700775, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05786925554275513, 'eval_precision': 0.7394695787831513, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7511885895404121, 'eval_accuracy': 0.9901121663344667, 'eval_runtime': 5.6265, 'eval_samples_per_second': 268.905, 'eval_steps_per_second': 33.769, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0007, 'grad_norm': 0.10291828215122223, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.06037045642733574, 'eval_precision': 0.7429467084639498, 'eval_recall': 0.7632850241545893, 'eval_f1': 0.7529785544082604, 'eval_accuracy': 0.9901121663344667, 'eval_runtime': 5.6973, 'eval_samples_per_second': 265.564, 'eval_steps_per_second': 33.349, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0005, 'grad_norm': 0.3936459720134735, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.06402979046106339, 'eval_precision': 0.7286821705426356, 'eval_recall': 0.7568438003220612, 'eval_f1': 0.7424960505529226, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 5.7401, 'eval_samples_per_second': 263.585, 'eval_steps_per_second': 33.101, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0006, 'grad_norm': 0.10213807970285416, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.06336501240730286, 'eval_precision': 0.7330246913580247, 'eval_recall': 0.7648953301127214, 'eval_f1': 0.7486209613869188, 'eval_accuracy': 0.9900340016414586, 'eval_runtime': 5.6267, 'eval_samples_per_second': 268.897, 'eval_steps_per_second': 33.768, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0003, 'grad_norm': 0.04900967329740524, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.06346708536148071, 'eval_precision': 0.7476489028213166, 'eval_recall': 0.7681159420289855, 'eval_f1': 0.7577442414614773, 'eval_accuracy': 0.9901512486809708, 'eval_runtime': 5.644, 'eval_samples_per_second': 268.073, 'eval_steps_per_second': 33.664, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 1215.7152, 'train_samples_per_second': 84.592, 'train_steps_per_second': 2.645, 'total_flos': 1.179179319311082e+16, 'train_loss': 0.013463174572112548, 'epoch': 12.0, 'step': 3216}]

Evaluation, ltg/norbert3-large
***** predict metrics *****
  predict_accuracy           =     0.9905
  predict_f1                 =     0.7149
  predict_loss               =     0.0316
  predict_precision          =     0.6847
  predict_recall             =     0.7478
  predict_runtime            = 0:00:04.72
  predict_samples_per_second =    265.011
  predict_steps_per_second   =     33.232
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_norbert3-large_00_101.json completed. F1: 0.7148846960167715
/cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_202.json
/cluster/shared/nlpl/software/eb/packages/nlpl-python-candy/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/pydot.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Numpy: 1.24.4
PyTorch: 2.1.2
Transformers: 4.38.2



***Loading config file: /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_202.json
03121434_elsa-polarity_nb-bert_base Our label2id: {'O': 0, 'B-Neg': 1, 'I-Neg': 2, 'B-Neu': 3, 'I-Neu': 4, 'B-Pos': 5, 'I-Pos': 6}
Running tokenizer on train dataset:   0%|          | 0/8570 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Running tokenizer on train dataset:  12%|█▏        | 1000/8570 [00:00<00:02, 3316.57 examples/s]Running tokenizer on train dataset:  23%|██▎       | 2000/8570 [00:00<00:01, 4990.80 examples/s]Running tokenizer on train dataset:  35%|███▌      | 3000/8570 [00:00<00:00, 5740.65 examples/s]Running tokenizer on train dataset:  47%|████▋     | 4000/8570 [00:00<00:00, 6258.43 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 5000/8570 [00:00<00:00, 6528.00 examples/s]Running tokenizer on train dataset:  70%|███████   | 6000/8570 [00:00<00:00, 6813.24 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 7000/8570 [00:01<00:00, 6869.14 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 8000/8570 [00:01<00:00, 7051.60 examples/s]Running tokenizer on train dataset: 100%|██████████| 8570/8570 [00:01<00:00, 6072.67 examples/s]
Running tokenizer on validation dataset:   0%|          | 0/1513 [00:00<?, ? examples/s]Running tokenizer on validation dataset:  66%|██████▌   | 1000/1513 [00:00<00:00, 7048.02 examples/s]Running tokenizer on validation dataset: 100%|██████████| 1513/1513 [00:00<00:00, 3222.95 examples/s]
Running tokenizer on test dataset:   0%|          | 0/1252 [00:00<?, ? examples/s]Running tokenizer on test dataset:  80%|███████▉  | 1000/1252 [00:00<00:00, 7105.32 examples/s]Running tokenizer on test dataset: 100%|██████████| 1252/1252 [00:00<00:00, 4265.06 examples/s]
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/cluster/work/users/egilron/venvs/transformers_a100/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Checkpoint destination directory /cluster/work/users/egilron/finetunes/03121434_elsa-polarity_nb-bert_base/checkpoint-536 already exists and is non-empty. Saving will proceed but saved results may be invalid.
03121434_elsa-polarity_nb-bert_base Ready to train. Train dataset labels are now: ['sent_id', 'tokens', 'elsa_labels', 'entity', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']
{'loss': 0.1189, 'grad_norm': 0.41718974709510803, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0}
{'eval_loss': 0.049819644540548325, 'eval_precision': 0.55359765051395, 'eval_recall': 0.607085346215781, 'eval_f1': 0.5791090629800306, 'eval_accuracy': 0.982803767538203, 'eval_runtime': 2.485, 'eval_samples_per_second': 608.857, 'eval_steps_per_second': 76.459, 'epoch': 1.0}
{'loss': 0.04, 'grad_norm': 0.7658506631851196, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0}
{'eval_loss': 0.0434989258646965, 'eval_precision': 0.648062015503876, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.660347551342812, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2771, 'eval_samples_per_second': 664.447, 'eval_steps_per_second': 83.44, 'epoch': 2.0}
{'loss': 0.03, 'grad_norm': 0.6973714232444763, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0}
{'eval_loss': 0.044196393340826035, 'eval_precision': 0.6635367762128326, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6730158730158731, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3175, 'eval_samples_per_second': 652.856, 'eval_steps_per_second': 81.985, 'epoch': 3.0}
{'loss': 0.0228, 'grad_norm': 0.2847248613834381, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0}
{'eval_loss': 0.04321527108550072, 'eval_precision': 0.6973478939157566, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7083993660855784, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 2.2931, 'eval_samples_per_second': 659.802, 'eval_steps_per_second': 82.857, 'epoch': 4.0}
{'loss': 0.0179, 'grad_norm': 1.3166930675506592, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0}
{'eval_loss': 0.04796865954995155, 'eval_precision': 0.669218989280245, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6860282574568288, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3457, 'eval_samples_per_second': 645.023, 'eval_steps_per_second': 81.001, 'epoch': 5.0}
{'loss': 0.0136, 'grad_norm': 0.6475549340248108, 'learning_rate': 5e-06, 'epoch': 6.0}
{'eval_loss': 0.04943716153502464, 'eval_precision': 0.6765163297045101, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6882911392405063, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.3196, 'eval_samples_per_second': 652.279, 'eval_steps_per_second': 81.912, 'epoch': 6.0}
{'loss': 0.0102, 'grad_norm': 1.982527494430542, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0}
{'eval_loss': 0.05060989409685135, 'eval_precision': 0.7, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.710547184773989, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3158, 'eval_samples_per_second': 653.349, 'eval_steps_per_second': 82.047, 'epoch': 7.0}
{'loss': 0.0077, 'grad_norm': 0.24639643728733063, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0}
{'eval_loss': 0.05450328439474106, 'eval_precision': 0.7122641509433962, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.720763723150358, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 2.326, 'eval_samples_per_second': 650.468, 'eval_steps_per_second': 81.685, 'epoch': 8.0}
{'loss': 0.0061, 'grad_norm': 0.5217588543891907, 'learning_rate': 2.5e-06, 'epoch': 9.0}
{'eval_loss': 0.05531409755349159, 'eval_precision': 0.7248427672955975, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7334924423229913, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 2.2997, 'eval_samples_per_second': 657.909, 'eval_steps_per_second': 82.619, 'epoch': 9.0}
{'loss': 0.0052, 'grad_norm': 0.28595301508903503, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0}
{'eval_loss': 0.0566956028342247, 'eval_precision': 0.7078825347758887, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.722397476340694, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 2.2686, 'eval_samples_per_second': 666.941, 'eval_steps_per_second': 83.753, 'epoch': 10.0}
{'loss': 0.0044, 'grad_norm': 0.12233524024486542, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0}
{'eval_loss': 0.05836883559823036, 'eval_precision': 0.7026194144838213, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7181102362204724, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 2.282, 'eval_samples_per_second': 663.029, 'eval_steps_per_second': 83.262, 'epoch': 11.0}
{'loss': 0.0039, 'grad_norm': 0.11783494055271149, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.05816805735230446, 'eval_precision': 0.710077519379845, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7235387045813586, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.2937, 'eval_samples_per_second': 659.645, 'eval_steps_per_second': 82.837, 'epoch': 12.0}
{'train_runtime': 617.342, 'train_samples_per_second': 166.585, 'train_steps_per_second': 5.209, 'train_loss': 0.023381381596795363, 'epoch': 12.0}
***** train metrics *****
  epoch                    =       12.0
  train_loss               =     0.0234
  train_runtime            = 0:10:17.34
  train_samples            =       8570
  train_samples_per_second =    166.585
  train_steps_per_second   =      5.209
[{'loss': 0.1189, 'grad_norm': 0.41718974709510803, 'learning_rate': 9.166666666666666e-06, 'epoch': 1.0, 'step': 268}, {'eval_loss': 0.049819644540548325, 'eval_precision': 0.55359765051395, 'eval_recall': 0.607085346215781, 'eval_f1': 0.5791090629800306, 'eval_accuracy': 0.982803767538203, 'eval_runtime': 2.485, 'eval_samples_per_second': 608.857, 'eval_steps_per_second': 76.459, 'epoch': 1.0, 'step': 268}, {'loss': 0.04, 'grad_norm': 0.7658506631851196, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.0, 'step': 536}, {'eval_loss': 0.0434989258646965, 'eval_precision': 0.648062015503876, 'eval_recall': 0.6731078904991948, 'eval_f1': 0.660347551342812, 'eval_accuracy': 0.9860866846445461, 'eval_runtime': 2.2771, 'eval_samples_per_second': 664.447, 'eval_steps_per_second': 83.44, 'epoch': 2.0, 'step': 536}, {'loss': 0.03, 'grad_norm': 0.6973714232444763, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.0, 'step': 804}, {'eval_loss': 0.044196393340826035, 'eval_precision': 0.6635367762128326, 'eval_recall': 0.6827697262479872, 'eval_f1': 0.6730158730158731, 'eval_accuracy': 0.9866338374956032, 'eval_runtime': 2.3175, 'eval_samples_per_second': 652.856, 'eval_steps_per_second': 81.985, 'epoch': 3.0, 'step': 804}, {'loss': 0.0228, 'grad_norm': 0.2847248613834381, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.0, 'step': 1072}, {'eval_loss': 0.04321527108550072, 'eval_precision': 0.6973478939157566, 'eval_recall': 0.7198067632850241, 'eval_f1': 0.7083993660855784, 'eval_accuracy': 0.9877672255442217, 'eval_runtime': 2.2931, 'eval_samples_per_second': 659.802, 'eval_steps_per_second': 82.857, 'epoch': 4.0, 'step': 1072}, {'loss': 0.0179, 'grad_norm': 1.3166930675506592, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.0, 'step': 1340}, {'eval_loss': 0.04796865954995155, 'eval_precision': 0.669218989280245, 'eval_recall': 0.7037037037037037, 'eval_f1': 0.6860282574568288, 'eval_accuracy': 0.9864384257630828, 'eval_runtime': 2.3457, 'eval_samples_per_second': 645.023, 'eval_steps_per_second': 81.001, 'epoch': 5.0, 'step': 1340}, {'loss': 0.0136, 'grad_norm': 0.6475549340248108, 'learning_rate': 5e-06, 'epoch': 6.0, 'step': 1608}, {'eval_loss': 0.04943716153502464, 'eval_precision': 0.6765163297045101, 'eval_recall': 0.7004830917874396, 'eval_f1': 0.6882911392405063, 'eval_accuracy': 0.9870246609606441, 'eval_runtime': 2.3196, 'eval_samples_per_second': 652.279, 'eval_steps_per_second': 81.912, 'epoch': 6.0, 'step': 1608}, {'loss': 0.0102, 'grad_norm': 1.982527494430542, 'learning_rate': 4.166666666666667e-06, 'epoch': 7.0, 'step': 1876}, {'eval_loss': 0.05060989409685135, 'eval_precision': 0.7, 'eval_recall': 0.7214170692431562, 'eval_f1': 0.710547184773989, 'eval_accuracy': 0.9875718138117012, 'eval_runtime': 2.3158, 'eval_samples_per_second': 653.349, 'eval_steps_per_second': 82.047, 'epoch': 7.0, 'step': 1876}, {'loss': 0.0077, 'grad_norm': 0.24639643728733063, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.0, 'step': 2144}, {'eval_loss': 0.05450328439474106, 'eval_precision': 0.7122641509433962, 'eval_recall': 0.7294685990338164, 'eval_f1': 0.720763723150358, 'eval_accuracy': 0.987962637276742, 'eval_runtime': 2.326, 'eval_samples_per_second': 650.468, 'eval_steps_per_second': 81.685, 'epoch': 8.0, 'step': 2144}, {'loss': 0.0061, 'grad_norm': 0.5217588543891907, 'learning_rate': 2.5e-06, 'epoch': 9.0, 'step': 2412}, {'eval_loss': 0.05531409755349159, 'eval_precision': 0.7248427672955975, 'eval_recall': 0.7423510466988728, 'eval_f1': 0.7334924423229913, 'eval_accuracy': 0.988353460741783, 'eval_runtime': 2.2997, 'eval_samples_per_second': 657.909, 'eval_steps_per_second': 82.619, 'epoch': 9.0, 'step': 2412}, {'loss': 0.0052, 'grad_norm': 0.28595301508903503, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.0, 'step': 2680}, {'eval_loss': 0.0566956028342247, 'eval_precision': 0.7078825347758887, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.722397476340694, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 2.2686, 'eval_samples_per_second': 666.941, 'eval_steps_per_second': 83.753, 'epoch': 10.0, 'step': 2680}, {'loss': 0.0044, 'grad_norm': 0.12233524024486542, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.0, 'step': 2948}, {'eval_loss': 0.05836883559823036, 'eval_precision': 0.7026194144838213, 'eval_recall': 0.7342995169082126, 'eval_f1': 0.7181102362204724, 'eval_accuracy': 0.987923554930238, 'eval_runtime': 2.282, 'eval_samples_per_second': 663.029, 'eval_steps_per_second': 83.262, 'epoch': 11.0, 'step': 2948}, {'loss': 0.0039, 'grad_norm': 0.11783494055271149, 'learning_rate': 0.0, 'epoch': 12.0, 'step': 3216}, {'eval_loss': 0.05816805735230446, 'eval_precision': 0.710077519379845, 'eval_recall': 0.7375201288244766, 'eval_f1': 0.7235387045813586, 'eval_accuracy': 0.9880017196232462, 'eval_runtime': 2.2937, 'eval_samples_per_second': 659.645, 'eval_steps_per_second': 82.837, 'epoch': 12.0, 'step': 3216}, {'train_runtime': 617.342, 'train_samples_per_second': 166.585, 'train_steps_per_second': 5.209, 'total_flos': 3922873450278684.0, 'train_loss': 0.023381381596795363, 'epoch': 12.0, 'step': 3216}]

Evaluation, NbAiLab/nb-bert-base
***** predict metrics *****
  predict_accuracy           =     0.9896
  predict_f1                 =     0.7333
  predict_loss               =     0.0424
  predict_precision          =     0.7194
  predict_recall             =     0.7478
  predict_runtime            = 0:00:01.93
  predict_samples_per_second =    648.438
  predict_steps_per_second   =     81.314
Train and save best epoch to /cluster/work/users/egilron/seq-label_github/configs/saga/03121434_elsa-polarity_nb-bert_base_02_202.json completed. F1: 0.7333333333333334

Job 10922760 consumed 115.5 billing hours from project nn9851k.

Submitted 2024-03-12T15:55:53; waited 2.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 20.0 hours
Elapsed wallclock time:   14.4 hours

Task and CPU statistics:
ID              CPUs  Tasks  CPU util                Start  Elapsed  Exit status
10922760           1            0.0 %  2024-03-12T15:55:55   14.4 h  0
10922760.batch     1      1    54.5 %  2024-03-12T15:55:55   14.4 h  0

Used CPU time:   7.9 CPU hours
Unused CPU time: 6.6 CPU hours

Memory statistics, in GiB:
ID               Alloc   Usage
10922760          24.0        
10922760.batch    24.0     5.5

GPU usage stats:
Job 10922760 completed at Wed Mar 13 06:22:07 CET 2024
